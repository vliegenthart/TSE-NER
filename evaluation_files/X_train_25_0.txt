estimate the cross-lingual relevance model using equations 1 and 2. We observe tremendous improvements of 100% to 200% percent  , by adding the TDT data  , even though this data was automatically generated using SYSTRAN.However  , these datasets do not include multilingual CH metadata. GERBIL can be used with systems and datasets from any domain.The spatial data is collected by the OpenStreetMap 5 project and it is available in RDF format. The goal of LinkedGeoData is to add a spatial dimension to the Semantic Web.In the absence of adequate explicit user feedback  , AlgoViz usage data has helped us to generate networks and find common usage patterns. One example of a project that combines an educational portal with online community is the AlgoViz Portal http: //algoviz.org.Of the 197 occurrences of 'bank'  , the vector analysis correctly assigned 45 percent of them to the correct sense. Wilks manually disambiguated all occurrences of the word 'bank' within LDOCE according to the senses of its definition and compared this to the results of the cosine correlation.We never attempted to identify the individuals whose profiles we analyzed. To adhere to ethical standards concerning incorporation of user data into research  , we decided to only use data that is publicly available – either as online profiles Quora  , Health Q&A  , or as datasets used in numerous other studies AOL.Thus in our analysis of Quora  , we only refer to upvotes and disregard downvotes . Downvotes are processed and only contribute to determining the order answers appear in.We investigated the effort to implement a BAT-framework adapter in contrast to evaluation efforts done without a structured evaluation framework in Section 4. Due to the community effort behind GERBIL  , we could raise the number of published annotators from 5 to 9.BrightKite was a location-based social networking website where users could check in to physical locations. BRIGHTKITE.All four systems evaluated in  TDT-2002 cluding ours are using it. Currently  , TFIDF is the prevailing technique for document representation and term weighting for the New Event Detection task.Another thread of research on social search is closely related to social question-and-answer QA systems  , like Quora 30   , that allow users to ask questions to a larger community  , or Aardvark 15  , 8  , which connected users to individual members to whom they could ask a question. Moreover  , finding others willing to collaborate at the same time seems to be a barrier to wider adoption of this technology.Though the precise details of topics and stories in the TDT4 corpus are still unavailable  , the fact that all the systems that were run on it as part of this year's official TDT evaluations performed badly lends credence to the belief that the TDT4 corpus is more challenging than the TDT3 corpus. Figure 8shows the DET curves for SYSTEM 1  , SYSTEM 2  , and SYSTEM 3.To assign the examples to the categories  , we crawled all 50 example websites  , downloading the homepage from each example  , and following site-internal links up to one level deep. Therefore   , we use the descriptions from the 50 examples and the 21 ,872 Wikitravel suggestions to assign the 50 examples to the 5 Wikitravel categories.We provided a list of TDT-2 judged topics  , with checkboxes  , where the evaluator could check off which topics was represented  , as well as text boxes where the evaluator could enter their own topic descriptor  , if none matched. Clicking on a story hyperlink brought up a new browser window containing the story.The two key issues which arise in the context of crowdsourcing are quality— is the obtained solution or set of contributions of high quality ?— as well as participation— there is a nonzero effort or cost associated with making a contribution of any quality in a crowdsourcing environment which can be avoided by simply choosing to not participate  , and indeed many sites have too little content . Answers  , Stack- Overflow or Quora.We evaluate our system initially at Cf=/C , ,~0~ = 1  , which was the standard metric in the 1998 TDT-2 evaluation. This cost measure is parameterized by an adjustable parameter Cj=/Cm~ which controls the relative costs of misses and false alarms.We computed Fleiss' Kappa to measure the inter-annotator agreement for this task  , obtaining 0.241 for the Quora topics   , 0.294 for the HF topics  , and 0.157 for the NYT topics. The first condition can capture  , for instance   , words related to diseases  , the second can capture words related to political or religious positions.This team gathered attractions from Wikitravel and created vector representations of all the venues based on their titles and descriptions. The output of this technique RunA is compared with using KNN instead of the Softmax algorithm RunB.In this paper  , we presented and evaluated GERBIL  , a platform for the evaluation of annotation frameworks. In the future  , we also plan to provide information about the point in time since when an annotator is stable  , i.e.  , the algorithm underlying the webservice has not changed.BRIGHTKITE. We describe each of the datasets in detail below.We gathered our Quora dataset through web-based crawls between August and early September 2012. The basic statistics of both datasets are shown in Table 1Quora.Thus  , a straightforward way to evaluate our model is to use a corpus  , calculate the similarities between all documents and manually check whether two documents are similar or not. While TDT systems associate a main event with documents and cluster incoming news articles according to these events  , we take into account all events extracted from documents to calculate event-centric similarity scores.We collected concrete examples of research tasks  , and classified them into categories. Through interviews we conducted with scholars  , we learned that while the uncertain quality of OCRed text in archives is seen as a serious obstacle to wider adaption of digital methods in the humanities  , few scholars can quantify the impact of OCR errors on their own research tasks.Issuing the generated queries based on the top 30 keywords per site resulted in a ranked list of the 5 candidate categories for each given example website. The crawled and concatenated text of each of the 5 Wikitravel categories served as document representations  , which we indexed using Indri.We examine how the social repin network selectively samples the underlying network of Pinterest. We then define the social repin network  , as the subgraph of links in the Pinterest network over which at least one social repin happens in our data.Interestingly  , such reappropriation and curation of content discovered by other users termed as " repins "  is by far the most common activity on Pinterest  , constituting about 90% of user actions  , as compared to directly discovering and pinning new images  , which constitutes only 10% of actions 1 . Users on Pinterest can copy images pinned by other users  , and " repin " onto their own pinboards.Pinterest incorporates social networking features to allow users to connect with other users with similar interests. Most of pinboards are associated with one of 32 categories such as 'Design'  , 'Products'  , 'Home Decor'  , 'Animals and Pets'  , etc.  , which are globally recognised on Pinterest.Empirically measuring the quality of recommendations has  , in the past  , fallen into two camps. This dataset  , from the German movie-rental site MoviePilot  , was released as part of theWe make the new dataset publicly available for further research in the field. In contrast with the previous standard benchmark  , WS-353  , our new dataset has been constructed by a computer algorithm also presented below  , which eliminates subjective selection of words.In GERBIL  , we make use of the D2KB task  , which evaluates entity disambiguation only. To evaluate DoSeR as well as the competitive disambiguation systems we use the GERBIL -General Entity Annotator Benchmark 23  which offers an easy-touse platform for the agile comparison of annotators using multiple data sets.While it is public knowledge that Quora differs from its competitors in its use of social networks and real identities  , few additional details or quantitative measures are known about its operations. This is a difficult question to answer  , given Quora's own lack of transparency on its inner workings.The KDDCUP 2005 winning solution included two kinds of base classifiers and two ensemble classifiers of them. In addition  , we propose a category-selection method to select the categories in the intermediate taxonomy so that the effectiveness and efficiency of the online classification can be improved.13 The plot shows that the additionally curated identities are young identities that have probably not had time to gain sufficient reputation on Pinterest  , confirming our expecta- tions. Figure 6compares the CDF of the age of the accounts on Pinterest for identities that are curated using intra-domain reputation signals  , and the additionally curated identities.The intensity of color in Figure 11captures the number of users with a given set of similarity scores. Conversely  , a user at position 1 ,1 in the Pinterest plot would have 1 exact similarity between the LIWC of their own pins and tweets  , and 2 exact similarity between the LIWC of their pins and the pins of other Pinterest users.Our estimated number of questions in Quora for June 2012 is 700K  , which is consistent with previously reported estimates 24. Both lines increase smoothly without gaps  , suggesting that Quora did not reset qid in the past and the questions we crawled are not biased to a certain time period.Traditional information retrieval measures are inappropriate for measuring users' perceptions of results in this task as they do not explicitly represent the tradeoff between costs of false-positives and misses. We adopt the TDT cost function to evaluate our result-filtering task.All data sets are integrated in GERBIL and strongly differ in document length and amount of entities per docu- ment. In the following  , we present seven well-known and publicly available data sets which are used in our evaluation.Table 1 shows topic-weighted and story weighted minimum normalized costs for our systems on the TDT3 dataset. When testing on TDT3  , we used TDT2 for training  , when testing on TDT4 the official TDT-2002 evaluation data  , we used TDT2 and TDT3 for training.The crawls follow a BFS pattern through the related questions links for each question. We followed the advice from a Quora data scientist 3 and start our question crawls using 120 randomly selected questions roughly evenly distributed over 19 of the most popular question topics.To highlight key results  , we use comparisons against Stack Overflow  , a popular Q&A site without an integrated social network. In this paper  , we perform a detailed measurement study of Quora  , and use our analyses to shed light on how its internal structures contribute to its success.Our study design was driven by several features that we discovered in this massive corpus. For scanned articles  , per-article metadata such as titles  , issue dates  , and boundaries between articles are also derived algorithmically from the OCRed data  , rather than manually curated.2013  has shown that behavior on Pinterest differs significantly by gender. Other work Ottoni et al.Experimental results show that DSN-based recommendation performs better compared to when only text similarity is used. As our testbed we use the AlgoViz Portal 1 which collects metadata on Algorithm Visualizations and provides community support.Section 3 discusses the corpus and evaluation measures that are used in this study. In Section 2 we discuss the TDT initiative  , its basic ideas  , and some related work.What role do the " related questions " feature play ?  Given the rapid growth of questions on question-and-answer sites  , how does Quora help users find the most interesting and valuable questions and avoid spammy or low-value questions ?We proposed two strategies to collect data from About.me. To accurately establish this mapping  , we employ the emerging social services such as About.me 3 and Quora 4   , where they encourage users to explicitly list their multiple social accounts on one profile.For example  , NASDAQ real-time data feeds include 3 ,000 to 6 ,000 messages per second in the pre-market hours 43; Network and application monitoring systems such as Net- Logger can also receive up to a thousand messages per sec- ond 44. Depending on the application  , the number of messages per second ranges from several to thousands.Note that streams for synthetic data differs from NASDAQ data in terms of the lag and the missing update distributions. The stream generation process is as follows: A stream would pick elements of the Z vector sequentially and could perform the following three operations: a Simulate missing update: Ignore the picked element and move to the next element with Bernouilli probability = pmiss k   , b Simulate independent error: Add Gaussian noise with precision β k > 1  , c Simulate Lag: Publish the noisy update after lag governed by Uniform distribution in the range 1 − 10.Table 2 shows some more factors for the TDT-1 collection which clearly reeect the vocabulary dealing with certain events: the war in Bosnia and Iraq  , the crisis in Rwanda  , and the earthquake in Kobe. In PLSA  , mixing proportions can be computed by EM iteration  , where the factors are xed such that only the mixing proportions P zjq are adapted in each MMstep.We focus on a popular black-market site called addmefast.com. There are multiple black-market sites where Pinterest repins or likes can be fraudulently obtained 6  , 7  , 8.Both sites are built around members evaluating and discussing beer. We use this framework to study two large  , active online communities: RateBeer and BeerAdvocate.By estimating the Wikitravel category for the provided examples  , we created personalised category prior probabilities. This year we experimented with the Wikitravel suggestion categories for buying  , doing  , drinking  , eating and seeing.In Section 5  , we compare the approaches empirically on the tasks of KDDCUP 2005 competition. In Section 4  , we briefly introduce the previous methods and put forward a new method.Depending on the user's option  , three possible scenarios can be generated from this pattern. With further customization  , the user can enable three possible methods for refreshing data from Nasdaq.By integrating such a large number of datasets  , experiment types and frameworks  , GERBIL allows users to evaluate their tools against other semantic entity annotation systems short: entity annotation systems by using exactly the same setting  , leading to fair comparisons based on exactly the same measures . GERBIL is an opensource and extensible framework that allows evaluating tools against currently 9 different annotators on 11 different datasets within 6 different experiment types.Best results on the TDT-2001 evaluation are reported with half time being around 2 days. The similarity to documents inside this window isOur study focuses on gender-based analysis of user behavior and our contributions are the following:  We develop a distributed crawler to collect a large dataset from Pinterest. To our knowledge this is the first study to conduct a large scale analysis of Pinterest.Previous TDT research I has tended to focus on building better clustering techniques to improve detection accuracy. In this project we are concerned mainly with the task of online new event detection.In addition to listing the citing articles  , Citebase provides a summary graph of citations and downloads e.g.   , the articles cited by the current article  , articles that have cited the current article  , and articles co-cited alongside the current article.Another kind of social interaction explored was board sharing  , for which we calculate the percentage of shared boards among the total amount of boards  , separated by gender Table 2. As shown in figure 4  , Pinterest users tend to follow others entirely and this behavior is not mediated by gender.However  , GERBIL is currently only importing already available datasets. 28 The extensibility of the datasets in GERBIL is furthermore ensured by allowing users to upload or use already available NIF datasets from DataHub.Figure 1 shows the relation between the number of suggestions in the context city and the fraction of geographically  There is a clear relation between the number of suggestions available in a city and the P@5G score. If suggestions from outside the context cities are geographically irrelevant  , we should focus on finding other sources for suggestions in those cities where few are provided on Wikitravel.In Pinterest  , user interest is conveyed by the images a user pins and repins in different boards  , each of which is tagged with a category. Second  , we investigate the interests of users on these two different platforms .Over a period of 50 days  , we collected more than 2 million profiles  , which comprise beyond 850 million images and videos pinned into more than 20 million boards. Our study focuses on gender-based analysis of user behavior and our contributions are the following:  We develop a distributed crawler to collect a large dataset from Pinterest.Thus  , even though each pinboard may be exclusive to a user  , the act of pinning implicitly categorises the image. However  , each pinboard may be associated to one of 32 categories defined globally for all users by Pinterest.the TDT-1 collection: real love in the context of family life as opposed to staged love in the sense of Hollywood". An asterix for LSI indicates that no performance gain could be achieved over the baseline  , the result at 256 dimensions with a 1 : 2 combination with the baseline score is reported in this case.The assumptions we make on the considered dataset are as follows. In total  , we collected around 13 ,000 spatial objects in Milano and 30 ,000 in London; those objects are instances of around 180 LinkedGeoData ontology classes our spatial features.Some users are interested in highly unstructured text data OCRed from field journals  , or more conventional relational tables of data  , so BigSur does not require that these super-classes are used. This section of the schema is not mandatory.In this paper  , we perform a detailed measurement study of Quora  , and use our analyses to shed light on how its internal structures contribute to its success. A simple search on Quora about how it works produces numerous unanswered questions about Quora's size  , mechanisms  , algorithms  , and user behavior.In question-answering sites  , e.g.  , Quora and Stack Overflow  , an important task is to route a newly posted question to the 'right' user with appropriate expertise and several methods based on link analysis have been proposed 45  , 6  , 46. The factorization technique can be naturally extended by adding biases  , temporal dynamics and varying confidence levels.Pinterest is a pinboard-style image sharing social network designed to let users collect and share images and videos in an organized  , categorized way. 1 http://bit.ly/1jfjRHL 2 http://bit.ly/1ksdYHv 3 http://bit.ly/1dxEJSX 4 http://bit.ly/OFmPrj Figure 1: Pinterest profile of a famous designer/blogger.To ensure that the sample was as unbiased as possible  , the user IDs were randomly sampled from a near complete snapshot of the Pinterest social network collected and provided by Zhong et al. To analyse users' activities and understand how they accumulate and use social capital  , we first created a dataset by crawling the entire pinning activity history of 50 ,000 users from the time they joined Pinterest until an ar-bitrarily chosen end date of April 1  , 2014.TDT evaluations have included stories in multiple languages since 1999. Topics are defined by a small number of training stories  , typically one to four  , and the task is to find all the stories on those topics in the incoming stream.Some users are mainly interested in bibliography entries. The clustering results along with the topics highlighted in the previous section indicate that AlgoViz users have clusters of interests when it comes to using online resources related to algorithm visualizations.Our goal is to make it easier to re-appropriate and re-categorise content for personal use. Consequently  , curation on Pinterest remains a highly manual process as well.For the event detection problem  , many approaches have been based on clustering or classification to estimate the similarity between the events and documents e.g. New Event Detection  , one of the five tasks in TDT  , aims to detect whether a given news story is concerned with already known events to a system or not.purity when curating identities using all signals intra-domain and inter-domain reputation signals  , only Pinterest signals only intradomain reputation signals  , and by simply randomly picking identities . Figure 5shows the coverage vs. the level of Figure 5: Coverage of the curated set for a given purity level for identities curated using all signals  , only Pinterest signals  , and identities curated at random.Being a web-based platform it can be also used to publish the disambiguation results. 29  proposed GERBIL - General Entity Annotator Benchmark  , an easy-to-use platform for the agile comparison of annotators using multiple data sets and uniform measuring approaches.The results presented in the experimental section were obtained using the Quora topic model as the background knowledge model. Training corpus changes.This process was conducted recursively  , until no further profiles were discovered. We started the extraction process with one highly connected FriendFeed user and crawled the profiles of all his subscribers and subscriptions .Thus  , line features are designed to estimate properties of OCRed text within a line  , which can be calculated based on OCRed text and bounding box information in the DjVu XML file. Additionally  , text within the same line usually has the same style.The related question graph provides an easy way for users to browse through Quora's repository of questions with similarity as a distance metric. This effectively creates a related question graph  , where nodes represent questions  , and links represent a measure of similarity as determined by Quora.This is because Quora recommends topics during the sign-up process. First  , the large majority 95% of users have followed at least 1 topic.To validate our hypothesis  , we examine the correlation between a user's follower count and the quantity and quality of her answers to questions. Thus our hypothesis is that  , outside of the small portion of celebrities who get followers just by their mere presence  , the majority of Quora users attract followers by contributing a large number of high-quality answers.We adapt RIP to apply the replication thresholds tdt and tpt on blocks of documents and postings instead of single el- ements. The first block has size k  , and the size of the following blocks increases exponentially  , using a power of 2.The category Microsoft has a homonymous page  , categorized under Companies listed on NASDAQ which has the head lemma companies. An example is provided in Figure 2.The techniques adopted for TDT and event detection can be broadly classified into two categories: 1 clustering documents based on the semantic distance between them 34  , or 2 grouping the frequent words together to represent events 22. Both TDT and event detection are concerned with the development of techniques for finding and following events in broadcast news or social media.The assessors' instructions were taken from the TDT assessor manual. The assessors were instructed to make a distinction between target items that describe the same event as the source item and targets that describe related events.We believe that the clusters of features found are indicative of the major news stories that were covered by the news organizations during the time spanned by the corpus and provide a good summation of these topics. Our final run on the evaluation portion of TDT-2 produced 146 clusters.The item consumed in this case is the check-in location given by its anonymized identity and geographical coordinates. BrightKite is a now defunct location-based social networking website www.brightkite.com where users could publicly check-in to various locations.By activity  , we mean the amount of content generated. As stated before  , Pinterest is all about pins  , thus our first analysis focuses on the activity of the users.An exception is the Datahub data set D  , where the distribution of resources in type sets and property sets seems comparable. Similar observations can be made for the data set A  , F and G  , though to a lower extent.For each scanned volume  , the metadata generation system takes the DjVu XML file as input and parses the hierarchy of objects contained within the file. Thus  , line features are designed to estimate properties of OCRed text within a line  , which can be calculated based on OCRed text and bounding box information in the DjVu XML file.The y-axis of the Pinterest scatter plot captures the cosine similarity between each user's Pinterest LIWC-vector and the network LIWC-vector for Pinterest. We refer to this as the " Identity " axis.EM algorithm. Using parallelization with 20 threads  , our model could be fit on our largest dataset RateBeer of 2 million total events within two minutes.For identities that posted malicious pins  , we use a threshold for the fraction of malicious pins posted  , which corresponds to the top 1% most untrustworthy identities. Figure 4 compares the CDFs of various reputation signals for random Pinterest identities and four different kinds of untrustworthy Pinterest identities.To accurately establish this mapping  , we employ the emerging social services such as About.me 3 and Quora 4   , where they encourage users to explicitly list their multiple social accounts on one profile. To represent the same users with multiple sources  , we need to first tackle the problem of " Social Account Mapping "   , which aims to align the same users across different social networks by linking their multiple social accounts 1.In particular  , it indicates the significance of mining correlating features for detecting corresponding events. If a large number of them can be uncovered  , it could significantly aid TDT tasks.The first is TDT 1  collections  , which are benchmarks for event detection . We prepare two datasets for experiments.For our empirical analysis  , we use the different segments of the data set provided for the Billion Triple Challenge BTC 2012. The stream-based approach is also applicable to the full data crawls of D Datahub ,Pinterest has an interesting way of dealing with commercial products in the network . We used the tau-b version of Kendall's tau  , which is defined by:A simple search on Quora about how it works produces numerous unanswered questions about Quora's size  , mechanisms  , algorithms  , and user behavior. While it is public knowledge that Quora differs from its competitors in its use of social networks and real identities  , few additional details or quantitative measures are known about its operations.This cluster contains 43 questions  , and all questions are related to " Quora. " Table 4shows an example of one generated cluster.Another important kind is detecting new events  , which has been studied in the TDT evaluations. This is by no means the only kind of novelty detection.Since our goal is not to develop a new burst detection algorithm   , we simply adopt Kleinberg's 2-state finite automaton model 22 to identify bursty periods of entities. Burst detection from a stream of documents have been thoroughly investigated in TDT and event detection 22  , 17  , 31.The social graph of Pinterest is created through users following other users or boards they find interesting. In addition to pinning or repinning  , users can like a pin or comment on a pin.In contrast with the previous standard benchmark  , WS-353  , our new dataset has been constructed by a computer algorithm also presented below  , which eliminates subjective selection of words. Finally  , empirical evaluation shows that TSA exhibits superior performance compared to the previous state of the art method ESA  , and achieves higher correlation with human judgments on both datasets.Social collecting is the collection  , categorization  , and representation of a digital object in a system that is accessible via the Web. We define a site like Pinterest  , that combines social and collecting capabilities  , as a " social collecting " website.Then  , we extract all the unique URLs corresponding to events annotated in GDELT with one of these themes for each day. This results in a set of 39 themes full list in our data release   , details at the end of the paper.Two forms of transcription are available for the audio stream. TDT data consist of a stream of news in multiple languages and from different media -audio from television  , radio  , and web news broadcasts  , and text from newswires.The distribution is somewhat different over the 50 examples than over the Wikitravel suggestions. Of the 50 examples  , 10 are assigned to the Buy category column 4 in Table 1  , 12 to Do  , 7 to Drink  , 9 to Eat and 12 to See.An algorithm based on the linguistic features for text message analysis is also put forward. In this paper  , we choose the single-pass clustering algorithm which is popular in TDT as the baseline and then propose three variations of the single-pass clustering algorithm to take the temporal information into consideration .Thus we have retained the simple approach used last year  , based on overlapping rectangular windows of the audio stream 1 . Although other approaches   , such as those investigated in the TDT programme  , are of some interest  , we have no evidence of their suitability for spoken document retrieval.In order to generate user profiles the ratings users gave for the example attractions along with the created vectors that represent each sample attractions are combined and passed to the Softmax algorithm. This team gathered attractions from Wikitravel and created vector representations of all the venues based on their titles and descriptions.All gathering is based on HTTP requests   , which is a challenge due to how the network was developed . There is no official public API to gather data from Pinterest   , thus we create our own distributed framework based on a client-server model.For our classification of TDT-4 we trained on judged documents from both TDT-2 and TDT-3. For our classification experiments  , we trained on TDT-2 judged documents and tested on TDT-3 documents.This comprises articles  , advertisements  , ocial notifications  , and the captions of illustrations see Table 1for details. The newspaper data set made available to us ranges from 1618 to 1995 4 and consists of more than 102 million OCRed newspaper items.To analyse users' activities and understand how they accumulate and use social capital  , we first created a dataset by crawling the entire pinning activity history of 50 ,000 users from the time they joined Pinterest until an ar-bitrarily chosen end date of April 1  , 2014. Dataset.Given that the TDT program was already investigating technology for story segmentation  , we did not want to require SDR systems to find the topical boundaries in the audio recordings. Therefore  , it was decided that SU systems would output a ranked list of time pointers.In total  , this test corpus contains 1 ,5 million news articles. GDELT contains a set of entities for each article ; however  , we ignored these annotations and solely relied on our own methods to extract and disambiguate entities.To cater to the characteristics of text stream  , we propose three variations of the single-pass clustering algorithm. We employ the single-pass clustering algorithm incremental clustering which is tested in TDT 20 as the baseline algorithm.For the exponential model  , minimum topic-weighted detection first increases and then approaches the baseline; for the linear model  , detection cost starts very high and decreases with larger window sizes and also approaches the baseline. Figure 5 shows the baseline result without using time information horizontal line  , and results for halftimes exponential decay and window sizes linear decay ranging from one hour to 4320 hours 180 days when training on TDT- 2 data and testing on TDT-2002 dry run data.Six collections  , relevant to the assignment about television and film personalities  , from various archives were indexed: 1 a television program collection containing 0.5M metadata records; 2 a photo collection with 20K photos of people working at television studio; 3 a wiki dedicated to actors and presenters 20K pages; 4 25K television guides that are scanned and OCRed; 5 scanned and OCRed newspapers between 1900 and 1995 6M articles; and 6 digital newspapers between 1995 and 2010 1M articles. Collections.For example  , it takes two days for EM to finish for the RateBeer dataset  , whereas our method takes just two minutes. EM takes more than 1 ,000 times as long to execute.GERBIL is not just a new framework wrapping existing technology. The persistent URIs enhance the long term quotation in the field of information extraction.This is too small a number for a statistically reliable estimation of performance. Since there are only 25 events containing 1131 stories defined in the TDT corpus  , and each event has only one story as the first report of that event  , only 25 stories should have a flag of New for the entire corpus.This effectively creates a related question graph  , where nodes represent questions  , and links represent a measure of similarity as determined by Quora. One of Quora's core features is the ability to locate questions " related " to a given question.The first set is a relatively coarse-grained approach wherein we measure how many images a user repins in each of the 32 different Pinterest categories. To describe users' preferences for particular types of content  , we use two sets of features: Category Preferences and Object Preferences .Quora applies a voting system that leverages crowdsourced efforts to promote good answers. Second  , do super users get more votes  , and do these votes mainly come from their followers ?The tasks defined within TDT appear to be new within the research community. 24It was shown tasks can be accomplished efficiently with Citebase regardless of the background of the user. " The report found that " Citebase can be used simply and reliably for resource discovery.In the bottom half of Table 2we show rating statistics per Wikitravel category  , based on the estimated category per example. The ratings over the examples are distributed more evenly  , with the lowest rated example having an average rating of 1.41 and the highest 3.49.In the breadth-of-interest model from Section 3.3.3  , we set the parameter k to 0.3  , i.e. The latter two models were trained on NYT and Quora corpora described in Section 4.The winner of the KDDCUP 2005 competition found that the best result was achieved by combining the exact matching method and SVM. This indicates that the bridging classifier works in a different way as the exact matching method and SVM  , and they are complimentary to each other.In the context of sub-question 3  , we will perform various crowdsourcing tasks e.g. The " Open Knowledge Extraction " challenge at ESWC 7 and frameworks such as GERBIL 28 are good systems to validate our approach.Each Quora user has a profile that displays her bio information  , previous questions and answers  , followed topics  , and social connections followers and followees. In addition  , users can follow topics they are interested in  , and receive updates on questions and answers under this topic.Let M be the output annotations of an entity disambiguation system on the same input. Note that in all the results reported  , mentions that contain NIL or empty ground truth entities are discarded before the evaluation; this decision is taken as well in Gerbil version 1.1.4.Figure 6also shows that to curate identities using Pinterest reputation signals alone  , we have to wait at least 15 months  , but by exploiting inter-domain reputation signals  , we can curate identities more than 10 months in advance. 13 The plot shows that the additionally curated identities are young identities that have probably not had time to gain sufficient reputation on Pinterest  , confirming our expecta- tions.The goal of a topic detection algorithm is to impose an organization on a collection of documents such that the underlying topical structure is exposed. TDT comprises several tasks; in this paper we focus on the topic detection task.Instead  , we assume that identities with higher fractions of blocked pins are more likely to be untrustworthy. We assume that a vast majority of the random Pinterest identities are indeed trustworthy  , and hence  , we do not consider all identities that posted a single blocked pin to be untrustworthy.Before diving into main analytical results of our work  , we begin in this section by first describing our data gathering methodology and presenting some preliminary results. We also analyze some high level metrics of the Quora data  , while using Stack Overflow as a baseline for comparison.Experiments on the KDDCUP 2005 data set show that the bridging classifier approach is promising. In our solution  , an intermediate taxonomy is used to train classifiers bridging the queries and target categories so that there is no need to collect the training data.Next we consider how experience relates to user retention. We mention the parallel work of 9  , which also studies BeerAdvocate and RateBeer data: there  , a user's failure to adopt the linguistic norms of a community is considered as a factor that may influence whether they will abandon that community.The following concepts are formally defined in TDT community 12:  Topic: seminal event or activity along with all directly related events and activities.  Story: a topically cohesive segment of news that includes two or more declarative independent clauses about a single event.If the NASDAQ Computer Index were further divided into software  , hardware  , services  , etc.  , one can further analyze comparisons with them. In this instance  , the computer sector has been outperformed by one of its members Apple by a large margin.So  , when we merge the group profiles the items considered in training were the items rated by at-least one member who has a group identifier. We note that the MoviePilot data does not contain the group information for all the users in the training data.An explanation for this is that teasers often mention different events  , but according to the TDT labeling instructions they are not considered on-topic. Exclusion of very short stories from similarity calculations tends to improve results.Some of the top-ranked posts discuss the relationship of human capital and ICT-related developments. The selected EconStor article and its related blog posts show a meaningful relationship.Textual memes. Overall  , we consider 1 ,084 ,816 reviews from 4 ,432 users in BeerAdvocate  , and 2 ,016 ,861 reviews from 4 ,584 users in RateBeer.There are 724 ,672 Pinterest identities with at least one blocked pin  , which includes 43% of all Pinterest identities. We refer to pins with blocked URLs as blocked pins.In TDT the atomic units were documents   , which were grouped into those discussing the same event. These tasks all share the insight that atomic units of information should be grouped together into semantic equivalence classes to provide more useful responses to users we refer to this generically as " clustering " .If the system is being evaluated for 4 training stories  , then the training corpus is all stories up to and including the fourth training story and the test corpus is the remainder of the corpus. For those reasons  , the TDT corpus is split into training and test information at a different point for each event.It is desirable in TDT to have a cost function which has a constant threshold across topics. The problem lies in the assumption of P rel being constant.With the choice of the TDT-2 corpus and its known topics  , we added a third question for our evaluation: "Does this cluster of phrases correspond to any of the TDT-2 topics ?" The corpus BBN supplied us with contained 56 ,974 articles.We focus on questions with some minimum number of user interactions ≥4 answers   , which filters out all but 87K 20% questions from our # of Followers and Followees Followers Followees dataset. Unfortunately we could not do the same for question page views  , because Quora only reveals the identity of users who answer questions   , but not those who browse each question.The research results reported in this study are our own; the framework for the work is only partly ours. We wish to make it clear that the corpus and evaluation methodology that were devised in the TDT study were a joint effort by four groups.In comparison to earlier frameworks  , it extends the stateof-the-art benchmarks by the capability of considering the influence of NIL attributes and the ability of dealing with data sets and annotators that link to different knowledge bases. GERBIL is not just a new framework wrapping existing technology.For example  , paid-for services such as pin4ever.com enable users to also store the visual juxtapositions  , re-pins and likes that relate to pinned content. Services already exist to support the backup of Pinterest boards  , tweets and blogs  , and this can take advantage of the curation that has already occurred online.For each test trial  , the system attempts to make a yes/no decision. TDT tasks are evaluated as detection tasks.We are surprised to find that the curves from Stack Overflow and Quora are nearly identical. Next  , we plot the distribution of views and answers per question in Figure 5and Figure 6.Finally we also evaluated with a precision-oriented metric CyyJCm~ ,  , = 10  , which rewards forming very pure clusters  , and is more tolerant of splitting topics into several system clusters. We also evaluated with a recal/-oriented metric Cf=/C ,n~46 = 0.1  , which was the standard metric in the 1999 TDT-3 evaluation   , and which favors large clusters and tolerates lower precision in favor of better recall.In Storify  , the people tend to use social media and web resources to create their narratives about events  , or something of interest. Most of the pins on Pinterest come from blogs  , or uploaded by users.This fan-in  " citations-from "  and fan-out  " citations-to "  then provides the user with links to all articles in the database that have cited a given article  , as well as to all articles that have been co-cited alongside hence are related to the given article. First  , wherever possible  , Citebase links each reference cited by a given article to the full-text of the article that it cites if it is in the database.Currently  , GERBIL offers 9 entity annotation systems with a variety of features  , capabilities and experiments. Furthermore  , we were not able to find a running webservice or source code for this approach.We start with a macro-scale analysis of users with different kinds of social links  , and check whether the active users are also consistent  , by measuring user retention. Users with both types of repins 21% of all users contribute 71% of Pinterest activities.Thus it is important to understand how social ties affect Q&A activities. Quora is unique because it integrates an effective social network shown above into a tradition Q&A site.P2 explicitly stated that while he did publish results based on quantitative methods in the past  , he would not use the same methods again due to the potential of technology-induced bias. We asked P1  , P2 and P4 about the possibilities of more quantitative tools on top of the current digital archive  , and in all cases the interviewees' response was that no matter what tools were added by the archive  , they were unlikely to trust any quantitative results derived from processing erroneous OCRed text.As ISJ involved a particular searching system and a particular set of searchers  , the system or searchers may have influenced the success measured in the two validations of the approach. In addition  , the method was adopted in more recent TDT work Cieri et al  , 2002.The first is in the context of attention rewards on user-generated content UGC based sites  , such as online Q&A forums like Quora or StackOverflow. We are motivated by two different kinds of questions that arise in the context of designing rewards for crowdsourced content  , depending on the setting and the nature of the rewards .Such non-social repins are not hard to find: Pinterest highlights the most recent pins on the platform on its home page. Thus  , given access to the homepage which proves to be a simple and easy to find source of interesting information  , social-based information seeking becomes less critical.Our concerns about this approach are that it is sensitive to clustering accuracy  , and is based on strong assumptions about the nature of redundancy   , which we think is user dependant. This approach is similar to solutions for the TDT First Story Detection problem.The by-author ranking is calculated as the mean number of citations or hits to an author e.g. After generating a search  , Citebase allows the results to be ranked by 6 criteria: citations to the article or authors  , Web hits to the article or authors  , date of creation  , and last update.So instead of IDs  , we rely on other methods to identify users whether registered or unregistered. Despite a small number of registered users  , AlgoViz project leaders are interested in understanding the trends of its overall user base.The table shows clearly that while the greedy and na¨ıvena¨ıve approach achieve similar runtimes on the LinkedGeoData fragment with 1 ,000 resources  , the greedy clustering approach is orders of magnitude slower than the na¨ıvena¨ıve approach in all other cases. Table 1.Downvotes are processed and only contribute to determining the order answers appear in. Quora makes visible the list of upvoters  , but hides downvoters.Additionally   , the MPD and w7 were the result of an extensive organization effort by a whole series of computational lexicologists who had refined its format to a very easily computed structural description Reichert  , Oiney & Paris 69  , Sherman 74  , Amsler and White 79  , Peterson 82  , Peterson 871 The LDOCE while very new  , offered something relatively rare in dictionaries  , a series of syntactic and semantic codes for the meanings of its words. The MPD and w7 provided a mature collection of definitions   , and the family resemblance of the smaller MPD to the w7 and the w7 to the definitive American English dictionary  , the unabridged Merriam-Webster Third international ~31 provided the ability to find out more about definitions in any of the smaller books by consulting its " big brother " when the need arose.GERBIL is an opensource and extensible framework that allows evaluating tools against currently 9 different annotators on 11 different datasets within 6 different experiment types. In this paper  , we present GERBIL – a general entity annotator benchmark –  , a community-driven effort to enable the continuous evaluation of annotation tools.More information can be found at our project webpage http:// gerbil.aksw.org and at the code repository page https: //github.com/AKSW/gerbil. We conclude with a discussion of the current state of GERBIL and a presentation of future work.The TDT cost function assumes that this probability is constant across topics. ferred to as P target; we use the more IR-related form.The presence of a sharp rise in the TDT detection cost for the lowest threshold value can clearly be seen to result from present cost rather than model quality  , since that rise is not reflected in MAP. We also notice that the system can gain a good future utility at the expense of the present utility.These low values confirm that sensitivity is rather subjective . We computed Fleiss' Kappa to measure the inter-annotator agreement for this task  , obtaining 0.241 for the Quora topics   , 0.294 for the HF topics  , and 0.157 for the NYT topics.The methodology uses 11 passes through the stream. Since only 25 events in the corpus were judged  , an evaluation methodology developed for the TDT study was used to expand the number of trials.Other services can harvest this enhanced metadata from Citebase to provide a reference-linked environment  , or perform further analysis or they can be harvested by the source archives to enhance their own data. AMF encapsulates the relationships within the scholarly research: between authors  , articles  , organisations  , and publications.Fig- ure 1 shows a typical profile of a user that Silbermann's strategy successfully attracted to the network Chafkin 2012. Recently  , but after our crawling period  , Pinterest enabled the possibility of creating secret boards: basically  , boards which only the owner has access Milam 2012.Experimental results. We treat BeerAdvocate as a 'development domain'  , because we used it for developing the models and experimental setting  , and RateBeer as a 'test domain' in which we validate our final models on previously unseen data.Descriptions from positive examples in the user profiles are used as queries to rank suggestions. This systems extracts suggestions for sightseeing  , shopping  , eating  , and drinking from Wikitravel pages dedicated to US cities.A more detailed description of the task and evaluation metric can be found in the TDT-2002 evaluation plan 9. It is also used to determine the minimum normalized cost of the system  , i.e.  , the cost that one would achieve if the optimal threshold on the score were chosen.The corresponding GERBIL result sheet is available on the GERBIL website 4 and can be used to make comparisons to our approach in future evaluations. Table 3 shows the F1 values in comparison to the competitor systems on all data sets.We plot the log of negative log-likelihood due to scale of the values  , and so lower value implies that model has higher likelihood. Figures 4b shows the performance of our model in comparison with the best baseline B3 over the NASDAQ.Synonyms are the first type of words for which the TSA method seems to outperform the ESA method. Table 12presents additional examples of pairs belonging to these relations and the ranking of human judgments  , ESA and TSA algorithms for the WS-353 dataset.The process for data cross-linking is based and initiated from the metadata that are used to describe the authors and publications in EconStor. Given this  , the set of publications where a is author is represented asFigure 1displays part of the interactive timeline generated for the complete TDT-2 corpus  , comprising six months of data. The ten most highly ranked clusters  , and the assigned labels  , are presented in Table 4.If users are satiating on items  , we expect to see some k for which the probability of continuing runs decreases as the run length Figure 5: Lack of satiation in MAPCLICKS  , BRIGHTKITE  , and GPLUS. In Figure 5  , we show this curve for several of our datasets.This work provides a first look into this matter on a relative new social network that grew quickly in importance and became one of the most popular and peculiar social network. To our knowledge  , no research paper focused on user behavior  , based on gender  , was conducted in a heavy image-based network such as Pinterest.Intuitively  , using cluster topic vectors to compare with subsequent news stories should outperform using story vectors. Nevertheless  , in TDT domain  , we need to discriminate documents with regard to topics rather than queries.For open questions with no answer   , we infer the question posting time based on the latest activity timestamp on the question page. Since Quora does not show when a question is posted  , we estimate the posting time by the timestamp of its earliest answer.To assess word relatedness  , we use the WS-353 benchmark dataset  , available online 14  , which contains 353 word pairs. Evaluating word relatedness is a natural ability humans have and is  , therefore  , considered a common baseline.By this approach we conclude that roughly two in each ten users who have a webpage linked in their profiles are in fact selfpromoters   , and the proportion of males in this situation is higher than females. By calculating the most used pin source of each user and comparing it with the user's personal website  , available in her description  , we are able to identify self-promoters  , people who use Pinterest to promote their outside webpage.On Pinterest  , we then collected all identities that had liked or repinned two or more to improve the likelihood of discovering untrustworthy identities of the 135 collected pins. We subscribed to the site and collected a list of 135 such pins in one day.In both modalities  , recognition should use adaptation techniques to adjust to changes in the collection language over time. This is in contrast to a TDT-type system which performs online retrieval as the audio is recognized.For AIDA we downloaded the default entity repository that is suggested as reference for comparison. Spotlight and WAT are integrated in GERBIL by default  , whereas we manually downloaded Wikifier and AIDA and installed them on our server with its best settings.The corpora provided for TDT contains approximately 130000 stories from 9 months of broadcast news  , newswire  , and newspaper 7. Results are evaluated by comparing the system clusters  , produced automatically  , with the topics  , annotated by a human judge.Since Quora does not show when a question is posted  , we estimate the posting time by the timestamp of its earliest answer. Assuming we are correct about the use of qid  , we can plot an estimate of the growth of Quora and Stack Overflow   , by plotting qid against time.11 Out of the 1.7M Pinterest identities  , we found that 74 ,549 have been suspended. We use this signal to identify suspended identities on Pinterest.In order to avoid fitting our data for the final evaluation  , we set aside the evaluation section of TDT-2 May and June and built and trained our system on the training and developments sets. Our evaluation method would be to compare the generated clusters with the known topics   , which was the final evaluation our assessors would be performing.We focus on English as a language and newspapers as the source since LODifier can currently only deal with English text and presumably degrades on potentially noisy automatic radio and TV transcripts. We follow the general lead of the original TDT-2 benchmark evaluation schema.The comparison results of TSA on the WS-353 dataset are reported in Table 1. Again  , TSA performs substantially better than ESA  , confirming that temporal information is useful on other datasets.Which identities benefit the most ? Figure 6 : Age of curated Pinterest identities: identities curated using Pinterest reputation signals vs additionally curated identities using all signals.In this project we aim to develop new techniques for story and event representation and so improve system accuracy. Previous TDT research I has tended to focus on building better clustering techniques to improve detection accuracy.More information about GERBIL and its source code can be found at the project's website. In comparison to earlier frameworks  , it extends the stateof-the-art benchmarks by the capability of considering the influence of NIL attributes and the ability of dealing with data sets and annotators that link to different knowledge bases.We used two different corpora for our experiments. The latter is dangerously close to testing on the training data the topics are different but mirrors the official TDT'01 evaluation settings 16.Our use of TDT5 here was merely to evaluate the contribution of each component of our model. Finally we would like to mention that our method is completely unsupervised  , in contrast to many TDT systems which tune their parameters over a training dataset from an earlier TDT run.However  , each pinboard may be associated to one of 32 categories defined globally for all users by Pinterest. Unlike traditional social bookmarking  , pinning on Pinterest does not involve creating an explicit vocabulary of tags to describe the image.In Table 6 we see the distribution of Wikitravel categories over the top 5 retrieved suggestions and over all suggestions in the index. However  , the examples from the Eat category were rated even higher but fail to push Eat suggestions to the top of the ranking.The stream-based approach is also applicable to the full data crawls of D Datahub  , As small data sets  , we used A the full Rest subset 22 ,328 ,242 triples  , B an extract of the Datahub subset 20 ,505 ,209 triples and C an extract of the Timbl subset 9 ,897 ,795 triples 7 .By these means  , we allow benchmarking tools against reference datasets from any domain grounded in any reference knowledge base.  With GERBIL we introduce the notion of knowledge base-agnostic benchmarking of entity annotation systems through generalized experiment types.Coverage: To estimate and compare the coverage for a given target purity level  , we apply the two classifiers on the test data and we rank all the Pinterest identities according to the probabilities returned by the classifier. We then study what kinds of identities benefit the most from inter-domain reputation signals.For SHAKESPEARE  , since the consumption is contrived  , there is no recency the real and permuted curves are near-identical  , which both validates our measure as capturing the amount of repeat consumption  , and shows that the separations in MAPCLICKS and BRIGHTKITE are nontrivial . Interestingly  , caching on the permuted sequences is still higher on this measure than the stable top-k cache  , suggesting that temporally " local " preferences recently consumed items are more important than temporally " global " preferences all-time favorites.In AlgoViz we used the results in two ways: 1 within the content recommendation block that suggests a list of entries based on the DSN analysis results and 2 within the ranking function that generates the ordered list of entries for users during browse and search operations. Knowing the groups  , their interests  , and size gives us leverage on better serving the target audience.The broadcast sources have been speech recognized 30 -35~ word error rate; corresponding manual transcripts either close-captioning or FDCH have also been provided. The corpora provided for TDT contains approximately 130000 stories from 9 months of broadcast news  , newswire  , and newspaper 7.can be reconstructed in a unique manner in future works. Hence  , by using GERBIL for experiments  , tool developers can ensure that the settings for their experiments measures  , datasets  , versions of the reference frameworks  , etc.We then combine page features and line features for volume level and issue level metadata generation. During the parsing of the XML file  , the system calculates features for every word  , line  , paragraph  , and page of the OCRed text.Evaluation of traditional information retrieval tasks focuses on the positions of relevant documents in ranked result sets  , treating the false-positive error of retrieving non-relevant documents as equivalent to its counterpart of missing relevant documents. However  , the standard TDT cost function assumes that the ratio of relevant to non-relevant documents is uniform across queries 4.However  , it was more convenient for us to download the most up-todate original OpenStreetMap data about Bremen  , available as Shapefiles 10 . OpenStreetMap datasets are available in RDF format from the LinkedGeoData project 9 .Most of pinboards are associated with one of 32 categories such as 'Design'  , 'Products'  , 'Home Decor'  , 'Animals and Pets'  , etc.  , which are globally recognised on Pinterest. lection of related pins e.g.  , one pinboard may have pins with images of different wedding dresses.Wilks manually disambiguated all occurrences of the word 'bank' within LDOCE according to the senses of its definition and compared this to the results of the cosine correlation. The occurrences of the defined word in all sentences whose vectors have the greatest similarity to the vector for a given sense are then assigned that sense7. Easy integration of datasets: We also provide means to gather datasets for evaluation directly from data services such as DataHub. In particular  , we integrated 6 additional annotators not evaluated against each other in previous works e.g.  , 7.Users can create connections to other users on Pinterest in two ways. Pinterest incorporates social networking features to allow users to connect with other users with similar interests.6fshows that this result extends to measures of influence on Pinterest. Fig.Note that not all questions remain on the site  , as Quora actively deletes spam and redundant questions 5. The largest qid from our crawled questions is 761030  , leading us to estimate that Quora had roughly 760K questions at the time of our crawl  , and our crawl covered roughly 58% of all questions.Over the course of 10 years the BeerAdvocate and RateBeer communities have evolved both in terms of their user base as well as ways in which users review and discuss beer. User lifespan.We focus in particular on how annotators and datasets can be added to GERBIL and give a short overview of the annotators and tools that are currently included in the framework. Thereafter  , we present the GERBIL framework.We also used a second corpus  , tdt2  , which includes the English news stories from the TDT-2 collection   , amounting to approximately 40 ,000 news stories from newswire and broadcast news sources. These are documents from FBIS dated 1994.This was used both to evaluate the outcomes of the project  , and to help guide the future direction of Citebase as an ongoing service. As part of the project report a user survey 23 was conducted on Citebase.The evaluation of our framework by contributors suggests that adding an annotator to  GERBIL demands 1 to 2 hours of work. Moreover  , 6 novel annotators were added to the platform.Figure 1shows a typical user profile on Pinterest. Personal profiles on Pinterest include a profile image  , a brief self-description  , and lists of the user's boards  , pins  , likes  , followers  , and friends i.e.  , those who the user follows.Assuming that they used a different training corpus to select a value  , this indicates that even the averages vary with the corpus and cannot be assumed constant. Further  , the average P rel across all stories is 0.002 which is different from the number assumed in the standard TDT cost function 0.02.The count is smoothed by a sliding window scheme to assign partial credit to system boundaries at incorrect positions that are close to the reference boundaries. Most currently used story segmentation measures  , such as P k 2  , TDT Cseg 3 and WindowDiff 4  are based on counting the number of incorrectly proposed boundaries.This dataset contains the purchase history from 2004-01-01 to 2009-03-08. Since this paper focuses on the recommendation in ecommerce sites  , we collect a dataset from a typical e-commerce website  , shop.com  , for our experiments.However  , any publishsubscribe system implementing the optimal centralized algorithm in XPath query processing 18 would require a single depth-first traversal of the document tree visiting  , in our example  , twice the nasdaq server. Publish-subscribe systems are more in-line with moving the processing to the data.Figure 15 plots the complementary cumulative distribution function CCDF for both the incoming degree follower and outgoing degree followee. We begin by examining the follower and followee statistics of Quora users.In our work  , a digitized volume corresponds to a collection of objects  , including scanned images of pages  , OCRed text  , manually-generated metadata  , among others. The system detects various types of structural information  , including sentence boundaries  , filler words  , and disfluencies  , within speech transcripts using lexical  , prosodic  , and syntactic features.The first is the unique document found containing both of the words " income " and " forecast " as well as the American Tobacco Company logo and a dollar amount a recognized entity type greater than $500K. We apply conjunctive constraints on document image components to a straightforward document ranking based on total query-word frequency in the OCRed document text; in Fig- ure 2we show document images retrieved for two such queries.Covering these cases enables us to model queries over such data and analyze the effects of executing such queries. Another example is the LinkedGeoData project 4 which provides Linked Data about any circular and rectangular area on Earth 4.The decision of whether or not to harvest from aggregator repositories is made more complex because these aggregators contain records that are not currently available through OAI channels  , and they do not always contain all the records of a particular original repository. As an example  , a search performed in OAIster for " double-well Duffing oscillator " retrieves two records  , exactly the same  , but one was harvested from the arXiv.org Eprint Archive repository an original repository and one harvested from the CiteBase repository an aggregator.This systems extracts suggestions for sightseeing  , shopping  , eating  , and drinking from Wikitravel pages dedicated to US cities. These ranked suggestions are then filtered based on the context.They proposed several features based on users contributions and graph influence. 33  proposed an expertise modeling algorithm for Pinterest.By contrast  , the TDT list of topics only contains events that are represented in the corpus of news stories. Our test documents simply have more to say about the battle of Shiloh than the battle of Springfield  , Missouri.In Section 7.1 we directly compare the approaches on the basis of its results achieved with GERBIL. link to a KB task.We have extended the ontology of LinkedGeoData by the appropriate classes and properties. In 16  , we have created an information model as well  , which is related to the research question 2b.Figure 5 shows the baseline result without using time information horizontal line  , and results for halftimes exponential decay and window sizes linear decay ranging from one hour to 4320 hours 180 days when training on TDT- 2 data and testing on TDT-2002 dry run data. The similarity to documents outside this window i.e.  , age > m is 0.We compute the probability of Pinterest identities to misbehave in the future in two ways: first  , we only use intra-domain reputation signals  , and then we use both intra-domain and inter-domain reputation signals. We then pick the top n trustworthy identities such that the purity remains within a target purity level.Adaptive Hypermedia: Adaptive hypermedia is a rich research field that dates back to the early 1990s 9. In contrast  , the naturally evolving nature of discussion threads and the need for fine-granularity segment boundary identification make the problem of topic segmentation significantly harder than the new-event detection problem addressed by the TDT technologies.Therefore   , these counts will be generated from a fixed training set  , independent of the source  , and they do not change over time. However  , for the TDT tasks  , the test data does not contain ROI labels and the amount of training data is small.The First Story Detection FSD task is defined as detecting the first story that discusses a previouslyunknown event. A TDT system monitors a stream of chronologically-ordered documents  , usually news stories.The newspaper data set made available to us ranges from 1618 to 1995 4 and consists of more than 102 million OCRed newspaper items. We used the default Snowball stemmer for Dutch 6 .The user-topic interaction has considerable impact on question answering activities in Quora. Following the right topics can introduce users to valuable questions and answers  , but is not the only way to access questions.We choose our incremental TF- IWF model to weight terms for its steadier performance in our experiments. Incremental TF-IDF model is widely applied to term weight calculation in TDT 3  , 5  , 7  , 13 .The key concern is users who have many followers can get their followers to vote for their answers  , thus gaining an " unfair advantage " over other users. However  , the social interaction among Quora users could impact voting in various ways.We found moderate to strong correlations between journal-level altmetrics except with Pinterest and the Q&A site  , which is  The absence of high correlations between altmetrics and citationbased metrics shows the existence of differences between scholarly and social importance. All correlations were significant at p < 0.01.We use rule-based approach for title detection using page and line features calculated from OCRed text  , bounding box information  , and context analysis. In terms of the mapping between page index  , the index of a scanned page in the viewable PDF file  , and page number  , the number printed on the original volume  , the program recognizes available page numbers on scanned pages by analyzing the OCRed text in particular areas of pages.We gathered 1 ,706 such untrustworthy identities. On Pinterest  , we then collected all identities that had liked or repinned two or more to improve the likelihood of discovering untrustworthy identities of the 135 collected pins.To detect the first story  , current TDT systems compare a new document with the past documents and make a decision regarding the novelty of the story based on the content-based similarity values. TDT systems monitor continuously updated news stories and try to detect the first occurrence of a new story; i.e.  , an event significantly different from those news events seen before.The cosine distance metric is very common in information retrieval and has been a popular similarity measure in TDT evaluations. The negative of the cosine of the angle between a sentence vector and each previously seen sentence vectors then determines the novelty score for that sentence.Citebase was developed as part of the JISC/NSF Open Citation Project  , which ended December 2002. Citebase contains 230 ,000 full-text eprint records  , and 6 million references of which 1 million are linked to the full-text.Figure 6shows the number of categorized sessions per user in our dataset. The category for a Pinterest session is simply the most frequent category among the pins in that session.PhantomJS is a headless version of the WebKit browser engine that can be automated using Javascript. The Pinterest website is a complex Javascript application  , so we leveraged PhantomJS to crawl the site.To answer this  , we examine the complete activity history of ∼50k randomly selected users on Pinterest .com as a case study. In light of these conflicting results  , it is natural to ask what is the value of social networks on interest-based and contentdriven sites.We followed the advice from a Quora data scientist 3 and start our question crawls using 120 randomly selected questions roughly evenly distributed over 19 of the most popular question topics. Since Quora has no predefined topic structures for its questions questions can have one or more arbitrary topic " labels "   , getting the full set of all questions is difficult.However  , copying continues to be important for the creation of native links over which interaction happens  , even in networks like Pinterest  , where copying appears to be governed by norms of social closeness – Fig. Thus  , while there is no universal pattern for how users on different target networks copy links  , it appears that in both cases  , the links  , and hence the target communities  , tend to become more interest-based over time.Two of the top-most topics in the September 2010 DSN include words related to AlgoViz bibliography entries i.e.  , biblio. The number of sampling iterations for the topic model of each month was 200.A good case can be made for the suitability of the full collection of words as the basra for determining document similarity and eventually clusters. In the TDT effort  , and m most other clustering work for information indexing and retrieval purposes  , the words in the document have dominated as the sole features on which the clustering is based.In other words  , the emphasis of our problem is to identify sets of bursty features  , whereas the emphasis of TDT is to find clusters of documents. The key issue of our hot bursty events detection is to find the minimal sets of bursty features automatically.This suggests that starting with a relatively strict threshold and relaxing that threshold somewhat as relevant documents are discovered might be a productive strategy when using models of this design. Indeed  , the stable MAP at that point suggests that when averaged over topics the system is being overly aggressive in selecting  Figure 1: Adaptation effectiveness: a Model qual- ity high=good b TDT detection cost low=good uments early on when the model is weakest.To compare the performance with previously published results  , we test our segmenter under the conditions of the TDT-3 1 segmentation task. We observe similar improvement over the baseline as in the English TDT-4 data.The six evaluation measures offered by GERBIL as well as the error count are expressed as qb:Measures. Each observation features the qb:Dimensions experiment type  , matching type  , annotator   , corpus  , and time.The clustering results along with the topics highlighted in the previous section indicate that AlgoViz users have clusters of interests when it comes to using online resources related to algorithm visualizations. In both cases  , for any given time span  , if an entry E in AlgoViz received a certain number of views within a cluster whose topics were highly related to that of E  , then E would be weighted more compared to other entries of similar type.There are interesting problems with using this cost function in the context of a DET curve  , the other official TDT measure. We feel that a TDT system would do better to attempt both of those at the same time.TDT2 contained stories in English and Mandarin. TDT evaluations have included stories in multiple languages since 1999.It is the task of online identification of the earliest report for each topic as soon as that report arrives in the sequence of documents. New Event Detection NED is one of the five tasks in TDT.We then analyse Citebase's database  , and summarise the findings of a user survey conducted by the Open Citation Project 7. We introduce the Celestial tool 4 a cache/gateway for the OAI-PMH and Citebase 5 an end-user service that applies citation-analysis to existing OAI-PMH compliant eprint archives.To address these challenges  , we built a distributed crawler that collected pins from the selected users every day. The second challenge occurs because Pinterest does not have a public API for gathering pins with timestamps in bulk.Since this includes a significant fraction of all identities  , in the same way as for identities with blocked pins  , we assume that identities with higher fractions of low WoT reputation pins are more likely to be untrustworthy. There are 1 ,083 ,951 such Pinterest identities that have at least 1 pin with a low WoT reputation score or no pin with a WoT score.Three different clustering schemes were investigated: single link  , average link  ,and complete link. In order to avoid fitting our data for the final evaluation  , we set aside the evaluation section of TDT-2 May and June and built and trained our system on the training and developments sets.We used GDELT http://gdeltproject.org/ news dataset for our experiments. We show that our methods can perform well not only on properly edited texts that are rich in terms of events and facts i.e.  , WikiWars  , WikiBios but also on the news that are compiled from a large source of news channels.Table 1compares the implemented annotation systems of GERBIL and the BAT-Framework. provide the source code 25 as well as a webservice.Our evaluation corpus is built from the TDT-2 corpus 8  of approximately 60 ,000 news stories covering January through June of 1998. The remaining 11 test topics were never looked at except to present £nal information for this paper.1 In both communities users provide ratings accompanied by short textual reviews of more than 60 ,000 different types of beer. For the purpose of this study we will employ data from two large beer review communities BeerAdvocate and RateBeer.Our view is that we will eliminate whatever senses we can  , but those which we cannot distinguish or for which we have no preference  will be considered as falling into a word sense equivalence class. In the experiment in disambiguating the 197 occurrences of 'bank' within LDOCE  , Wilks found a number of cases where none of the senses was clearly 'the right one' Wilks 891.TDT uses a measure that allows for varying the cost of misses versus false positives which they term false alarms. desire 3.We crawled all Wikitravel pages of locations within the US  , starting with the page on the United States of America as the seed list. These are provided by a community of travellers and locals and can be used as a source for contextual sugges- tions.In the experiment in disambiguating the 197 occurrences of 'bank' within LDOCE  , Wilks found a number of cases where none of the senses was clearly 'the right one' Wilks 891. In addition  , it is not always clear just what the 'correct sense' is.For question answering  , nuggets were short phrases that provided interesting information about a target entity. In TDT the atomic units were documents   , which were grouped into those discussing the same event.The composition of the parallel corpus is detailed in Table 2. We discuss the impact of adding the TDT corpus in section 5.In GAC  , all articles are sorted in chronological order  , and then an agglomerative clustering is performed. 's algorithm12 were proposed several years ago  , in terms of empirical results  , it is still one of the best algorithms in TDT evaluations.OpenStreetMap datasets are available in RDF format from the LinkedGeoData project 9 . OpenStreetMap OSM maintains a global editable map that depends on users to provide the information needed for its improvement and evolution.Indeed  , because the user profile and a stream of relevant documents define a far smaller universe of documents than encountered in the TDT task  , we might expect novelty/redundancy detection in a filtering environment to be an easier task than FSD in TDT. We would also expect the two tasks to be sensitive to different vocabulary patterns.However  , instead of truly detecting the first story as was the objective in the TDT program  , here we aim to improve detection performance at the expense of slightly delayed detection. The key idea for First Story Detection  , is that acting on formed 3NN clusters rather than individual documents is less likely to return false positives .There are multiple black-market sites where Pinterest repins or likes can be fraudulently obtained 6  , 7  , 8. This has led to the emergence of a variety of black-market sites where one can buy services that help to artificially boost the popularity of their content.A year-long pilot study was undertaken to define the problem clearly  , develop a test bed for research  , and evaluate the ability of current technologies to address the problem. The TDT tasks and evaluation approaches were developed by a joint effort between DARPA  , the University of Massachusetts  , Carnegie Mellon  , and Dragon Systems.In the Shop.com dataset  , however  , we have both the product price information and the quantity that a consumer purchased in each record. We adopt the consumer purchasing records dataset from Shop.com 1 for model evaluation  , because an important information source leveraged in our framework is the quantity of product that a consumer purchased in each transaction   , which is absent in many of the public datasets.In particular  , the culprit was single-digit OCR errors in the scanned article year. This turned out to be an artifact of OCRed metadata.The only interactions supported by Pinterest are repins  , comments on pins  , and likes. Each pin is characterized by a relative timestamp e.g.  , " posted 22 hours ago "   , " posted 5 days ago "   , a description freeform text  , and a link to the source of the content if it originated from a third-party website.Frames associated with 50 general events were constructed by hand. One study that is close in spirit to the TDT work was done by DeJong using frame-based objects called " sketchy scripts " 8.Askers post new questions and assign them to categories selected from a predefined taxonomy  , such as Pets > Dogs in the example shown in Figure 1 . Answers is a question-centric CQA site  , as opposed to more social-centric sites such as Quora.separating the wheat from the chaff  , is a very difficult problem. A key observation is that given the broad and growing number of topics in Quora  , identifying the most interesting and useful content  , i.e.According to a 2012 survey by the Pew Research Center   , Pinterest has attracted 15% of internet users to its virtual scrapbooking. Pinterest was launched on march of 2010 as an effort to compete in this new trend with an innovative and pioneering paradigm: a pinboard-style image sharing network for people with good taste Chafkin 2012.In shop.com dataset  , the short-head 20% involves 0.814% of popular products. The 80:20 rule 7  is commonly used to divide between long-tail products and popular ones.By lowering tdt  , RIP decreases the highest scores associated to t for a non local document. For the example described on Figure 3  , tdt 1 is 24.2  , while tpt 1 is 22.8.This simple assertion  , which we call the native language hypothesis  , is easily tested in the TDT story link detection task. We began with the hypothesis that if two stories originated in the same language  , it would be best to compare them in that language  , rather than translating them both into another language for comparison .The Merriam-Webster and Longman dictionaries offered different capabilities as repositories of data about lexical concepts. Three of the most accessible were the Merriam-Webster Pock& Dictionary MPD  , its larger sibling  , the Merriam-Webster Seventh Colegiate ~7 and the Longman Di@ionary of Contemporary English LDOCE.Exclusion of very short stories from similarity calculations tends to improve results. All TDT sources contain a number of very short documents that do not describe an event but are announcements  , teasers  , or other non-topical documents.The user narrows down the search to " software industry " 5 which reduces the results to 246. A search with " ICT industry growth in EU " presents 272 results from EconStor; the STW terms used in this search are " ICT industry " and " economic growth " .We then define the social repin network  , as the subgraph of links in the Pinterest network over which at least one social repin happens in our data. To measure this effect  , we first define the concept of a social repin  , which is a repin in which a user repins a pin of someone whom she follows.TDT tasks are evaluated as detection tasks. Both are based on the rates of two kinds of errors a detection system can make: misses  , in which the system gives a no answer where the correct answer is yes  , and false alarms  , in which the system gives a yes answer where the correct answer is no.This issue is partially due to the lack of automated mechanisms for generating reliable and up-to-date dataset metadata  , which hinders the retrieval  , reuse or interlinking of datasets. However  , as witnessed in the popular dataset registry DataHub 2   , dataset descriptions are often missing entirely  , or are outdated  , for instance describing unresponsive endpoints 7.Second  , does the presence of popular users correlate with high quality questions or answers ? First  , what triggers Quora users to form social ties ?Social collecting is indicative information organization  , use  , and sharing in a social web environment. In this work we introduced social collecting and the website Pinterest as an exemplar of the concept.The corpus of TDT 2004  , the TDT 5 test collection  , consists of 400 ,000 news stories from a number of sources and languages. By repeatedly merging the two most similar clusters in a new cluster  , a binary cluster tree is con- structed.New event detection shares some characteristics of online information filtering. We described overall system performance using a bootstrap method that produced performance distributions for the TDT corpus.Another difference between CCR and TDT is that CCR needs to make fine-grained citation-worthiness distinctions between relevant documents further. Different from above works  , we model entities' occurrences to capture bursty activities instead of words' occurrences.In contrast  , the task discussed in this paper is based on a batch evaluation at the sentence level. Finally  , TDT is concerned with story-level online evaluation  , where news stories are presented in a particular order and each one must be evaluated before the next is seen.We then ask whether time matters: i.e.  , Do social repins become more important as the user matures and conducts more activities on Pinterest ? However   , the striking result is that across all kinds of information seeking  , whether for the first pins in new boards/categories  , or for subsequent pins  , non-social means of finding information from other users dominate over social repins.The usage of blocks brings several benefits to RIP. This operation is then repeated for tdt 5 and tpt 4 .Chen et al proposed an aging theory to improve the performance for event detection 3 . Moreover  , temporal evidence was also used in New Event Detection Task in TDT 3  , 4 .Overall  , we consider 1 ,084 ,816 reviews from 4 ,432 users in BeerAdvocate  , and 2 ,016 ,861 reviews from 4 ,584 users in RateBeer. For a similar reason  , we discard beers which are individual events in our setting that have been reviewed by fewer than 50 users.Finally we calculate the cosine similarity score 2 between the extracted phrase p and each retrieval document's title t j   , and keep the document with the highest score as the Wikitravel page for that city. for City Youngstown  , OH  , we get phrase " Youngstown Ohio travel guide " .Even though small  , this evaluation suggests that implementing against GERBIL does not lead to any overhead. Further developers were invited to complete the survey  , which is available at our project website .Therefore  , we might expect that the ability of social networks to provide access to new informationwould be important on Pinterest. A core function of interest-based and content-driven sites such as Pinterest is to enable users to find the information that suits their interests.If  , otherwise  , the later is true  , then there is a subset of categories with commercially-appealing content -and these are not the ones users pin the most. If the first hypothesis is true  , then the shoppers will be a subset of users who use Pinterest differently from the others  , either for selling or for buying content.This feature was not implemented at the time of the TDT submission  , and we cannot test this hypothesis since NIST has not published the labeled evaluation data yet. Our hypothesis is that results are slightly improved when excluding short documents  " noshort " .Workers are expected to answer product-related questions in a biased manner  , and in some cases post dummy questions that are immediately answered by other colluding workers. Finally  , " Q&A " involves posting and answering questions on social Q&A sites like Quora quora.com.Further developers were invited to complete the survey  , which is available at our project website . Moreover  , all developers reported they felt comfortable—4 points on average on a 5-point Likert scale between very uncomfortable 1 and very comfortable 5—implementing the annotator in GERBIL.Since only 25 events in the corpus were judged  , an evaluation methodology developed for the TDT study was used to expand the number of trials. For the detection task  , a miss occurs when the system fails to detect a new event  , and a false alarms occur when the system indicates a story contains a new event when it does not.Here  , the mechanism designer  , or site owner  , has a choice about how many of the received contributions to display  , i.e.  , how to reward the contributions with attention — he could choose to display all contributions for a particular task  , or display only the best few  , suppressing some of the poorer contributions. The first is in the context of attention rewards on user-generated content UGC based sites  , such as online Q&A forums like Quora or StackOverflow.Halloween " is a topic  , which is reported once per year  , thus  , each year's reports can be regarded as an event; for " Earthquake " and " Air Disaster "   , their events lists can be found from corresponding official websites. Because manually defining events are very subjective  , we use similar methods to define and label events just like the TDT project. "The available results by UMass and Dragon are also included for comparison  , according to their reports at the TDT workshoplO. The CMU results correspond to the modified GAC method described earher .To compensate for the fact that the number of off-topic stories is far greater than the number of on-topic stories  , and that the difference varies across topics  , the cost also includes a factor which depends on the prior probability of finding an on-topic story. Fiscus and Doddington 2 provide an excellent review of the TDT community's motivations in coming up with that cost function .There is ample research into how to reduce the error rates of OCRed text in a post-processing phase. While this makes it easier for scholars to use the archive  , it also denies them the possibility to investigate potential tool-induced bias.Since the majority of Quora profiles contain hundreds of posts  , to ensure that proper care is given to evaluating them  , we collected the judgements employing 19 students from our institutions. To evaluate the AOL and Health Q&A datasets  , we employed AMT master workers from the USA and collected 5 judgements for each of the profiles.The model which optimizes per-item scores without recency outperforms the model that fixes the per-item scores to be item popularity over all datasets. Clearly  , the recency only model is the second best and the improvements by the hybrid model over the recency model are significant for MAPCLICKS and BRIGHTKITE.Although that assumption was known to be incorrect  , it is used in the evaluation's official cost function. The TDT evaluation program assumes a constant for the probability that a story is on topic.In other words  , Pinterest users can be seen as performing a massive distributed human computation  , categorising images found on the Web onto an extremely coarse-grained global taxonomy of 32 categories. Thus  , even though each pinboard may be exclusive to a user  , the act of pinning implicitly categorises the image.The collection can be sorted by author  , title  , publication type  , or publication year. One of the prominent collections of AlgoViz is the bibliography of publications related to algorithm visualizations .We represent a document by a vector of categories  , in which each dimension corresponds to the confidence that the document belongs to a category. A text classifier similar to that used in 2 is applied to classify each Web document in D into predefined categories in KDDCUP 2005.These are augmented by a set of features drawn from the Pinner who published the image  , as various characteristics of the original Pinner  , such as her " taste " of images  , and how influential a user she is on Pinterest  , may affect repinnability of the image. The pin p is described by the set of image features I in Table 1  , which may be attributed to the content of the pinned image.Our results showed that a marginal increase in system effectiveness is achieved when lexical chain semantic representations were used in conjunction with proper noun syntactic representations. In this paper we investigated if improved FSD performance could be achieved when a composite document representation was used in this TDT task.These are the two Wikia encyclopedias with the largest number of articles evaluated by users regarding their quality. From the Wikia service  , we selected the encyclopedias Wookieepedia  , about the Star Wars universe  , and Muppet  , about the TV series " The Muppet Show " .Pinterest developed an image search platform and thus they showed that content recommendation powered by visual search improves user engagement 10. Several prominent companies have already deployed their own image retrieval solutions.26 To this end  , GERBIL implements a Java-based NIF 15 reader and writer module which enables loading arbitrary NIF document collections  , as well as the communication to NIF-based webservices. Moreover  , we capitalize upon the uptake of publicly available  , NIF based corpora over the last years 40  , 36.In this paper we investigated if improved FSD performance could be achieved when a composite document representation was used in this TDT task. Results from data fusion research have suggested that significant improvements in system effectiveness can be obtained by combining multiple index representations  , query formulations and search strategies.We use the centroid-based approach 23  since it is a popular scheme for compact clusters which are similar to the clusters we see in the AlgoViz DSN. Approaches such as point-based measures or cluster centroids are often used to assign newly arriving points to an existing cluster.The See category is overrepresented in the top 5  , whereas the Eat and Drink categories are underrepresented . In Table 6 we see the distribution of Wikitravel categories over the top 5 retrieved suggestions and over all suggestions in the index.Systems in the TDT domain must be able to distinguish between events  , regardless of whether they are part of the same topic or not. For example  , the EgyptAir-990 crash is an event  , but not a topic  , and "airplane accidents" is a topic but not an event.Here the results were stronger. We also asked the assessors to compare the generated clusters with the TDT-2 topics and indicate if they agreed.The second is repinning   , or copying an existing pin on Pinterest. The first way is pinning  , which imports an image from an external URL.First  , the organization itself can create an account  , set up Pinboards  , and add pins. Institutional content can be collected on Pinterest in two ways.Note that in all the results reported  , mentions that contain NIL or empty ground truth entities are discarded before the evaluation; this decision is taken as well in Gerbil version 1.1.4. Let M * be the ground truth entity annotations associated with a given set of mentions X.Examples include Pinterest boards  , blogs  , and even collections of tweets. A second category refers to content that primarily exists and is curated online.A similar approach of comparing the features of a frame locally with its immediate predecessors works well in identifying shot boundaries in video streams 27 . Thus  , unlike the previous work on TDT  , a new node does not need to and cannot be compared to all its ancestors  , but has to be compared to its immediate parent or an immediate sequence of ancestors as it is they are causally closest to the current node.The second source of information is trade-level data for over 8000 publically traded companies on the NYSE  , AMEX and NASDAQ exchanges. 6 6 We do not consider the many important news stories that appear " after the bell  , " focusing here only on stories for which we have trading data.RIP reactively adjusts these thresholds using the activity of the local users to determine which documents and postings are replicated. For each term t  , a site maintains two replication thresholds  , expressed in partial score values: the document replication threshold tdt and the postings replication threshold tpt.As stated before  , Pinterest is all about pins  , thus our first analysis focuses on the activity of the users. We calculate the complementary cumulative distribution function CCDF of boards and pins per user by gender Figure 2 and report that  , although the distribution of boards does not vary greatly by gender  , females tend to catalog relatively more pins thus being more active in the network.In both datasets TSA significantly outperformed the baselines. The results using the WS-353 and Mturk dataset can be seen in Table 3.As we described in §2 and §3.1.3  , we can use a binary classifier to compute the probability of Pinterest identities to misbehave in the future. We compute the probability of Pinterest identities to misbehave in the future in two ways: first  , we only use intra-domain reputation signals  , and then we use both intra-domain and inter-domain reputation signals.One system also ignores individual user preferences  , while the other tries to take those preferences into account when ranking suggestions. We extracted a larger number of suggestions from Wikitravel pages on cities and towns in the US and created two systems that generate geographically independent rankings.Figure 5shows the coverage vs. the level of Figure 5: Coverage of the curated set for a given purity level for identities curated using all signals  , only Pinterest signals  , and identities curated at random. Coverage: To estimate and compare the coverage for a given target purity level  , we apply the two classifiers on the test data and we rank all the Pinterest identities according to the probabilities returned by the classifier.Figure 8top left shows the accuracy of the classifier for the AlgoViz Fall 2009 dataset. Figure 8 and Figure 9show the experimental results for the two DSNs.Thus we do not discuss the effects of classification accuracy to NED performance in the paper. Since the class labels of topic-off stories are not given in TDT datasets  , we cannot give the classification accuracy here.29  proposed GERBIL - General Entity Annotator Benchmark  , an easy-to-use platform for the agile comparison of annotators using multiple data sets and uniform measuring approaches. To avoid this problem  , the authors of Uzbeck et al.The sudden rise and then gradual fall of the stories is characteristic of this type of event. For example  , Fig- ure 5shows how many stories appeared per day in the TDT corpus for the Oklahoma City bombing event.Let the distribution of scores of off-topic non-relevant documents be given by px|nrel and let the distribution of scores of on-target relevant documents be given by px|rel. Consider the scores produced by a TDT system for each document .The training set contains the chronologically first 44 shows 1157 stories; the test set contains the chronologically last 18 shows 474 stories. The training and test data are extracted from the PRI subset of the TDT-4 cor- pus 5.It is  , however  , the most popular incremental clustering algorithm as can be seen from its popularity in the event detection domain -see TDT. The Single-Pass method also suffers from this disadvantage  , as well as from being order dependant and from having a tendency to produce large clusters Rasmussen  , 92.Using a Mandarin text document as a query  , we obtained a monolingual baseline mean average precision mAP of 0.701 for word-based indexing and 0.762 for character-bigram indexing. Retrieval Performance: We indexed the ASR transcriptions of the TDT-2 Mandarin audio using the HAIRCUT system.Thus  , the collection is a means of curating but also supporting the re-finding of content that has been encountered online. the person who uploaded a photo that has been pinned on Pinterest may hold the original and may host it elsewhere online  , these would be difficult to obtain by the user if their collection was lost even if simply because it's hard to remember what's in the collection .UMass also uses TDIDF and single link; all their TFIDF statistics are generated incrementally. Except for a set of slides at the TDT-2001 meeting 6  , no information about their system is available.Finally we would like to mention that our method is completely unsupervised  , in contrast to many TDT systems which tune their parameters over a training dataset from an earlier TDT run. After the build-up period  , the average time to process a document stabilized around 60 ms per document for K = 100 the residual growth is due to the increasing number of stories .moviepilot provides its users with personalized movie recommendations based on their previous ratings. In order to empirically estimate the magic barrier  , a user study on the real-life commercial movie recommendation community moviepilot 4 was performed.Step i uses the CKAN API to extract dataset metadata for datasets part of the LOD-Cloud group in DataHub. The main steps shown in Figure 1are the following: i dataset metadata extraction from DataHub; ii resource type and instance extraction; iii entity and topic extraction; iv topic filtering and ranking; and v dataset profile representation.So  , an event should be stated in at least one declarative independent clause. In the task definition of TDT  , a story is defined as " a topically cohesive segment of news that includes two or more declarative independent clauses about a single event " 1.The main steps shown in Figure 1are the following: i dataset metadata extraction from DataHub; ii resource type and instance extraction; iii entity and topic extraction; iv topic filtering and ranking; and v dataset profile representation. In this section  , we provide an overview of the processing steps for generating structured dataset profiles.We previously considered BeerAdvocate and RateBeer data in 28   , though not in the context of recommendation. In principle we obtain the complete set of reviews from each of these sources; data in each of our corpora spans at least 10 years.Another example is the LinkedGeoData project 4 which provides Linked Data about any circular and rectangular area on Earth 4. Example 1 illustrates that such cases are possible in practice.With both the ESA index and the proposed selectioncentric context language model pw|s  , c  , we can compute a selection-centric context semantic vector Vs  , c based on the centroid of the semantic vector of each term. To assess the quality of our ESA index   , we apply it to compute word relatedness on the widelyaccepted WS-353 benchmark dataset 12  , which contains 353 word pairs  , and our experiments show a Spearman's rank correlation of 0.735  , which is consistent to the previously reported numbers 16  , 17.We conclude with a discussion of the current state of GERBIL and a presentation of future work. We then present an evaluation of the framework that aims to quantify the effort necessary to include novel annotators and datasets to the framework.Considering the large amount of resources per dataset  , we investigate samplebased strategies as follows: SPARQL endpoint from DataHub in step i  , step ii extracts resource types and instances via SPARQL queries 5 that conform to the definition of resource types and instances in Section 2.Pinterest is a photo sharing website that allows users to store and categorise images. 4 This setting has been changed in April 2013.The level of influence of a user is merely the activity of other users directed towards that user  , i.e.  , the number of repins and likes received by that user for her pins. To quantify the level of user activity on Pinterest  , we employ three different measures: the numbers of boards created  , pins made including repins of other users' pins  , and likes of others' pins.We will do that by using the topic clusters generated by TDT systems. We are now looking at the impact of completely off-topic stories.We present a principled method to create additional datasets  , as opposed to the WS-353 benchmark where the word pairs were extracted manually. As an effort to provide additional evaluation data in this problem domain  , we created a new dataset 1 to further evaluate our results upon.First  , wherever possible  , Citebase links each reference cited by a given article to the full-text of the article that it cites if it is in the database. Citation-navigation provides Web-links over the existing author-generated references.Primarily a user-service  , Citebase provides a Web site that allows users to perform a meta-search title  , author etc. It was shown tasks can be accomplished efficiently with Citebase regardless of the background of the user. "However  , as witnessed in the popular dataset registry DataHub 2   , dataset descriptions are often missing entirely  , or are outdated  , for instance describing unresponsive endpoints 7. To facilitate search and reuse of existing datasets  , descriptive and reliable metadata is required.The " Open Knowledge Extraction " challenge at ESWC 7 and frameworks such as GERBIL 28 are good systems to validate our approach. identification of locations  , actors  , times at hand.The CMU results are depicted by the solid lines  , which show better performance at the high precision area. We used the DET software provided in the TDT project to generate the DET curves  , and converted each data point a pair of miss/false-alarm values in these DET curves to the corresponding recall and precision values noninterpolated  to obtain the recall-precision curves.Our goal  , on the other hand  , is to identify as many events in documents as possible  , and to use the identified events for calculating document similarity. In contrast to our work  , however  , TDT systems try to identify a main event that can be associated with documents.This year we experimented with the Wikitravel suggestion categories for buying  , doing  , drinking  , eating and seeing. Per geographic context the ranked suggestions are filtered on location.Per geographic context the ranked suggestions are filtered on location. We extracted a larger number of suggestions from Wikitravel pages on cities and towns in the US and created two systems that generate geographically independent rankings.Table 3 presents the accuracy comparison of static QAC  , Filtering QAC  , and adaQAC-Batch. We set TDT = 0.9 and TP = 1.If the query only contains one term  , then the replication operation is trivial  , and the algorithm determines that tdt 1 should be w. However  , if the query contains several terms  , then the algorithm has to decide whether it should replicate documents or posting lists. t |q| .We excluded a few topics for which assessment was terminated due to time constraints before adequate exhaustiveness was achieved as determined by the Linguistic Data Consortium. We evaluated this system using topics from the TDT-5 collection for which there are at least 20 known relevant documents in the evaluation epoch.While it is difficult to prove causal relationships  , our data analysis shows strong correlative relationships between Quora's internal structures and user behavior. We find that all three of its internal graphs  , a user-topic follow graph  , a userto-user social graph  , and a related question graph  , serve complementary roles in improving effective content discovery on Quora.This approach was introduced in 25 in 2008 and is based on different facts like prior probabilities  , context relatedness and quality  , which are then combined and tuned using a classifier. 7 They provide the source code for their approach as well as a webservice 8 which is available in GERBIL.1b we examine the entire history of activities of all users  , as they " age " in Pinterest by accumulating more activities. In Fig.In this section  , we introduce Quora  , using Stack Overflow as a basis for comparison. Quora is a question and answer site with a fully integrated social network connecting its users.If suggestions from outside the context cities are geographically irrelevant  , we should focus on finding other sources for suggestions in those cities where few are provided on Wikitravel. Is there a relation between the number of suggestions available in the context city and the number of suggestions that are geographically relevant ?Pinterest is a pinboard-style image sharing social network  , where everything is about photos and videos. Chafkin 2012.In that task  , a system is expected to monitor a stream of stories on a particular topic and extract sentences that discuss new developments within the topic. The one task within TDT that most closely resembles this work is " new information detection " 5.We tried to follow crawler-etiquette defined in Quora's robots.txt. We gathered our Quora dataset through web-based crawls between August and early September 2012.GERBIL can be used with systems and datasets from any domain. compared more than 15 systems on 20 different datasets.We use GDELT  , currently the largest global event catalog  , to automatically discover relevant events with high MSM coverage. Mainstream Media Collection.The results of the performance for the TSA algorithm with cross correlation distance function over WS-353 are presented in Table 8. The correlation of such words  , such as " Mars " and " water " in 1900 should be weighted differently from the correlation they exhibit in 2008  , when NASA images suggested the presence of water on Mars.The poor agreement between assessors on what constitutes a topic is not very surprising  , as debates on what topic means have occurred throughout the TDT research project. Only twenty of the 146 were not judged to be a single topic by the majority of assessors  , and of these twenty there were only three where the assessors unanimously agreed.All the rest are long-tail prod- ucts. In shop.com dataset  , the short-head 20% involves 0.814% of popular products.The National Institute of Informatics  , Japan proposed a task that is similar to the Story Link Detection at the TDT evaluation  , and which involves identifying whether two blog posts discuss the same topic. What this deeper context may add to explicit and implicit search is touched on. We evaluate Section 4 the probabilistic model alongside state-of-the-art CF approaches  , including popularity based  , neighbourhood  , and latent factor models using household rating data from MoviePilot 1 .  We then use this model to derive a framework for group recommendation Section 3.2 that  , unlike previous work—which focuses on merging recommendations computed for individual users—uses the principles of information matching in order to compute the probabilities of items' relevance to a group  , while taking the entirety of the group into consideration.To simplify the problem slightly for the pilot study  , we generally ignored issues of degraded text coming from speech recordings  , and used written newswire sources and human-transcribed stories from broadcast news. The goals of creating the corpus and evaluation methodology were two-fold: 1 to make strides toward a solid definition of " event " as outlined in Section 2.1  , and 2 to evaluate how well " state of the art " approaches could address the TDT tasks.Nevertheless  , it is interesting to note that the proposed method can detect this incident and treats it as an isolated event i.e.  , e 14  in the evolution graph. This is probably the reason that TDT annotators included the documents in the topic.for all selected LinkedGeoData classes. We compute the Morishita and the Moran indexes for all spatial features  , i.e.A publicly available dataset periodically released by Stack Overflow  , and a dataset crawled  from Quora that contains multiple groups of data on users  , questions   , topics and votes. Our analysis relies on two key datasets.We examine the relation between the length of a sequence and the duration measured by the number of events that the sequence spends at each stage. The length of sequence can be of great interest in many datasets; for example  , it represents how actively a user enters reviews on BeerAdvocate and RateBeer  , how popular a phrase is in NIFTY  , or the skill of a player on Wikispeedia.If our service returns a NIL annotation  , GERBIL treats it like " not annotated " . We note that the GERBIL version that we use does not consider NIL annotations when computing the F1  , recall and precision values.showing the proportion of social and non-social repins vs. the " maturity " of user on Pinterest  , as measured by the number of repins made since joining. Cosine similarity was computed at random time points between vectors of pins of users and 1 pins of their friends marked as " friends "  andThis knowledge might prove its potential in a task such as linking user-generated data e.g.  , Pinterest pins to online webshops 7. Given such collection  , the goal is to find how these two language idioms colloquial vs. formal relate to each other.We find that users who are influential  , measured by repins  , tend to have lower copy ratios. 6fshows that this result extends to measures of influence on Pinterest.However  , the standard TDT cost function assumes that the ratio of relevant to non-relevant documents is uniform across queries 4. They represent the tradeoff between these costs with a DET curve.On the other hand  , the first rank of the Model-Text suggestion is the WikiTravel page of the state of Michigan that is judged as a relevant suggestion. It is so interesting to know that the Model-Anchor suggests the WikiTravel page of the Kalamazoo city that is judged as an irrelevant suggestion in the first rank.One explanation is that the 'best' products tend to be ones that require expertise to enjoy  , while novice users may be unable to appreciate them fully. Recall that in Figure 1we examined the same relationship on RateBeer data in more detail.To ensure the practicability and convenience of the GER- BIL framework  , we investigated the effort needed to use GERBIL for the evaluation of novel annotators. The results of this experiment are shown in Figure 4.Each of the images in the datasets are thus implicitly labeled by users with one of the 32 categories. We recall that a Pinterest user may have several different pinboards each assigned to one of 32 globally defined categories.The task is to link users' pins posted on the social media site Pinterest .com given by their textual description to a set of relevant webshops where the user might search for or buy the " pinned " product. To evaluate the utility of the new multi-idiomatic topic model  , we perform the same retrieval task as proposed in 7.Previous work 8  , 9  , 24 studied effectively finding previously answered questions that are relevant to a new question asked by a user. Answers and Quora.Because of this UI design decision in Pinterest  , we expect that a user's choice of the Pinterest category to associate with a pin is made independently of other users or the system itself. Even the owner of the board is only shown the category on the page for editing a board's details not normally seen when the owner views her board.Both lines increase smoothly without gaps  , suggesting that Quora did not reset qid in the past and the questions we crawled are not biased to a certain time period. We plot two lines for Quora  , a black dashed line for the total number of questions estimated by qid  , and the blue dashed line is the number of questions we crawled from each month.§3 gives a brief background of Pinterest and our dataset. §2 presents related work.In this section  , inspired by KDDCUP 2005  , we give a stringent definition of the QC problem. The difficulties include short and ambiguous queries and the lack of training data.Microsoft has a supercategory Computer and video game companies with the same head lemma. The category Microsoft has a homonymous page  , categorized under Companies listed on NASDAQ which has the head lemma companies.The second challenge occurs because Pinterest does not have a public API for gathering pins with timestamps in bulk. Our study necessitates highresolution timestamps  , thus we need to crawl pins while the timestamps are displayed in hours.That possibly returns to the different nature of the usage of both sites. This is interestingly in contrast with 20  , which found that the most frequent sites on Pinterest had low Alexa Global Ranking.As illustrated in Figure 3  , a similar pattern is observed for the evaluation by the TBG metric. However  , the Clarksville is not mentioned in the anchor text of the Nashville wikitravel page  , and it is reasonable that it is not included in the top-5 ranking of the Model-Anchor.Detailed results are also provided 1112 . Results of the experiments run on the Gerbil platform are shown in Table 2.We wish to make it clear that the corpus and evaluation methodology that were devised in the TDT study were a joint effort by four groups. Detailed results of that study are reported elsewhere3; this paper presents advances in our understanding of the problem after the end of the pilot study.We use two AlgoViz DSNs created from log data captured in Fall 2009 and Spring 2010. Density 20 for a network with edges E and vertices V is defined as:Given the finding that social links are not critical for identifying pins  , the most critical activity on Pinterest  , it is puzzling that its social network is counted amongst the fastest growing across all platforms 2 . We find evidence the Pinterest social network is useful for bonding and interaction.We introduce the Celestial tool 4 a cache/gateway for the OAI-PMH and Citebase 5 an end-user service that applies citation-analysis to existing OAI-PMH compliant eprint archives. The first part of this paper provides background about the OAI-PMH.This result confirms that the number of votes is the dominating feature for selecting best answers. We see that for 85% of the questions  , Quora's best answers also ranked the highest in votes  , and for 96% of the questions   , the best answers from Quora are among the top-2 most votes.Table 2 shows the statistics of our test corpora. All data sets are integrated in GERBIL and strongly differ in document length and amount of entities per docu- ment.Unfortunately we could not do the same for question page views  , because Quora only reveals the identity of users who answer questions   , but not those who browse each question. We verify this intuition by examining for each question the percentage of answers that came from followers of the question's topics.The community counts its users in hundreds of thousands  , ratings in dozens of millions and movies in tens of thousands. moviepilot provides its users with personalized movie recommendations based on their previous ratings.We study the extent to which follow acts are reciprocated and become bidirectional. On some services like Pinterest  , users follow others unilaterally  , creating directional links.We also have not considered overlapping topics currently for simplicity. We suggest it unnecessary to consider complicated hierarchies in the context of the state-of-the-art TDT techniques.We have built and described an evaluation corpus based on 22 topics from TDT news stories. We have described it in a way that it is possible to carry out laboratory evaluations of effectiveness   , avoiding costly user evaluations at every step of the process .For this context  , the Model- Anchor retrieves the disambiguation page of the wikitravel for Clarksville cities. As a matter of fact  , there are based on the only anchor text of the pages in the tiny aggregators sub collection.Services already exist to support the backup of Pinterest boards  , tweets and blogs  , and this can take advantage of the curation that has already occurred online. Collections of other-generated content are vulnerable to loss  , and so implications chiefly relate to supporting their backup.Moreover   , partial results are not considered within the evaluation. To address this problem  , we aim to develop/implement novel measures into GERBIL that make use of scores e.g.  , Mean Reciprocal Rank.can observe the tendency that the property sets convey more information than type sets. An exception is the Datahub data set D  , where the distribution of resources in type sets and property sets seems comparable.The TDT-2 corpus has 192 topics with known relevance judgments. Though not matching our wish list  , the TDT-2 corpus has some desirable properties.Figure 4presents the computation of the thresholds for the query " t4  , t5 " . We adapt RIP to apply the replication thresholds tdt and tpt on blocks of documents and postings instead of single el- ements.As Quora and its repository of data continues to grow in size and mature  , our results suggest that these unique features will help Quora users continue find valuable and relevant content. Finally  , the userto-user social network attracts views  , and leverages social ties to encourage votes and additional high quality answers.The input data was 50 TDT English newswire clusters and each cluster contained 10 documents. The first experiment aimed to test the importance of term order in a document.Community question and answer sites provide a unique and invaluable service to its users. As Quora and its repository of data continues to grow in size and mature  , our results suggest that these unique features will help Quora users continue find valuable and relevant content.Due to the community effort behind GERBIL  , we could raise the number of published annotators from 5 to 9. Since GERBIL is based on the BAT-framework  , annotators of this framework can be added to GERBIL easily.GERBIL abides by a service-oriented architecture driven by the model-view-controller pattern see Figure 1. The output of experiments as well as descriptions of the various components are stored in a serverless database for fastWhen the user searches " Peso " as his query  , we could correctly interpret the intent as Philippine Peso instead of Mexico Peso. For example  , when the user types " P " in a search box  , we can accurately suggest query " Peso " or " Philippine Peso " instead of " Pinterest " or " Paypal "   , which are more likely to be suggested by current search engines.Our survey comprised five developers with expert-level programming skills in Java. To achieve this goal  , we surveyed the workload necessary to implement a novel annotator into GERBIL compared to the implementation into previous diverse frameworks.The Spoken Document transcriptions used in our experiments are taken from the TDT-2 version 3 CD-ROMs. The recognition performance of this transcription is shown in Figure 1Identities with black-market association: In this dataset  , we collect information about identities associated with a black-market service. 11 Out of the 1.7M Pinterest identities  , we found that 74 ,549 have been suspended.The primary threshold was not regarded as a trainable parameter for this experiment but rather as a control of the operating point to investigate different styles of clustering. All adjustable parameters of our system except the primary threshold were trained on the TDT-2 corpus.The study was performed through a webpage mimicking the look-and-feel of the moviepilot website  , on this page users were presented with a random selection of movies they had previously rated  , with the ratings withheld. The purpose was withheld so to not affect the outcome.In total we have 107 ,372 untrustworthy identities the negative examples and slightly less than 1.6 million Pinterest identities that are not untrustworthy the positive examples. For identities that post malicious pins  , we consider the top 17 ,000 which corresponds to the 1% most untrustworthy Pinterest identities identities to be untrustworthy  , as ranked by their fraction of malicious pins.The incremental TF-IDF model as described in section 3.2 is built from previous stories plus all stories in the deferral period. The look-ahead that a TDT system is allowed to see also called the deferral period can be 1  , 10  , or 100 files 6 .For the implementation we use EconStor and an RDF dump file of Econstor. As a result  , an author's profile is enriched with additional information found in the cluster.The user selects an article from the result set and its thesaurus-related metadata are retrieved to further support her refine the results Fig. 1  , " EconStor Results " .iii Ground truth information about untrustworthy identities in Pinterest   , which enables us to evaluate how well we can reason about trustworthiness of identities in the target domain. We leverage these signals to reason about the trustworthiness of the matching identities in Pinterest.In this section  , we present a thorough analysis of the Pinterest users description by gender related to the context. 2007; Veltman 2006 .We begin by briefly describing Pinterest  , our terminology  , and the dataset used in the rest of this paper: Pinterest is a photo sharing website that allows users to organise thematic collections of images. We ask what is the probability P repin_catp  , i1  , allows users to find research papers stored in open access  , OAI-compliant archives -currently arXiv http://arxiv.org/  , CogPrints http://cogprints.soton.ac.uk/ and BioMed Central http://www.biomedcentral.com/. Citebase  , more fully described by Hitchcock et al.This technological affordance 3 makes it easy to reuse and share images. Pinterest users organize objects by selecting an image from the webpage where the object exists using a browser bookmarklet or by uploading an image from their computer.As an example  , a search performed in OAIster for " double-well Duffing oscillator " retrieves two records  , exactly the same  , but one was harvested from the arXiv.org Eprint Archive repository an original repository and one harvested from the CiteBase repository an aggregator. Because of this  , we have records in our system from original repositories and from aggregator providers collecting original repositories.Since this paper focuses on the recommendation in ecommerce sites  , we collect a dataset from a typical e-commerce website  , shop.com  , for our experiments. The method is denoted as SV Dmatrix.The sessions are the nodes and an edge between two sessions indicate they share k common pages. We use two AlgoViz DSNs created from log data captured in Fall 2009 and Spring 2010.Recency is clearly present in MAPCLICKS and BRIGHTKITE  , and absent from SHAKESPEARE and YES. iii SHAKESPEARE iv YES Figure 6: Normalized hit ratio as a function of cache size for four different datasets.Users may find out about the new pin externally to the website and upload the pin themselves. 2013; Zhong  , Karamshuk  , and Sastry 2015  have found that creating new pins or repins  is by far the most common activity on Pinterest  , and presents a quintessential information seeking activity.We find evidence the Pinterest social network is useful for bonding and interaction. In this section  , we address the issue of which users make social links  , and why it is important .We also asked the assessors to compare the generated clusters with the TDT-2 topics and indicate if they agreed. Each assessor felt that overall our system was good at finding what they considered to be a "topic"  , but they did not agree among themselves on what that meant.Nevertheless  , in TDT domain  , we need to discriminate documents with regard to topics rather than queries. The basic idea is that the fewer documents a term appears in  , the more important the term is in discrimination of documents relevant or not relevant to a query containing the term.For this  , we consider the task of curating identities in the target domain Pinterest. The goal of this section is to show the benefits of inter-domain trust transfer in a practical scenario.The online version of GERBIL can be accessed at http://gerbil.aksw.org/gerbil. More information can be found at our project webpage http:// gerbil.aksw.org and at the code repository page https: //github.com/AKSW/gerbil.In contrast  , the naturally evolving nature of discussion threads and the need for fine-granularity segment boundary identification make the problem of topic segmentation significantly harder than the new-event detection problem addressed by the TDT technologies. For example  , the method proposed in 5 is based on an incremental TF-IDF model  , and it involves segmentation of documents to locate all stories on a previously unseen new event in a stream of news stories.Citation-navigation provides Web-links over the existing author-generated references. As part of the development of Citebase we have looked at the relationship between citation impact  " how many times has this article been cited "  and web impact  " how many times has this article been read " .To adhere to ethical standards concerning incorporation of user data into research  , we decided to only use data that is publicly available – either as online profiles Quora  , Health Q&A  , or as datasets used in numerous other studies AOL. As a first data source  , we used the AOL query log collected between March  Ethics.The dataset integration and data preparation is done in two steps. We use similar configuration to index the Wikitravel dataset.User-Topic Graph: Quora users follow different topics  , and receive updates about questions under topics they follow. 1.To include further metadata  , annotator and corpus dimension properties link DataID 2 descriptions of the individual components. The six evaluation measures offered by GERBIL as well as the error count are expressed as qb:Measures.In Pinterest  , users pin photos or videos of interest to create theme-based image/video collections such as hobbies  , fashion  , events. That possibly returns to the different nature of the usage of both sites.A core function of interest-based and content-driven sites such as Pinterest is to enable users to find the information that suits their interests. But the difference between the two proportions is reduced as users become more experienced in the plat- formAll TDT sources contain a number of very short documents that do not describe an event but are announcements  , teasers  , or other non-topical documents. An explanation for this is that teasers often mention different events  , but according to the TDT labeling instructions they are not considered on-topic.Since each Quora user lists the topics she follows in her profile  , we estimate the number of followers by examining user profiles in our crawled dataset. Next  , we rank the topics by the number of followers.For the baseline system  , suggestions are ranked per user profile based on their positively rated examples and filtered on the geographic context. By estimating the Wikitravel category for the provided examples  , we created personalised category prior probabilities.The second question we ask is: how do users distribute their content across categories ? Our target population intentionally contains many avid Pinterest users  , which is reflected in the popularity of the design  , food  , and beauty categories.The source tree ST is the only structure that our XPath evaluation and incremental maintenance algorithms require. From the source tree we can see that both fragments F2 and F3 are stored in the same site S2  , the nasdaq site.In Quora  , the top 10 includes topics in various areas including technology  , food  , entertainment  , health  , etc. " We observed 56K topics in our dataset  , which is twice more than that of Stack Overflow  , even though Quora is smaller by 0   20   40   60   80   100   10 0 10 1 10 2 10 3 10 4 10 5 10Table 2lists the top 10 topics with most number of questions in each site.This may explain the relatively small absolute improvement of tLSA over LSA. However  , the words in the WS-353 dataset are relatively common  , and primarily related to static concepts  , such as " car " and " love " .Two users were connected only if they viewed at least 10 similar pages within a month. Figure 1shows DSNs based on AlgoViz log data for the months of September and October 2010 with a connection threshold of 10.Consider an image which has been introduced into Pinterest by an original pinner. Our goal is to make it easier to re-appropriate and re-categorise content for personal use.For example  , the method proposed in 5 is based on an incremental TF-IDF model  , and it involves segmentation of documents to locate all stories on a previously unseen new event in a stream of news stories. To detect the first story  , current TDT systems compare a new document with the past documents and make a decision regarding the novelty of the story based on the content-based similarity values.For the GALE project we trained segmentation models using data from multiple news programs from Al Arabiya and Al Jazeera Arabic  , and Phoenix Infonews Chinese. The lowest Cseg values for the full version of our system are 0.0593 for English and 0.0528 for Chinese  , compared with the 0.0810 and 0.0670 reported by the state of the art system in TDT-3 also shown on Figure 1c.By this method  , an input query is first mapped to an intermediate category  , and then a second mapping is applied to map the query from the intermediate category to the target category. The winning solution in the KDDCUP 2005 competition  , which won on all three evaluation metrics precision  , F1 and creativity  , relied on an innovative method to map queries to target categories.Consider the scores produced by a TDT system for each document . We can now look at the implications of having a constant P rel.A new evaluation metric is required for the hierarchical structure; the minimal cost metric described by Allan et al 1 is used. Within TDT a participant's cluster structure is evaluated by identifying the best cluster for each of the topics from a manually composed ground truth.We first describe the process of curating identities on Pinterest. For this  , we consider the task of curating identities in the target domain Pinterest.Overall  , our approach attains the best averaged F1 value of all systems. The corresponding GERBIL result sheet is available on the GERBIL website 4 and can be used to make comparisons to our approach in future evaluations.TDT-2 consists of a total of almost 84.000 documents from the year 1998  , drawn from newspapers  , radio news  , and television news in English  , Arabic and Mandarin. We use a subset of the TDT-2 benchmark dataset.There are a total of 37 solutions from 32 teams attending the competition. KDDCUP 2005 provides a test bed for the Web query classification problem.Each burst contains 10 new questions sent seconds apart  , and consistently produced 10 sequential qid's. To validate this statement  , we performed several small experiments where we added small bursts of new meaningful questions to Quora.Finally  , generated metadata information and OCRed text are integrated to support navigation and retrieval of content within scanned volumes. After the scanning and text recognition process  , the metadata generation system generates metadata describing the internal structure of the scanned volume and published articles contained within the volume.Our assessors were students whose instructions did not include a definition of the word "topic" nor explicit criteria for deciding what was or was not a topic. The poor agreement between assessors on what constitutes a topic is not very surprising  , as debates on what topic means have occurred throughout the TDT research project.The clustering results  , called as new topic candidates  , are compared with previous topics  , and then the results will show if they are really " new " or not. New coming stories are clustered into new topic candidates according to their pair-wise similarities  , which is similar to the process of Topic Detection in TDT.As a developing service Citebase often needs to completely re-harvest its metadata  , and using a local mirror avoids repeatedly making very large requests to source archives. The Celestial mirror is used within Southampton by Citebase Search.The report found that " Citebase can be used simply and reliably for resource discovery. This was used both to evaluate the outcomes of the project  , and to help guide the future direction of Citebase as an ongoing service.The largest qid from our crawled questions is 761030  , leading us to estimate that Quora had roughly 760K questions at the time of our crawl  , and our crawl covered roughly 58% of all questions. The result   , discussed below  , provides further support that this qid can be used as an estimate of total questions in the system.Since the class labels of topic-off stories are not given in TDT datasets  , we cannot give the classification accuracy here. Classification results are used for term reweighting in formula 11.They used a χ 2 test to identify days on which the number of occurrences of a given word or phrase exceed some empirically determined threshold  , and then generated timelines by grouping together contiguous sequences of such days. Working in the domain of news  , Swan and Jensen 24 automatically generated timelines from historic date-tagged news corpora TDT.BrightKite is a now defunct location-based social networking website www.brightkite.com where users could publicly check-in to various locations. BRIGHTKITE.The topics have been used in several evaluations and the story–topic assignments have been con£rmed by quality assurance cycles. As part of the TDT research program  , about 200 news topics were identi£ed in that period  , and all stories were marked as onor off-topic for every one of the topics.The reason for this is that the performance of the neighbourhood and latent factor models was close to 0 7 . Most notably  , we have only reported MAP scores for the MoviePilot data.Hence  , while keeping the implementation effort previously required to evaluate on a single dataset  , we allow developers to evaluate on currently  11 times more datasets. The evaluation of our framework by contributors suggests that adding an annotator to  GERBIL demands 1 to 2 hours of work.New coming stories are clustered into new topic candidates according to their pair-wise similarities  , which is similar to the process of Topic Detection in TDT. As pointed out in 9  , the similarity calculated equals to the inner product between topic vectors of cluster A and B  , so the topic similarity calculation is the same as the story similarity calculation.In total 68.7 million Pinterest users and 3.8 billion directed edges between them were obtained. To obtain the Pinterest social graph  , we used a snowball sampling technique  , starting to crawl from a seed set of 1.6 million users which we collected in advance.By going back to first principles we can derive a new cost function which will have a constant threshold. This is  , after all  , one of the goals of TDT.This step is necessary in order to make pins and tweets directly comparable: on Pinterest  , all pins fall into 1 of 33 pre-defined categories  , while tweets are freeform text. The next step in our methodology is to classify tweets from our target users with category labels.Because the time between two pins may be widely different across users  , we measure user age in terms of repin steps  , the number of re-pins made since joining Pinterest. We then ask whether time matters: i.e.  , Do social repins become more important as the user matures and conducts more activities on Pinterest ?We find that this lower dimension approximation of Pinterest has several desirable properties: First  , for a given image  , a remarkable ≈75% of repins tend to agree on the majority category  , although the level of agreement varies with the category. In other words  , Pinterest users can be seen as performing a massive distributed human computation  , categorising images found on the Web onto an extremely coarse-grained global taxonomy of 32 categories.One of the prominent collections of AlgoViz is the bibliography of publications related to algorithm visualizations . Two of the top-most topics in the September 2010 DSN include words related to AlgoViz bibliography entries i.e.  , biblio.We assume that a vast majority of the random Pinterest identities are indeed trustworthy  , and hence  , we do not consider all identities that posted a single blocked pin to be untrustworthy. There are 724 ,672 Pinterest identities with at least one blocked pin  , which includes 43% of all Pinterest identities.Images added on Pinterest are termed pins and can be created in two ways. Pinterest is a photo sharing website that allows users to store and categorise images.Clearly  , the recency only model is the second best and the improvements by the hybrid model over the recency model are significant for MAPCLICKS and BRIGHTKITE. For computational efficiency reasons  , we learn recency weights over the previous 200 positions only.Other work on news summarization  , including work that uses the TDT corpora  , focuses on single or multi-document summarization 25  , 34  , 12 of the stories  , without attempting to capture the changes over time. Maybury's work on event data 24 is different than this work because he was focused on events from simulations or application data rather than on events within news topics.The Do and Drink categories are the least liked while the Eat category is the highest rated. In the bottom half of Table 2we show rating statistics per Wikitravel category  , based on the estimated category per example.Our dataset is derived from a previous study 38  , and includes nearly all activities on Pinterest between Jan 3–21 2013. Dataset.The Pinterest website is a complex Javascript application  , so we leveraged PhantomJS to crawl the site. To address these challenges  , we built a distributed crawler that collected pins from the selected users every day.This is a difficult question to answer  , given Quora's own lack of transparency on its inner workings. So how does Quora succeed in directing the attention of its users to the appropriate content  , either to questions they are uniquely qualified to answer  , or to entertaining or informative answers of interest ?In order to determine the benefits of a close-knit structure  , we examine one of the most popular activities on the Pinterest network   , repinning. While these properties are expected to improve social interaction 18  , 26  , we ask whether the benefits of these structural properties are seen in the social interactions of the target network.For the mid-1990s events in the second TDT study  , systems had trouble treating the O. J. Simpson case or the investigation of the Oklahoma city bombing as a single event 11  , 13. The most significant problem in adapting TDT methods to historical texts is the difficulty of handling long-running topics.to the available blog post elements  , we conducted automatic indexing of posts based on the STW thesaurus 3 . This effectively brings blog posts at the same vocabulary level as publications from EconStor.In contrast  , this work looks both at inter-and intra-topic novelty detection ; in addition to determining whether two sentences cover the same topic  , we are concerned with identifying when a sentence contains new information about that topic. Most importantly  , the tasks of TDT are concerned with what can be called inter-topic or inter-event novelty detection  , where the concern is on whether two news stories cover the same event.With GERBIL  , we aim to push annotation system developers to better quality and wider use of their frameworks. In this paper  , we presented and evaluated GERBIL  , a platform for the evaluation of annotation frameworks.However  , there are several important differences that are unique to our problem domain: a Unlike the corpus of broadcast news audio used in TDT evaluations  , training and corporate videos can be quite heterogeneous  , b There is no notion of a story and the associated well-defined segment of expected relatively short duration. The problem we address bears the largest similarity to the TDT segmentation task.To ensure higher competitiveness   , the model is tuned among the 300 threshold value combinations in §2.4. In §2.4 we set up Filtering QAC with relevance scores  , by additionally filtering out all the suggested queries with certain dwell time thresholds TDT  and position thresholds TP  in the subsequent keystrokes in a composition.When viewing a cached full-text PDF  , Citebase overlays reference links within the document  , so a user can jump from viewing a full-text to the abstract page of a cited article. This provides a visual link between the citation and web impacts.In Section 7.2 we discuss our results in contrast to other works that are not publicly available. In Section 7.1 we directly compare the approaches on the basis of its results achieved with GERBIL.Despite a small number of registered users  , AlgoViz project leaders are interested in understanding the trends of its overall user base. These users are referred to as Anonymous users and have a default user ID of 0.This activity does not require the consent or knowledge of the organization  , and the context of social collecting allows reusing other's materials 3. Second  , Pinterest users can pin an organization's content to their personal pinboards.One area where none of the standards provided duced above was far from trivial. Some users are interested in highly unstructured text data OCRed from field journals  , or more conventional relational tables of data  , so BigSur does not require that these super-classes are used.The goal of LinkedGeoData is to add a spatial dimension to the Semantic Web. We have extended the ontology of LinkedGeoData by the appropriate classes and properties.While AGDISTIS has been in the source code of the BAT-Framework provided by a third-party after publication of Cornolti et al. Table 1compares the implemented annotation systems of GERBIL and the BAT-Framework.Dataset. Note that streams for synthetic data differs from NASDAQ data in terms of the lag and the missing update distributions.Recent work Zoghbi  , Vuli´cVuli´c  , and Moens 2013 verifies that the images posted to sites like Pinterest are an accurate reflection of a user's interest   , and can be used to recommend relevant products. 2013 that focus on quantifying and analyzing Pinterest user behavior.For instance  , even with Pinterest restrictions for social communication  , we show that such communication still happens in the form of lightweight interactions such as likes and repins. In this study we report significant findings.For BRIGHTKITE  , PDP captures essentially all of the likelihood. In all cases  , personalization captures over 75% of the available likelihood. The DjVu XML file presents logical structures of the OCRed text. We choose the DjVu XML 2 file as the main input of the metadata generation system for several reasons:  The DjVu XML file contains full OCRed text.The TDT1 corpus  , developed by the researchers in the TDT Pilot Research Project  , was the first benchmark evaluation corpus for TDT research. We use both corpora as they are and set the evaluation conditions as close as possible to those used in the TDT1 and TDT3 benchmark evaluations to make our results comparable to the published results on these evaluations.Moreover   , we report that females make more use of this kind of interaction and are more active in terms of content generation . For instance  , even with Pinterest restrictions for social communication  , we show that such communication still happens in the form of lightweight interactions such as likes and repins.Second  , do super users get more votes  , and do these votes mainly come from their followers ? First  , do user votes have a large impact on the ranking of answers in Quora ?However  , the main drawback of adapting these techniques for the new hot bursty events detection problem is that they require many parameters and it is very difficult to find an effective way to tune these parameters. The related works include TDT 2  , 3  , 14  , 18  , 21  , 26  , 27  , text mining 9  , 13  , 14  , 17  , 19  , 20  , 22   , and visualiza- tion 7  , 11  , 24 .Candidate Term Selection. 2014;Stepchenkova 2014—see our data release for full list— which we then expand in a snowball fashion as we did for themes/taxonomies in GDELT.Next  , we experiment with the extent that the algorithms can produce quality recommendations for groups  , using the MoviePilot data. Recommendations to Groups.We employ the single-pass clustering algorithm incremental clustering which is tested in TDT 20 as the baseline algorithm. Thread detection is in fact the task of grouping the messages in the text stream into different groups and each group represents a topic.As we argue next  , BeerAdvocate and RateBeer exhibit multiple features that make them suitable for the analysis of linguistic change. 1 In both communities users provide ratings accompanied by short textual reviews of more than 60 ,000 different types of beer.These collections are hosted online and largely comprise other-generated content. Examples include Pinterest boards  , blogs  , and even collections of tweets.We then give details on the key Quora graph structures that connect different components together. In this section  , we introduce Quora  , using Stack Overflow as a basis for comparison.The transcription set used is designated as1 on this release and was generated by NIST using the BBN BYBLOS Rough'N'Ready transcription system using a dynamically updated rolling language model. The Spoken Document transcriptions used in our experiments are taken from the TDT-2 version 3 CD-ROMs.To systematically identify all the GDELT themes and taxonomies that are related to climate change we first built the co-occurrence graph among them. To locate the URLs corresponding to news articles relevant to climate change  , we rely on GDELT themes and taxonomies  , which are topical tags that automatically annotate events.Although it is a continuous timeline  , we split it into two segments to follow the traffic trends seen in Fall and Spring semesters. We begin by constructing DSNs based on AlgoViz log data from Fall 2009 August 1 to December 31 and Spring 2010 January 1 to May 31.Quora makes visible the list of upvoters  , but hides downvoters. In terms of votes  , both Quora and Stack Overflow allow users to upvote and downvote answers.As another example  , in case the program can not recognize the volume and issue number due to OCR error  , such as " IV " was OCRed as " it "   , the program will use the previous or the following title page information  , if available  , to construct the current volume or issue metadata. For instance  , in order to tolerate OCR errors in volume and issue number line  , we set the Levenshtein Distance20 between an examined string and the target " volume " and " issue " keywords as a parameter and choose the optimal value based on experiments.In addition  , the training data must be found online because   , in general  , labeled training data for query classification are very difficult to obtain. In this section  , inspired by KDDCUP 2005  , we give a stringent definition of the QC problem.This approach is similar to solutions for the TDT First Story Detection problem. One approach to novelty/redundancy detection is to cluster all previously delivered documents  , and then to measure the redundancy of the current document by its distance to each cluster.One of the data sets contains 111 sample queries together with the category information. In this paper  , we use the data sets from the KDDCUP 2005 competition which is available on the Web 1 .As regards the 25 events that were prominently covered by both media  , 60% were primarily triggered by government/inter-governmental agencies e.g. " Disasters have been observed to be a prominent subject in international news articles collected by GDELT Kwak and An 2014.Very few text analysis tools can  , for example  , deal with different confidence values in their input  , apart from the extensive standardization these would require for the input/output formats and interpretation of these values. For task T4 not in the table  , the use of OCRed texts in other tools  , our findings are also mainly negative.Note that this definition differs from the concept of event in AI and dialog systems. In TDT  , an event is usually understood as " some unique thing that happens at some point in time " 3.The TDT-2 corpus contains text transcripts of broadcast news in English and Chinese spanning from January 1  , 1998 to June 30  , 1998. There should be a significant overlap between these two sets  , and there should be some correlation between features extracted by our system and human assigned descriptors.Tested on the most recent 20% of the annotated sets  , the Cseg values range between 0.034 and 0.067 for Arabic and between 0.042 and 0.061 for Chinese. In addition to the features used on the TDT data  , we included features based on speaker and video shot changes  , but they did not yield consistent performance improvement.Answers  , Stack- Overflow or Quora. Crowdsourcing  , where a problem or task is broadcast to a crowd of potential contributors for solution  , is a rapidly growing online phenomenon being used in applications ranging from seeking solutions to challenging projects such as in Innocentive or TopCoder  , all the way to crowdsourced content such as on online Q&A forums like Y!Still  , the results also show that a better clustering of tasks as performed by greedy clustering leads to higher hit ratios  , thus suggesting that clustering alone can already be beneficial for improving the scheduling of link discovery tasks. The table shows clearly that while the greedy and na¨ıvena¨ıve approach achieve similar runtimes on the LinkedGeoData fragment with 1 ,000 resources  , the greedy clustering approach is orders of magnitude slower than the na¨ıvena¨ıve approach in all other cases.Table 4 : Performance improvement resulting from incrementally adding our linguistic change features to the 'activity' model for RateBeer  , our 'test community'. For all sites and w  , the full model significantly improves over the activity-only model according to a paired Wilcoxon signed rank test on the F1 scores p < 0.001.Quora is a question and answer site where users can ask and answer questions and comment on or vote for existing answers. Quora.On the other hand  , Model-Text provides the wikitravel page of the " Nashville " city in the state of Tennessee as the 1st suggestion in the ranking. For this context  , the Model- Anchor retrieves the disambiguation page of the wikitravel for Clarksville cities.These collection are indexed using Lucene SOLR 4.0 and we use BM25 as the retrieval model. Six collections  , relevant to the assignment about television and film personalities  , from various archives were indexed: 1 a television program collection containing 0.5M metadata records; 2 a photo collection with 20K photos of people working at television studio; 3 a wiki dedicated to actors and presenters 20K pages; 4 25K television guides that are scanned and OCRed; 5 scanned and OCRed newspapers between 1900 and 1995 6M articles; and 6 digital newspapers between 1995 and 2010 1M articles.Figure 1shows DSNs based on AlgoViz log data for the months of September and October 2010 with a connection threshold of 10. A connection threshold of size k for an edge indicates that two users have viewed at least k common pages.The corpus was divided in this way in order to allow a holdout validation   , with initial model development on the training data  , model refinement being performed on the development data  , and final system evaluation being performed on the evaluation sub-corpus. This corpus was used in the 1998 TDT task  , and is divided into three sections: training January and February  , development March and April  , and evaluation May and June.Next to individual configurable experiments  , GERBIL offers an overview of recent experiment results belonging to the same experiment and matching type in the form of a Table 5: Results of an example experiment. Offering such detailed and structured experimental results opens new research avenues in terms of tool and dataset diagnostics to increase decision makers' ability to choose the right settings for the right use case.We also evaluated with a recal/-oriented metric Cf=/C ,n~46 = 0.1  , which was the standard metric in the 1999 TDT-3 evaluation   , and which favors large clusters and tolerates lower precision in favor of better recall. We evaluate our system initially at Cf=/C , ,~0~ = 1  , which was the standard metric in the 1998 TDT-2 evaluation.The 24-hour requirement stems from the fact that Pinterest displays relative timestamps with decreasing resolution  , i.e.  , a pin from an hour ago will display " posted 1 hour ago  , " whereas a pin from yesterday will display " posted 1 day ago. " However  , collecting pins requires addressing two technical challenges: 1 each pin must be gathered within 24- hours after it was generated  , and 2 each pin must be gathered individually.In the following  , we present current state-of-the-art approaches both available or unavailable in GERBIL. Currently  , GERBIL offers 9 entity annotation systems with a variety of features  , capabilities and experiments.Our analysis reveals interesting details about the operations of Quora. This further supports our hypothesis that Quora's social graph and question graph have been extremely effective at focusing user attention and input on a small subset of valuable questions.SPARQL endpoint from DataHub in step i  , step ii extracts resource types and instances via SPARQL queries 5 that conform to the definition of resource types and instances in Section 2. From the extracted dataset metadata i.e.Although Pinterest is primarly on image-based OSN  , users may write typically brief freeform textual descriptions for each pin. Third  , we compare the language used in pin descriptions and tweets.To our knowledge  , no research paper focused on user behavior  , based on gender  , was conducted in a heavy image-based network such as Pinterest. Most existing works focus in popular social networks where direct interactions and textual communication are of paramount importance.This analysis indicates that the consumption of items strongly exhibit recency  , which we will model in Section 4.1. The behavior of caching for all the other datasets are in line with MAPCLICKS and BRIGHTKITE.By calculating the most used pin source of each user and comparing it with the user's personal website  , available in her description  , we are able to identify self-promoters  , people who use Pinterest to promote their outside webpage. Food Drink and DIY Crafts.When we use only similarity between the page titles to build the model  , the recommendation framework does not perform well. Figure 11 left shows the performance of the recommendation for the AlgoViz Fall 2009 dataset.Recall that in Figure 1we examined the same relationship on RateBeer data in more detail. In other words  , products with high average ratings are rated more highly by experts; products with low average ratings are rated more highly by beginners.In the current system  , the page number of a scanned page is recognized by analyzing the OCRed text. As another example  , in case the program can not recognize the volume and issue number due to OCR error  , such as " IV " was OCRed as " it "   , the program will use the previous or the following title page information  , if available  , to construct the current volume or issue metadata.Anecdotal information from TDT participants indicates that this value was chosen using training data to be 0.02. A constant value for P rel is used over all topics.TDT data consist of a stream of news in multiple languages and from different media -audio from television  , radio  , and web news broadcasts  , and text from newswires. For all the research reported here  , we used manual transcriptions and reference boundaries.The resuiting TDT corpus includes 15 ,863 news stories spanning July 1  , 1994  , through June 30  , 1995. To simplify the problem slightly for the pilot study  , we generally ignored issues of degraded text coming from speech recordings  , and used written newswire sources and human-transcribed stories from broadcast news.Six topics are selected from the same scenario science/discovery  , with a total of 280 news stories. The collection used in the experiments is part of TDT- 3 1 .We described overall system performance using a bootstrap method that produced performance distributions for the TDT corpus. System misses and false alarms were used to measure detection error in a cross-validation approach that found stable system parameters for our implementation.For each EconStor author  , we harvest several other repositories for correlations with other authors  , publications or other relevant information about the initial author. EconStor content has also been published in the LOD.Figure 6compares the CDF of the age of the accounts on Pinterest for identities that are curated using intra-domain reputation signals  , and the additionally curated identities. To understand what kind of identities benefit the most from inter-domain reputation signals  , we study the properties of identities that do not get curated using only intra-domain reputation signals  , but that get curated using both intra-domain and inter-domain reputation signals – we call these identities the additionally curated identities.Using parallelization with 20 threads  , our model could be fit on our largest dataset RateBeer of 2 million total events within two minutes. Updating Θ can be done in parallel for each class and stage  , and updating stages and classes can be parallelized for each sequence.We begin by examining the follower and followee statistics of Quora users. editors  , actors and CEOs.Thus our hypothesis is that  , outside of the small portion of celebrities who get followers just by their mere presence  , the majority of Quora users attract followers by contributing a large number of high-quality answers. According to a recent survey of Quora users 31  , they tend to follow users who they consider interesting and knowledgeable .Similarly  , a digital document may exist in different media types  , such as plain text  , HTML  , I&TEX  , DVI  , postscript  , scanned-image  , OCRed text  , or certain PC-a.pplication format. Note that  , however  , indirection duplicates are not possible with technical reports.By definition  , replicating document provides the corresponding postings  , so tdt ≥ tpt. In order to avoid these cases of false positives  , RIP identifies these documents and fully indexes them in SIi.Fiscus and Doddington 2 provide an excellent review of the TDT community's motivations in coming up with that cost function . It is that cost function that is used to tune system parameters on training data and that is the basis for deciding which system " wins " an evaluation task.GDELT contains a set of entities for each article ; however  , we ignored these annotations and solely relied on our own methods to extract and disambiguate entities. We used GDELT http://gdeltproject.org/ news dataset for our experiments.Cosine distance is a symmetric measure related to the angle between two vectors 6. Prior research showed that a Cosine distance based measure was useful for the TDT FSD task 4.We note that the MoviePilot data does not contain the group information for all the users in the training data. Once again  , it is clear that the group recommendation model based on the IMM outperforms the other two methods.We call social links created in this way native links. The social graph of Pinterest is created through users following other users or boards they find interesting.Events are typically short in duration and thus only a small portion of any corpus will be about any particular event. Systems in the TDT domain must be able to distinguish between events  , regardless of whether they are part of the same topic or not.In many ways  , the temporal summarization problem is an event-and sentence-level analogue of TDT's " £rst story detection " problem  , where the task is to identify the £rst story that discusses each topic in the news. The problems tackled by TDT are all story-based rather than sentence based.The scanned document collection was based on the 21 ,759 " NEWS " stories in TDT-2 Version 3 December 1999. Measuring the actual recognition performance of the OCR system relative to the reference transcription is a topic of current work.Dataset. The user who introduces an image into Pinterest is its pinner; others who copy onto their own pinboards are repinners.Quora is unique because it integrates an effective social network shown above into a tradition Q&A site. But neither channel appears to be the primary way of attracting answers  , and both channels appear to complement each other in this process.This allows for a quick comparison of tools and datasets on recently run experiments without additional computational effort. It is accessible at http://gerbil.aksw.org/gerbil/ experiment ?id=201503050003 visualizations  , 30 see Figure 2 .We use this signal to identify suspended identities on Pinterest. Since this includes a significant fraction of all identities  , in the same way as for identities with blocked pins  , we assume that identities with higher fractions of low WoT reputation pins are more likely to be untrustworthy.Again  , there is a clear relationship between products' overall popularity and the extent to which experts prefer them; non-alcoholic beer is naturally not highly rated on a beer rating website  , while lambics and IPAs are more in favor. The results are highly consistent across BeerAdvocate and RateBeer  , in spite of the differing product categorizations used by the two sites Kvass is a form of low-alcohol beer  , Kristallweizen is a form of wheat beer  , IPA is a form of strong ale  , and Gueuze is a type of lambic.The front-end of Citebase is a meta-search engine. Other services can harvest this enhanced metadata from Citebase to provide a reference-linked environment  , or perform further analysis or they can be harvested by the source archives to enhance their own data.In the following  , we present nine well-known and publicly available data sets which are integrated in GERBIL and are used in our evaluation. The reported results of our approach and competitive systems are based on this platform and serve as comparable results for future systems.BRIGHTKITE. For privacy reasons  , we only consider pages clicked on by at least 50 distinct users  , and only consider users with at least 100 clicks.BBN also gave NIST a basic language modeling toolkit to work with. GTE/BBN offered NIST a LINUX instantiation of their fast BYBLOS Rough 'N Ready recognizer which now operated at 4Xrt to use as a baseline in the SDR and TDT tests Kubala  , et al.  , 2000.Burst detection from a stream of documents have been thoroughly investigated in TDT and event detection 22  , 17  , 31. For every target entity e  , when A in state q0  , it has low emission rateWe also used the MoviePilot data  , by disregarding the group memberships. We repeat this process five times to compute 5-fold cross validated results.They represent the tradeoff between these costs with a DET curve. TDT uses a measure that allows for varying the cost of misses versus false positives which they term false alarms.With this information we propose a simple algorithm to detect users who use Pinterest to promote their external website  , hereby called self promoters . the source entropy of the users  , we are able to determine which is the main source of pins from each user  , just by observing the domain with the highest probability p i .Thus  , using inter-domain reputation signals allows us to curate more identities and enables us to do it faster. Figure 6also shows that to curate identities using Pinterest reputation signals alone  , we have to wait at least 15 months  , but by exploiting inter-domain reputation signals  , we can curate identities more than 10 months in advance.It is so interesting to know that the Model-Anchor suggests the WikiTravel page of the Kalamazoo city that is judged as an irrelevant suggestion in the first rank. Second  , the reason of the difference between the average M RR of Model-Anchor and Model-Text for the profile 700 is his/her judgment in " Kalamazoo MI " context.We evaluate our algorithm on the purchase history from an e-commerce website shop.com. Applying our utility function to SVD leads to a new utility function SV D util in this paper.This allows the user to search for articles by author  , keywords in the title or abstract  , publication e.g. The front-end of Citebase is a meta-search engine.We use this as a minimum threshold for our later analyses on social factors on system performance. In addition  , 99% of questions end up with less than 10 answers  , and 20% of all Quora questions managed to collect ≥4 answers.Example 2 shows a similar problem in a different domain. If the NASDAQ Computer Index were further divided into software  , hardware  , services  , etc.  , one can further analyze comparisons with them.This estimate might provide an upper bound of actual number of questions  , and our coverage of 58% would be a lower bound. Note that not all questions remain on the site  , as Quora actively deletes spam and redundant questions 5.We implemented TDT based on 12  cos+TD+Simple- Thresholding. Instead of fixing the number of chains  , we continued to add chains until additional coverage was less than 20% of the total coverage since we use greedy coverage  , there will be at most 5 chains.We ask whether we can observe evidence for the different roles of social ties in interest-based social networks. To answer this  , we examine the complete activity history of ∼50k randomly selected users on Pinterest .com as a case study.Our analysis relies on two key datasets. Out of the 264K extracted users  , we found that roughly 5000 1.9% profiles were no longer available  , likely deleted either by Quora or the user.Among the dissimilarities  , the following are noteworthy: a Information services/goods and network services have many more parameters other than just price and quantity  , which describe the products and services. Nasdaq.The length of sequence can be of great interest in many datasets; for example  , it represents how actively a user enters reviews on BeerAdvocate and RateBeer  , how popular a phrase is in NIFTY  , or the skill of a player on Wikispeedia. Or  , do sequences that go through stages very quickly have more events ? Given the rapid growth of questions on question-and-answer sites  , how does Quora help users find the most interesting and valuable questions and avoid spammy or low-value questions ? Can they generate and focus user attention on individual questions  , thus setting them apart from questions on related topics ?Within TDT a participant's cluster structure is evaluated by identifying the best cluster for each of the topics from a manually composed ground truth. The root node represents the complete story collection; child clusters further down the DAG define smaller subsets of stories   , corresponding to finer detailed topics 5.Of these  , 96 have exhaustive relevance judgments where an evaluation was done for every document in the collection  , and the other 96 have a pooled evaluation  , with many documents tagged  , but not all documents have been compared to those topics . The TDT-2 corpus has 192 topics with known relevance judgments.The results provide evidence for the need to weigh the recent changes in time series distance measurement higher than the ancient changes. The results of the performance for the TSA algorithm with cross correlation distance function over WS-353 are presented in Table 8.However  , the Clarksville is not mentioned in the anchor text of the Nashville wikitravel page  , and it is reasonable that it is not included in the top-5 ranking of the Model-Anchor. Due to the fact that the Nashville is just 47.8 miles further than the Clarksville in the state of Tennessee  , this page is judged as a relevant suggestion.We are investigating those issues. There are interesting problems with using this cost function in the context of a DET curve  , the other official TDT measure.This logical structure information can be used to help the metadata extraction process. In each DjVu XML file  , the OCRed text is organized in a page  , paragraph  , line  , and word hierarchy.2014;Stepchenkova 2014—see our data release for full list— which we then expand in a snowball fashion as we did for themes/taxonomies in GDELT. climatechange   , global warming Pearce et al.  , navigate the literature using linked citations and citation analysis  , and to retrieve linked full-texts in Adobe PDF format. Primarily a user-service  , Citebase provides a Web site that allows users to perform a meta-search title  , author etc.Based on the observation  , title pages have relatively fewer number of text lines and larger average distance between text lines  , and they contain text lines indicating volume number and issue number in issue title pages. We use rule-based approach for title detection using page and line features calculated from OCRed text  , bounding box information  , and context analysis.We adopt the TDT cost function to evaluate our result-filtering task. The TDT cost function is based on the probabilities of missing relevant results  , Pmiss  , and retrieving non-relevant results  , Pfa  , combining them by explicitly assigning a cost to each  , C miss and C fa   , and weighting this combination by the relative amount of relevant documents in general  , Prel  , as shown in Equation 1.Identities with pins that are blocked: Each pin on Pinterest has an associated URL that redirects the user to the page hosting the image. We gathered 1 ,706 such untrustworthy identities.We treat BeerAdvocate as a 'development domain'  , because we used it for developing the models and experimental setting  , and RateBeer as a 'test domain' in which we validate our final models on previously unseen data. Each split used 70% of the data for training and 30% for testing.This presents us with an unprecedented opportunity to study linguistic change over users' entire lifespans  , from the moment they joined the community—which we define as the time of their first post 2 — to the moment they abandon the community. Over the course of 10 years the BeerAdvocate and RateBeer communities have evolved both in terms of their user base as well as ways in which users review and discuss beer.compared more than 15 systems on 20 different datasets. Using GERBIL  , Usbeck et al.The second collection is the largest provided by the Wikia service  , Wookieepedia  , about the Starwars universe. In this article  , we refer to this sample as WPEDIA.While developing GERBIL  , we spotted several flaws in the formal model underlying previous benchmarking frameworks which we aim to tackle in the future. More information about GERBIL and its source code can be found at the project's website.In terms of votes  , both Quora and Stack Overflow allow users to upvote and downvote answers. We use this as a minimum threshold for our later analyses on social factors on system performance.This paper also contributes to image analysis and understanding. 20  , who propose a model for recommending boards to Pinterest users.Experiments make it clear that the number of on-topic stories – hence P rel – varies over different topics. The TDT cost function assumes that this probability is constant across topics.Thus  , the contribution of the quotation keywords to the overall frequency of the term varies based on the value of the corresponding impact factor. Other similarity and distance measures  , such as Hellinger distance and Kullback- Leiber divergence are also shown to work well in the TDT domain 3  , 5  , 37.According to a recent survey of Quora users 31  , they tend to follow users who they consider interesting and knowledgeable . To understand how Quora's social network functions  , a basic question of interest is how users choose their followees.Surprisingly  , for a goal-oriented and interestbased social network  , a non-trivial proportion of information seeking happens through non-social means which indicates a decreased importance of social ties in content discovery on Pinterest. Findings.Pinterest Figure 2: The fraction of topical followers to the total number of followers for the set of basketball players that were selected in the example of Fig. They proposed several features based on users contributions and graph influence.Therefore  , we denote it by F1 instead of " performance " for simplicity. " performance " adopted by KDDCUP 2005 is in fact F1.However  , their tasks are not consistent with ours. TDT project has its own evaluation plan.We choose TDT4 dataset to run experiments  , which contains 80 events annotated from almost 28 ,500 news articles . The first is TDT 1  collections  , which are benchmarks for event detection .Figure 4depicts the novel process we developed to categorize tweets. This step is necessary in order to make pins and tweets directly comparable: on Pinterest  , all pins fall into 1 of 33 pre-defined categories  , while tweets are freeform text.It is accessible at http://gerbil.aksw.org/gerbil/ experiment ?id=201503050003 visualizations  , 30 see Figure 2 . Next to individual configurable experiments  , GERBIL offers an overview of recent experiment results belonging to the same experiment and matching type in the form of a Table 5: Results of an example experiment.We describe details below. A publicly available dataset periodically released by Stack Overflow  , and a dataset crawled  from Quora that contains multiple groups of data on users  , questions   , topics and votes.To achieve this goal  , we surveyed the workload necessary to implement a novel annotator into GERBIL compared to the implementation into previous diverse frameworks. To ensure the practicability and convenience of the GER- BIL framework  , we investigated the effort needed to use GERBIL for the evaluation of novel annotators.The latter is dangerously close to testing on the training data the topics are different but mirrors the official TDT'01 evaluation settings 16. The second set of numbers was produced on TDT3/2 by the system trained on TDT3/1.We then present an evaluation of the framework that aims to quantify the effort necessary to include novel annotators and datasets to the framework. We focus in particular on how annotators and datasets can be added to GERBIL and give a short overview of the annotators and tools that are currently included in the framework.the person who uploaded a photo that has been pinned on Pinterest may hold the original and may host it elsewhere online  , these would be difficult to obtain by the user if their collection was lost even if simply because it's hard to remember what's in the collection . There is no digital original under the user's control  , and while duplicates exist e.g.For stories from an audio source  , the closed caption was used rather than speech recognition output. We also used a second corpus  , tdt2  , which includes the English news stories from the TDT-2 collection   , amounting to approximately 40 ,000 news stories from newswire and broadcast news sources.A key observation is that given the broad and growing number of topics in Quora  , identifying the most interesting and useful content  , i.e. Despite their different topics of interest  , Quora and Stack Overflow share many similarities in distribution of content and activity.This section presents various digital resources of each scanned volume  , selection of input for the metadata generation system  , the method for automatic metadata generation  , and the set of metadata elements generated by the system. Finally  , generated metadata information and OCRed text are integrated to support navigation and retrieval of content within scanned volumes.First  , we ask what proportion of a user's reciprocated and directed unreciprocated links have incurred repins. We examine how the social repin network selectively samples the underlying network of Pinterest.For our classification experiments  , we trained on TDT-2 judged documents and tested on TDT-3 documents. In our experiments  , the terms in a document  , weighted by their frequency of occurrence in it  , were used as features.We also extract the topics of the questions in the cluster and rank the topics based on how many questions they are associated with. This cluster contains 43 questions  , and all questions are related to " Quora. "These codes were a fascinating repository of raw linguistic " ore " from which the possibility of additional " finds " could be made. Additionally   , the MPD and w7 were the result of an extensive organization effort by a whole series of computational lexicologists who had refined its format to a very easily computed structural description Reichert  , Oiney & Paris 69  , Sherman 74  , Amsler and White 79  , Peterson 82  , Peterson 871 The LDOCE while very new  , offered something relatively rare in dictionaries  , a series of syntactic and semantic codes for the meanings of its words.The citation impact of an article is the number of citations to that article. Citebase provides information about both the citation impact and usage impact of research articles and authors  , generated from the open-access pre-print and postprint literature that Citebase covers.Our final run on the evaluation portion of TDT-2 produced 146 clusters. The clusters produced by our system were randomized and the users were given different starting points in the system so we could guarantee coverage.As shown in figure 4  , Pinterest users tend to follow others entirely and this behavior is not mediated by gender. .Pinterest users organize objects by selecting an image from the webpage where the object exists using a browser bookmarklet or by uploading an image from their computer. Pinterest also has many social components: users add boards to subject categories ,'repin' objects from other users to their own boards  , " like " or comment on objects  , and " follow " boards and other user's activity.We used the DET software provided in the TDT project to generate the DET curves  , and converted each data point a pair of miss/false-alarm values in these DET curves to the corresponding recall and precision values noninterpolated  to obtain the recall-precision curves. These curves were obtained by moving thresholds on the confedence scores of detection decisions.To address this problem  , and at the same time down-weight frequent terms  , we developed stop lists for each category from training stories in TDT-2. Hence constraining the stories into water-tight categories proved detrimental in our case.We adopt the consumer purchasing records dataset from Shop.com 1 for model evaluation  , because an important information source leveraged in our framework is the quantity of product that a consumer purchased in each transaction   , which is absent in many of the public datasets. These amount to roughly 100k transactions by 34k consumers on 30k products in the testing dataset.We observe similar trends in Quora. 60% of Stack Overflow users did not post any questions or answers  , while less than 1% of active users post more than 1000 questions or answers.The ROI can be seen as a higher-level categoriztion of the events. Events for the TDT tasks are further categorized into " rules of interpretation " ROI.For each term t  , a site maintains two replication thresholds  , expressed in partial score values: the document replication threshold tdt and the postings replication threshold tpt. The replication algorithm works as follows.The TDT evaluation program assumes a constant for the probability that a story is on topic. We are investigating those issues.Figure 6 : Age of curated Pinterest identities: identities curated using Pinterest reputation signals vs additionally curated identities using all signals. Thus  , using inter-domain reputation signals allows us to curate more identities and enables us to do it faster.Despite its short history Quora exited beta status in January 2010  , Quora seems to have achieved where its competitors have failed  , i.e. Various estimates of user growth include numbers such as 150% growth in one month  , and nearly 900% growth in one year 23.As a result a list of all publications  , co-authors and co-author's publications from our repository will be created and returned to the user of our prototype. Kubler  , Felix "   , in EconStor.Images posted by identities on Pinterest are called pins. Identities with black-market association: In this dataset  , we collect information about identities associated with a black-market service.Youngstown travel guide -Wikitravel " . For City Youngstown  , OH  , its Wikitravel page is " 2.We find that all three of its internal graphs  , a user-topic follow graph  , a userto-user social graph  , and a related question graph  , serve complementary roles in improving effective content discovery on Quora. In this paper  , we use a data-driven study to analyze the impact of Quora's internal mechanisms that address this challenge.GER- BIL will regularly check whether new corpora are available and publish them for benchmarking after a manual quality assurance cycle which ensures their usability for the implemented configuration options. However  , GERBIL is currently only importing already available datasets.The duration and number of cohesively topical segments can vary for our training/education category of videos. However  , there are several important differences that are unique to our problem domain: a Unlike the corpus of broadcast news audio used in TDT evaluations  , training and corporate videos can be quite heterogeneous  , b There is no notion of a story and the associated well-defined segment of expected relatively short duration.Thus  , we focus on the coordinate ascent approach for the remainder of this paper. For example  , it takes two days for EM to finish for the RateBeer dataset  , whereas our method takes just two minutes.Contrary  , in AOL the temporal component takes over. Many Quora users seem to frequently post replies prompted by others rather than by their personal situation ; hence the lower impact of the temporal component.Paul  , Hong  , and Chi found that  , on Quora  , users judge expertise and reputation of answers based on previous contributions 26. Expertise has been shown to be an important factor when people decide how and who to ask for information 6  , 12.Nallapati and Allan 5  represent term dependencies in a sentence using a maximum spanning tree and generate a sentence tree language model for the story link detection task in TDT. Term dependencies can be measured using their co-occurrence statistics.The feature extraction step uses OCRed text and the bounding box information to calculate line features for every text line contained within a scanned volume. There are two steps in the automatic metadata generation process: feature extraction and metadata labeling.Selecting word pairs to evaluate: To create a balanced dataset of both related words and unrelated words  , we applied the following procedure: Let W be a set of all words in the New York Times news articles. The relatedness of these pairs of words is then evaluated using human annotators   , as done in the WS-353 dataset.We computed Fleiss' Kappa to quantify the global interannotator agreement across all the topics. Since the majority of Quora profiles contain hundreds of posts  , to ensure that proper care is given to evaluating them  , we collected the judgements employing 19 students from our institutions.In this paper  , we choose the single-pass clustering algorithm which is popular in TDT as the baseline and then propose three variations of the single-pass clustering algorithm to take the temporal information into consideration . An exception is 9  , where the authors identified thread starts based on some patterns provided by experts.Citebase  , more fully described by Hitchcock et al. The usage impact is an estimate of the number of downloads of that article so far available for one arXiv.org mirror only.In this section  , we analyze the Quora social graph to understand the interplay between user social ties and Q&A activities. Therefore  , social relationships clearly affect Q&A activities  , and serve as a mechanism to lead users to valuable information.To study this  , we first compare social vs. non-social means of acquiring new infor- mation. Therefore  , we might expect that the ability of social networks to provide access to new informationwould be important on Pinterest.For the future work  , we want to collect news set which span for a longer period from internet  , and integrate time information in NED task. We did not consider news time information as a clue for NED task  , since most of the topics last for a long time and TDT data sets only span for a relative short period no more than 6 months.Also  , data mining for high-level behavioral patterns in a diachronous  , heterogeneous  , partially- OCRed corpus of this scale is quite new  , precedented on this scale perhaps only by 8 which brands this new area as " culturomics " . We list them here to explain our study design.We apply conjunctive constraints on document image components to a straightforward document ranking based on total query-word frequency in the OCRed document text; in Fig- ure 2we show document images retrieved for two such queries. We consider integrated queries that our prototype makes possible for the first time.To locate the URLs corresponding to news articles relevant to climate change  , we rely on GDELT themes and taxonomies  , which are topical tags that automatically annotate events. However  , given that we are interested in the peak in the coverage  , rather than in the number of events  , here we directly use the news articles  , not the events automatically mapped by GDELT; applying a consistent methodology for detecting events.For the mid-1990s events in the second TDT study  , systems had trouble treating the O. J. Simpson case or the investigation of the Oklahoma city bombing as a single event 17  , 19. The most significant problem in adapting TDT methods to historical texts is the difficulty of handling long-running topics.This approach is based on the system used by the University of Massachusetts at Amherst at the latest TDT workshop 3. The similarity between the cluster and an incoming document is the average similarity between the incoming document and every document in the cluster.The selected EconStor article and its related blog posts show a meaningful relationship. She can further filter out blog posts by date  , leaving only the most recent ones in the result set.In addition to the features used on the TDT data  , we included features based on speaker and video shot changes  , but they did not yield consistent performance improvement. For each source we collected and annotated approximately 50 hours of data  , spanning a month of programming.In this section  , we model the interaction between Quora users and topics using a user-topic graph  , and examine the impact of such interactions on question answering and viewing activities. A question  , once created or updated under a topic  , will be pushed to the newsfeeds of users who follow the topic.Crowdsourcing sensitivity and domain judgements. We trained 3 LDA models  , using the Mallet topic modeling toolkit: i with 500 topics  , on 600K Quora posts we crawled ii with 200 topics  , on 3M posts from health Q&A online forums  , and iii with 500 topics  , on a sample of 700K articles from the New York Times NYT news archive.7 GDELT covers a " cross-section of all major international  , national  , regional  , local  , and hyper-local news sources  , both print and broadcast  , from nearly every corner of the globe " 8 including major international news sources. GDELT releases data about daily media coverage in two formats: the Event Database and the Global Knowledge Graph GKG.33  proposed an expertise modeling algorithm for Pinterest. Recently  , Popescu et al.For the purpose of this study we will employ data from two large beer review communities BeerAdvocate and RateBeer. The framework presented in this paper is targeted at large and active online communities  , where individuals interact through written text visible to all members of the community .The winning solution in the KDDCUP 2005 competition  , which won on all three evaluation metrics precision  , F1 and creativity  , relied on an innovative method to map queries to target categories. As an example of a QC task  , given the query " apple "   , it should be classified into " Computers\Hardware; Living\Food&Cooking " .For instance  , on Pinterest  , perhaps the most prominent content curation site  , users can collect and categorise images and the URLs of the webpages that contain them by " pinning " them onto so-called " pinboards " . Following on the heels of the information glut created by the user-generated content revolution  , an interesting new phenomenon that has been termed content curation has emerged: Rather than create new content  , content curation involves categorising and organising collections of content created by others.Recently  , but after our crawling period  , Pinterest enabled the possibility of creating secret boards: basically  , boards which only the owner has access Milam 2012. The user is able to create a collection of boards  , which is summarized in her profile along with a self description  , a profile picture  , and information about her activity and relationship with other users such as her pins  , likes  , board  , users that she follows and follows her  , as well the last fifty activities .We used the TDT-2 corpus for our experiment. With the choice of the TDT-2 corpus and its known topics  , we added a third question for our evaluation: "Does this cluster of phrases correspond to any of the TDT-2 topics ?"In the task definition of TDT  , a story is defined as " a topically cohesive segment of news that includes two or more declarative independent clauses about a single event " 1. Why we choose the most important event clause for filtering ?The methodology that we adopted sought to align itself to the structure of the CAMRa challenge. Next  , we experiment with the extent that the algorithms can produce quality recommendations for groups  , using the MoviePilot data.We observe tremendous improvements of 100% to 200% percent  , by adding the TDT data  , even though this data was automatically generated using SYSTRAN. We show performance of Relevance Models estimated using just the Hong-Kong News portion of the corpus  , versus performance with the full corpus.On Pinterest  , as on other interest-and goal-oriented sites Baird and Fisher 2005   , the social network does play an important role in creating and fostering community formation. In summary  , these results show that  , in fact  , the previous literature results on interest-based social networks do not contradict each other.Both TDT and event detection are concerned with the development of techniques for finding and following events in broadcast news or social media. A similar research topic in recent years is event detection.Although not part of the TDT task  , systems such as 8  for visualizing news broadcasts on maps also take advantage of a time-tagged data stream. Due to its focus on news data  , TDT possesses " an explicitly time-tagged corpus " .Not surprisingly  , questions under well-followed topics generally draw more answers and views. The user-topic interaction has considerable impact on question answering activities in Quora.Researchers have traditionally considered topics as flat-clusters 2. We review related work in TDT briefly here.Although other approaches   , such as those investigated in the TDT programme  , are of some interest  , we have no evidence of their suitability for spoken document retrieval. 1999  , we do not perform any acoustic segmentation in the recognition phase the audio stream is decoded directly; anyway  , there is no good correlation between segments obtained purely from low-level audio features and story segments required for information retrieval.Here we consider the consumed items to be all latitude-longitude pairs of anonymized user check-ins. BrightKite was a location-based social networking website where users could check in to physical locations.In this study  , we also described measures of summary effectiveness that are based upon the traditional recall and precision measures . We have built and described an evaluation corpus based on 22 topics from TDT news stories.As our method also captures co-occurrences of words in a single article as we construct time-series aggregated over all articles on a certain date  , phrases can also be identified well. The rankings are based on the rank of the similarity of the pair of words out of the 353 pairs in the WS-353 dataset.Pinterest's users comprise mainly young people  , the well-educated  , those with higher income  , and women. According to a 2012 survey by the Pew Research Center   , Pinterest has attracted 15% of internet users to its virtual scrapbooking.GDELT releases data about daily media coverage in two formats: the Event Database and the Global Knowledge Graph GKG. We use GDELT  , currently the largest global event catalog  , to automatically discover relevant events with high MSM coverage.pins for majority to appear Figure 2: Emergence of consensus in Pinterest: a The category chosen by the ith pinner is independent of the category chosen by the previous i − 1 pinners  , and is the same as the category chosen by the majority of repinners with a remarkably high probability ≈0.75. a Probability of majority pin b Levels of agreement in categories c No.For example  , Fig- ure 5shows how many stories appeared per day in the TDT corpus for the Oklahoma City bombing event. As the triggering event fades into the past  , the stories discussing the event similarly fade.The TDT cost function assumes a constant value of P rel across different topics to obtain the standard TDT cost function described above. Currently  , this is artificially forced upon systems during evaluation.For example  , most of the 10 news sites  , which are used for the current GeoTopics  , have sidebars and footers in their articles  , which cause falsematching problems e.g.  , 'NASDAQ' was ranked high because it is appeared on the side bars in many of the news articles. Although the high-level processing steps are the same extracting articles  , filtering and classifying them  , and generating the HTML report  , the selection and coordination of the information management services need to be flexible and reconfigurable to handle dynamic situations.2013 that focus on quantifying and analyzing Pinterest user behavior. As Pinterest has grown  , there have been a number recent studies e.g.  , Feng et al.However  , the social interaction among Quora users could impact voting in various ways. By positioning good answers at the top of the questions page  , Quora allows users to focus on valuable content.There is no high-quality human reference transcription available for TDT- 2 -only " closed-caption " quality transcriptions for the television sources and rough transcripts quickly made for the radio sources by commercial transcription services. The collection contains a total of 21 ,754 stories with an average length of 180 words totalling about 385 hours of audio data.There are a number of ways in which graphs can be analyzed  , graph partitioning being one. Topics 1  , 2  , 4  , and 5 are mostly related to AlgoViz catalog entries  , These topics are prominent in clusters 2  , 4 and 5.Other similarity and distance measures  , such as Hellinger distance and Kullback- Leiber divergence are also shown to work well in the TDT domain 3  , 5  , 37. Once a keyword weight vector is computed for a message  , the cosine similarity between this vector and the keyword vector of the parent message or the keyword vector representing the segment being computed so far can be used to classify the input message as having a new topic or being of the same topic as that of the parent.TSA results shown in the table are computed using cross correlation with a quadratic weighted function as the distance metric between single time series. The comparison results of TSA on the WS-353 dataset are reported in Table 1.By positioning good answers at the top of the questions page  , Quora allows users to focus on valuable content. Quora applies a voting system that leverages crowdsourced efforts to promote good answers.Consequently  , curation on Pinterest remains a highly manual process as well. Similarly  , users' pinboards are highly personal and idiosyncratic represntations of their taste  , and furthermore   , users are free to choose to repin any image onto any pinboard .The one task within TDT that most closely resembles this work is " new information detection " 5. In contrast  , the task discussed in this paper is based on a batch evaluation at the sentence level.Citebase provides information about both the citation impact and usage impact of research articles and authors  , generated from the open-access pre-print and postprint literature that Citebase covers. This allows the user to navigate back in time articles referred-to  , forward in time cited-by  , and sideways co-cited alongside.Some efforts have been made to classify news stories or other documents into broad subject areas automatically using nearest neighbor matching 16  , pattern matchingg  , or other algorithms based on supervised training4  , 13  , 171. The tasks defined within TDT appear to be new within the research community.Events for the TDT tasks are further categorized into " rules of interpretation " ROI. The event frequency ef w is defined as follows:Since no benchmark news streams exist for event detection TDT datasets are not proper streams  , we evaluate the quality of the automatically detected events by comparing them to manually-confirmed events by searching through the corpus. We shall evaluate our two hypotheses  , 1important aperiodic events can be defined by a set of HH features  , and 2less reported aperiodic events can be defined by a set of LH features.From the Wikia service  , we selected the encyclopedias Wookieepedia  , about the Star Wars universe  , and Muppet  , about the TV series " The Muppet Show " . From now on  , we refer to this encyclopedia as WPEDIA.Note that most multi-document summarization systems have to include time as a component of their system to consolidate information across stories e.g.  , to decide which statement is more up-to-date. Other work on news summarization  , including work that uses the TDT corpora  , focuses on single or multi-document summarization 25  , 34  , 12 of the stories  , without attempting to capture the changes over time.The Wookieepedia collection provides two distinct quality taxonomies. These are the two Wikia encyclopedias with the largest number of articles evaluated by users regarding their quality.Results of the experiments run on the Gerbil platform are shown in Table 2. All these methods are tested in the setting where a fixed set of mentions is given as input  , without requiring the mention detection step.To distinguish the two  , we call the first case simply " Rocchio " and the second case " PRF Rocchio " where PRF stands for pseudorelevance feedback. Both cases are part of our experiments in this paper and part of the TDT 2004 evaluations for AF.Several other users may want to repin this onto their own pinboards. Consider an image which has been introduced into Pinterest by an original pinner.This behavior is particularly strong for the BRIGHTKITE dataset  , where cyclic behavior has been observed 10. Finally   , we observe that the time scores capture cyclic behavior in the check-in data around daily and weekly marks.Pins are 'pinned' onto a so-called 'pinboard' which is intended to be a thematic Figure 1: Social network is not critical for information seeking on Pinterest: a The source of pins in our dataset. The user who introduces an image into Pinterest is called its pinner; others who re-appropriate it for their own pinboards are repinners.Intuitively  , an event  , like " Earthquake in Afghanistan on May 30  , 1998 "   , could be captured by a few semantic elements  , such as date  , location  , persons or organizations involved. In our study  , we use one on-topic story for training  , as required in recent TDT evaluations 3. The DjVu XML file retains the bounding box information of every single OCRed word  , from which we can estimate format features. Figure 3shows logical structure and bounding box information embedded within a DjVu XML document.Detailed results of that study are reported elsewhere3; this paper presents advances in our understanding of the problem after the end of the pilot study. As the research is broadened to the larger TDT scope  , the unresolved questions become more troublesome.We observe similar improvement over the baseline as in the English TDT-4 data. We obtain substantial performance Table 1Figure 1 b compares the baseline system with the system including the multi-window  , position in the show  , and sentence duration features.They may be classified as distinct documents by some users  , and duplicates by some others. Similarly  , a digital document may exist in different media types  , such as plain text  , HTML  , I&TEX  , DVI  , postscript  , scanned-image  , OCRed text  , or certain PC-a.pplication format.Each user has a " Top page  , which displays updates on recent activities and participated questions of their friends followees  , as well as recent questions under the topic they followed. Each Quora user has a profile that displays her bio information  , previous questions and answers  , followed topics  , and social connections followers and followees.We use the Gerbil testing platform 37 version 1.1.4 with the D2KB setting in which a document together with a fixed set of mentions to be annotated are given as input. Further   , we show an empirical comparison between PBoH and well known or recent competitive entity disambiguation systems .In 22  , a finite automaton model is proposed to detect events in stream by modeling events as state transitions. The techniques adopted for TDT and event detection can be broadly classified into two categories: 1 clustering documents based on the semantic distance between them 34  , or 2 grouping the frequent words together to represent events 22.Given the datasets above  , we now describe how we tested and measured the efficacy of the recommendation algorithms described in Sections 2 and 3. We also used the MoviePilot data  , by disregarding the group memberships.Since " Illinois Wikifier " is currently only available as local binary and GERBIL is solely based on webservices we excluded it from GER- BIL for the sake of comparability and server load. The authors provide their datasets 9 as well as their software " Illinois Wikifier " 10 online.Figure 3shows that around 46% of all articles have been shared on two platforms. Pinterest and the Q&A sites have the lowest levels of coverage and usage.We now evaluate the potential for inter-domain trust transfer to reason about the trustworthiness of identities. Table 1 : Source domain distribution for random  , suspended and black market identities in Pinterest.We manually checked these users and found that they were legitimate accounts  , and come from various backgrounds such as CEOs  , cofounders   , bloggers  , students  , and were all very active Quora users. Finally  , a very small portion of users 27 or 0.01% followed more than 1000 topics.Besides metadata properties like titles  , descriptions and authors  , the source files of the open datasets themselves are linked as dcat:Distributions  , allowing Table 2: Datasets and their formats. GERBIL uses the recently proposed DataID 2 ontology that combines VoID 1 and DCAT 21  metadata with Prov- O 20 provenance information and ODRL 23  licenses to describe datasets.This leads to the question: do users share the same linguistic style across pin descriptions and tweets ? Although Pinterest is primarly on image-based OSN  , users may write typically brief freeform textual descriptions for each pin.Topics and news issues generated using our algorithms are called clusters  , actual topics and news issues called classes  , and Recall  , Precision are calculated as 11 We don't use C Det 20  , which is commonly used in TDT  , because the conditions of our problem and real TDT tasks are different.For each section  , first we extract all bold phrases. Wikitravel Page = the i th document  , where Table 2The "See" section of document "Houma travel guide -Wikitravel" After retrieving one city's Wikitravel homepage  , we examine the " See "   , " Do "   , " Eat "   , " Drink " and " Buy " sections in that page  , and extract famous venues from these sections.As Quora continues to grow  , it is clear that helping users easily identify and find the most meaningful and valuable questions and answers is a growing challenge. Our estimated number of questions in Quora for June 2012 is 700K  , which is consistent with previously reported estimates 24.Surprisingly   , we were able to observe that a representative amount of users who have a website in their profiles are in fact self promoters  The percentage of self promoters found by this simple approach could be an indicator that in fact Pinterest is becoming more related to e-commerce. We do so by verifying if the main domain of a user is also her personal website described in the profile.Experience versus rating variance when rating the same product. This phenomenon is the most pronounced on RateBeer Figure 5: Experienced users agree more about their ratings than beginners.To enable this comparison  , we selected 30K Pinterest users uniformly at random from our original sample of 2 million Pinterest users. The purpose of this comparison is to quantify any bias in our target population.As a result  , we create a wider author profile enriched with additional information. For each EconStor author  , we harvest several other repositories for correlations with other authors  , publications or other relevant information about the initial author.Despite their different topics of interest  , Quora and Stack Overflow share many similarities in distribution of content and activity. This is the focus of the rest of our paper  , where we will study different Quora mechanisms to understand which  , if any  , can keep the site useful by consistently guiding users to valuable information.Datasets. Table 2shows the most prominent words for each of the chosen topics from the Quora topic model.In contrast  , Stack Overflow anonymizes all voters and only displays the accumulated number of votes  , which can be negative Sorted Topic Bucket By # of Followers Thus in our analysis of Quora  , we only refer to upvotes and disregard downvotes .The Datahub data set shows a far more balanced behaviour. However  , at very different levels: the probability of knowing the type set for a given property set ranges between 15.15% and 54.85%.There are  , however  , differences in the application of TFIDF. All four systems evaluated in  TDT-2002 cluding ours are using it.We extracted site-internal links from all the States  , Regions  , Cities  , Districts and Burroughs sections. We crawled all Wikitravel pages of locations within the US  , starting with the page on the United States of America as the seed list.Finally  , we discuss a pervasive pattern exhibited in all of our datasets: recency  , the tendency for more recently-consumed items to be reconsumed than items consumed further in the past. Recency is clearly present in MAPCLICKS and BRIGHTKITE  , and absent from SHAKESPEARE and YES.We ran experiments on the TDT corpus itself  , but seeded the initial values with those obtained from an auxiliary corpus  " past " . Query weights were held constant  , and document weights were recalculated based on incrementally updated values.The dictionary we are using in our research  , the Longman Dictionary of Contemporary English LDOCE Proctor 781  , has the following information associated with its senses: part of speech  , subcategorizationl   , morphology  , semantic restrictions   , and subject classification. What's important for our purposes is that the senses have information associated with them that will help us to distinguish them.2013; Zhong  , Karamshuk  , and Sastry 2015  have found that creating new pins or repins  is by far the most common activity on Pinterest  , and presents a quintessential information seeking activity. Previous studies Zhong et al.The user who introduces an image into Pinterest is called its pinner; others who re-appropriate it for their own pinboards are repinners. Users on Pinterest.com collect so-called pins  , which are images   , together with associated URLs of webpages where they are found.Currently  , this is artificially forced upon systems during evaluation. It is desirable in TDT to have a cost function which has a constant threshold across topics.We have de£ned temporal summarization  , a new and important variant of the text summarization task. We will do that by using the topic clusters generated by TDT systems.Citebase harvests OAI metadata records for papers in these archives  , as well as extracting the references from each paper. 1  , allows users to find research papers stored in open access  , OAI-compliant archives -currently arXiv http://arxiv.org/  , CogPrints http://cogprints.soton.ac.uk/ and BioMed Central http://www.biomedcentral.com/.The basic units of data on Pinterest are the images and videos users pin to their boards. Pinterest pre-defines 33 categories  , varying from " Women's Fashion " and " Hair Beauty " to " Geek " and " Tattoos " .The coordination mechanism allows an additional filter to be added to filter out the sidebars and footers  , and to return only the pure article text. For example  , most of the 10 news sites  , which are used for the current GeoTopics  , have sidebars and footers in their articles  , which cause falsematching problems e.g.  , 'NASDAQ' was ranked high because it is appeared on the side bars in many of the news articles.We ran additional experiments using the NYT topic model described in section 4.1  , and noticed that for the topics which were captured in the other latent model as well  , we observe similar trends and dependencies in the results. The results presented in the experimental section were obtained using the Quora topic model as the background knowledge model.Naturally  , a TDT system would not make decision based on sheer temporal similarity  , but we believe it will provide valuable additional evidence. We show that this overlap  , coverage  , is higher when two documents discuss the same event than when they are not.The resulting collection of 561 ,644 URLs contains an average of about 30 ,000 URLs per month  , with over 80% of the tags being tagged with the theme ENV CLIMATECHANGE. Then  , we extract all the unique URLs corresponding to events annotated in GDELT with one of these themes for each day.The TDT 3 dataset roughly 35 ,000 documents was used as a preparation for participation in the trial HTD task of TDT 2004. to the clusters of the first 5 matching sample documents.This is because some of their related questions were not crawled questions deleted by Quora and thus are not included as nodes. However  , there are 9% questions with degree less than 5.The crawled and concatenated text of each of the 5 Wikitravel categories served as document representations  , which we indexed using Indri. Using a tf-idf measure  , we extracted the top 30 keywords for each example website  , that could serve as queries.To our knowledge this is the first study to conduct a large scale analysis of Pinterest. Moreover  , there is no official public API available for data collection  , which makes the process even more laborious.Figure 9plots the daily activity patterns of our target users  , broken down by individual days of the week. One deviation occurs the week of October 28: we hypothesize that Pinterest saw increased activity this week due Halloween-related pins.A cursory look at Table 3seems to reveal that our extensions to the basic model actually hurt performance. Though the precise details of topics and stories in the TDT4 corpus are still unavailable  , the fact that all the systems that were run on it as part of this year's official TDT evaluations performed badly lends credence to the belief that the TDT4 corpus is more challenging than the TDT3 corpus.In §2.4 we set up Filtering QAC with relevance scores  , by additionally filtering out all the suggested queries with certain dwell time thresholds TDT  and position thresholds TP  in the subsequent keystrokes in a composition. The optimal weights in Personal-S α = 0.34 and TimeSense-S α = 0.42 achieve the highest MRR for static QAC.However  , at very different levels: the probability of knowing the type set for a given property set ranges between 15.15% and 54.85%. Again  , and with the exception of Datahub D  , the other data sets exhibit a similar trend.Since the documents are translated by software  , we do not expect the quality of the TDT corpus to be as high as Hong-Kong News. These corpora contain 46 ,692 Chinese news stories along with their SYSTRAN translations into English.We focus on sentiment biased topic detection. However  , few researches consider the utilization of sentiment in the TDT domain.This is probably the reason that TDT annotators included the documents in the topic. However  , the documents of Theme 4 mentioned that Russian President Vladimir Putin was the target of the legislative bill because he used inappropriate foreign words when speaking to the families of the sailors killed in the submarine disaster.Moreover  , all developers reported they felt comfortable—4 points on average on a 5-point Likert scale between very uncomfortable 1 and very comfortable 5—implementing the annotator in GERBIL. This result in itself is of high practical significance as it means that by using GERBIL  , developers can evaluate on currently 11 datasets using the same effort they needed for 1  , which is a gain of more than 1100%.Identities with pins that have low reputation URLs: We measure reputation of URLs associated with pins using the Web of Trust WoT 15  website; which computes the reputation of different websites based on various metrics  , and input from the community . For example  , a small fraction 1% of Pinterest identities have a vast majority 65% of their pins blocked  , and these identities are more likely to be untrustworthy.The association between document records and references is the basis for a classical citation database. Citebase harvests OAI metadata records for papers in these archives  , as well as extracting the references from each paper.Though not matching our wish list  , the TDT-2 corpus has some desirable properties. We used the TDT-2 corpus for our experiment.The optimal configuration 1 was used for participation in the HTD task and outperformed all other participants see table 1. The TDT 3 dataset roughly 35 ,000 documents was used as a preparation for participation in the trial HTD task of TDT 2004.'s algorithm12 were proposed several years ago  , in terms of empirical results  , it is still one of the best algorithms in TDT evaluations. Although  , Yang et al.Second  , Pinterest users can pin an organization's content to their personal pinboards. First  , the organization itself can create an account  , set up Pinboards  , and add pins.We follow the general lead of the original TDT-2 benchmark evaluation schema. Each topic also comes with one " seed " story that is presumed to be representative for the topic.The AIDA annotator as well as the " Illinois Wikifier " will not be available in GERBIL since we restrict ourselves to webservices. 's initial work 7 in 2014  , GERBIL's community effort led to the implementation of overall 6 new annotators as well as the before mentioned generic NIF-based annotator.In story link detection  , the simplest case  , the comparison is between pairs of stories  , to decide whether given pairs of stories are on the same topic or not. All TDT tasks have at their core a comparison of two text models.The algorithm we propose relies on a parameter α to balance the replication between documents and postings. If the query only contains one term  , then the replication operation is trivial  , and the algorithm determines that tdt 1 should be w. However  , if the query contains several terms  , then the algorithm has to decide whether it should replicate documents or posting lists.In order to empirically estimate the magic barrier  , a user study on the real-life commercial movie recommendation community moviepilot 4 was performed. After 20 opinions were collected the next button terminated the study.Lowering tpt decreases the lowest score associated to t in FIi. By lowering tdt  , RIP decreases the highest scores associated to t for a non local document.We estimate the total number of questions in Quora for each month by looking at the largest qid of questions posted in that month. Since reading the question does not update this " latest activity " timestamp  , this timestamp can estimate posting time for unanswered questions.As our testbed we use the AlgoViz Portal 1 which collects metadata on Algorithm Visualizations and provides community support. The model takes into account a user's page viewing history  , page viewing trends captured using DSNs  , and text similarity between page titles.This dataset  , from the German movie-rental site MoviePilot  , was released as part of the We overcome this by using a dataset that contains individual user preferences and their group membership.17 That is  , the AIDA team discourages the use because they constantly switch the underlying entity repository  , and tune parameters. The authors publish a key-protected webservice 14 as well as their 43  , GERBIL will not use the webservice since it is not stable enough for regular replication purposes at the moment of this publication .However  , these algorithms can be integrated at any time as soon as their webservices are available. The AIDA annotator as well as the " Illinois Wikifier " will not be available in GERBIL since we restrict ourselves to webservices.Unlike traditional social bookmarking  , pinning on Pinterest does not involve creating an explicit vocabulary of tags to describe the image. Towards this end  , we revisit the notion of agreement in the context of Pinterest.These transcriptions are used to assess baseline retrieval performance. There is no high-quality human reference transcription available for TDT- 2 -only " closed-caption " quality transcriptions for the television sources and rough transcripts quickly made for the radio sources by commercial transcription services.The experimental results show that our approach can improve the base algorithm significantly with better precision  , recall and conversion rates. We evaluate our algorithm on the purchase history from an e-commerce website shop.com.Thereafter  , we present the GERBIL framework. We begin by giving an overview of related work.The system output consists of two parts. A more detailed description of the task and evaluation metric can be found in the TDT-2002 evaluation plan 9.These data sets are text streams with only two or three sentences in each segment. Some researchers developed text segmentation methods targeting at data sets from TDT corpus 19.The objective in the design of the printed version of the collection was to create story hardcopy similar in style to newspaper clippings. The scanned document collection was based on the 21 ,759 " NEWS " stories in TDT-2 Version 3 December 1999.Figure 6shows these curves as a function of the cache size k for MAPCLICKS and BRIGHTKITE  , and for comparison  , SHAKE- SPEARE and YES. We also compute a separate baseline to account for the most heavily consumed items: we calculate and report the fraction of hits when the cache is fixed to always contain the top k most frequently consumed items.We have conducted an extensive study comparing different clustering algorithms for building the topic representation . This approach is based on the system used by the University of Massachusetts at Amherst at the latest TDT workshop 3.This phenomenon is the most pronounced on RateBeer Figure 5: Experienced users agree more about their ratings than beginners. One explanation is that the 'best' products tend to be ones that require expertise to enjoy  , while novice users may be unable to appreciate them fully.Our study necessitates highresolution timestamps  , thus we need to crawl pins while the timestamps are displayed in hours. The 24-hour requirement stems from the fact that Pinterest displays relative timestamps with decreasing resolution  , i.e.  , a pin from an hour ago will display " posted 1 hour ago  , " whereas a pin from yesterday will display " posted 1 day ago. "An interesting fact which helps to understand the Pinterest community is that in its earliest days  , the sign up was restricted to invitees only. Every pin posted must have a description and those which are not uploaded must have a direct link to its original source in the web.As part of the TDT research program  , about 200 news topics were identi£ed in that period  , and all stories were marked as onor off-topic for every one of the topics. Our evaluation corpus is built from the TDT-2 corpus 8  of approximately 60 ,000 news stories covering January through June of 1998.Often data providers will export records from sources that are not Unicode-based. As a developing service Citebase often needs to completely re-harvest its metadata  , and using a local mirror avoids repeatedly making very large requests to source archives.It was obtained by repeatedly crawling Pinterest: To discover new pins  , each of the 32 category pages was visited once every 5 minutes   , and the latest pins of that category were collected. Our dataset is derived from a previous study 38  , and includes nearly all activities on Pinterest between Jan 3–21 2013.The Celestial mirror is used within Southampton by Citebase Search. While Celestial is a distinct  , freely-downloadable software package  , at Southampton University 3 a mirror of Celestial hosts a copy of the metadata from 161 different OAI archives OAI-registered archives including the OAI-registered eprints.org archives  , plus any unregistered eprints.org installations found  , and active archives registered with the Repository Explorer 9.Since OpenStreetMap is a prominent example of volunteered geographic information VGI 7  , LinkedGeoData knowledge reflects the way in which the environment is experienced 8 . LinkedGeoData uses the information collected by the OpenStreetMap project with the aim of providing a rich integrated and interlinked geographic dataset for the Semantic Web.Pinterest also has many social components: users add boards to subject categories ,'repin' objects from other users to their own boards  , " like " or comment on objects  , and " follow " boards and other user's activity. Images uploaded via the bookmarklet contain a link back to the source website  , but perhaps not to the original  , as images may be used and reused several times on the Web.Exhaustive relevance judgments are provided for several topics in TDT-2. The articles tend to be several hundred to a few thousand words long  , while the audio clips tend to be two minutes or less on average.The first is to explore the website and follow users they find interesting. Users can create connections to other users on Pinterest in two ways.The first way is pinning  , which imports from a URL external to pinterest.com. Images added on Pinterest are termed pins and can be created in two ways.EconStor content has also been published in the LOD. Our research is based on the EconStor 2 repository  , the leading German Open Access repository for economics which is maintained by ZBW.Syntactic parse of user queries can provide clues for when the word order constraint can be relaxed. Nallapati and Allan 5  represent term dependencies in a sentence using a maximum spanning tree and generate a sentence tree language model for the story link detection task in TDT.A snapshot of this dataset was taken in March 2007 containing 263 ,619 publications and from this 36 previous monthly snapshots were generated with the first one March 2004 containing 174 ,786 publications. Citebase holds articles from physics  , maths  , information science  , and biomedical science and contains over 200 ,000 publications.To alleviate this problem  , GERBIL allows adding additional measures to evaluate the results of annotators regarding the heterogeneous landscape of gold standard datasets. At the moment  , those measures ignore NIL annotations   , i.e.  , if a gold standard dataset contains entities that are not contained in the target knowledge base K and an annotator detects the entity and links it to any URI  , emerging novel URI or NIL  , this will always result in a falsepositive evaluation.In the future  , we would like to extend this approach to other spoken document classification tasks. A research over TDT database 5 is being carried out.These surrogates are then saved in personal collections  , called " pinboards " on Pinterest. Users of a social collecting site create and annotate surrogates of digital objects found on the Web  , such as photographs or webpages.Due to the fact that the Nashville is just 47.8 miles further than the Clarksville in the state of Tennessee  , this page is judged as a relevant suggestion. On the other hand  , Model-Text provides the wikitravel page of the " Nashville " city in the state of Tennessee as the 1st suggestion in the ranking.For those reasons  , the TDT corpus is split into training and test information at a different point for each event. Unfortunately  , events occur at different times  , meaning that it is nearly impossible to use the same training and test set for each event.Results show that TDT was positively correlated with usefulness  , meaning that TDT is a reliable indicator of usefulness; topic knowledge was not found to help in inferring usefulness. One type is total dwell time TDT  , which is the accumulated time a user spent on a document when seeing it multiple times.TDT project has its own evaluation plan. The micro-average by summing all contingency tables of all events  , and then compute the three measures  and the macro-averageby averaging the three measures of all events are generally used measures.