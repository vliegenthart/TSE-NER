5kudos to Andreas Langegger for the screen shot  , that generates statistics for datasets behind SPARQL-endpoints and RDF documents. Further  , we have gathered that SCOVO is used in the RDFStats framework 15   , see Fig.The FedWeb 2014 collection contains search result pages for many other queries  , as well as the HTML of the corresponding web pages. Participants have to rank the given 149 search engines for each test topic without having access to the corresponding search results.Since this context e.g.  , surrounding code snippets  , the complete answer   , or the corresponding question is available on Stack Overflow  , it would be possible to display it along with an insight sentence. Another 15% of the ratings indicated that more context was required for the sentence to be understandable.The community strongly discourages questions which could generate chit-chat  , opinions   , polls etc. 2 Stack Overflow has detailed  , explicit guidelines on posting questions and it maintains a firm emphasis on following a question-answer format.Thus  , for each theme  , sentences have a representation that depends on the theme while the associated relevance judgment depends on the topic in hand. By extracting a generic query for each theme defined as the most frequent terms of that theme  , we then characterize sentences in the latter by taking 12 features used in the Letor datasets 6 as well as a feature produced by a bigram language model proposed in the top performing system at DUC 2006 4.These are the two Wikia encyclopedias with the largest number of articles evaluated by users regarding their quality. From the Wikia service  , we selected the encyclopedias Wookieepedia  , about the Star Wars universe  , and Muppet  , about the TV series " The Muppet Show " .We call these sentences insight sentences. To fill this gap  , we compare techniques for automatically extracting sentences from Stack Overflow that are related to a particular API type and that provide insight not contained in the API documentation.For example  , most of the 10 news sites  , which are used for the current GeoTopics  , have sidebars and footers in their articles  , which cause falsematching problems e.g.  , 'NASDAQ' was ranked high because it is appeared on the side bars in many of the news articles. Although the high-level processing steps are the same extracting articles  , filtering and classifying them  , and generating the HTML report  , the selection and coordination of the information management services need to be flexible and reconfigurable to handle dynamic situations.The configuration can determine the replay policies  , such as whether to emulate the networking latencies. In the replaying stage  , the data in WPBench Store are fed to browsers by a proxy according to the local configuration so that browsers could obtain the Web content as if they were actually from the Internet.They find that programming languages are a mixture of concepts and questions on Stack Overflow are concerned with the code example rather than the application domain. Allamanis and Sutton perform a topic modeling analysis on Stack Overflow questions to combine topics  , types and code 5.However  , accurate estimation of visit probabilities is impossibile due to the lack of login and browsing data of TripAdvisor users. Meanwhile   , we want to obtain a visit probability sequence that is similar at least in trend to the real data.For list-wise LETOR  , we are using ListNet 6  , which uses a simple one layer Neural Network with Gradient Decent to optimize a defined list-wise loss function based on " top one probability " . We expect the pairwise methods to perform better than point-wise approaches  , as the features collected from the error pairs are more meaningful as they define relative distances.Hence  , it is important to perform a longitudinal study about deleted questions on Stack Overflow. Therefore  , despite the presence of comprehensible and explicit question posting guidelines – Stack Overflow receives a high number of extremely poor quality questions which are not fit to exist on its website.Overall  , there are 492  , 104 communities withheld from Orkut data set one community withheld for each user. We set k to be 1001  , so that the number of random communities selected for ranking evaluation is 1000.The result pages of Ask.com with fact answers can be accessed at http://lepton.research.microsoft.com/facto/doc/ask_answer.zip. For example  , for query {raven symone gives birth} it answers " Raven-Symoné is not and has never been pregnant according to reports "   , which shows it knows what has not happened besides what has.To facilitate the crowdsourcing of documentation  , the Stack Overflow community explicitly encourages contributions where the person asking the question also provides an answer. Answers on Stack Overflow often become a substitute for official product documentation when the official documentation is sparse or not yet existent 5 .Hotel service characteristics: We extracted the service characteristics from the reviews from TripAdvisor. We extracted these characteristics within an area of 0.25-mile  , 0.5 mile  , 1-mile  , and 2-mile radius.In our work  , a digitized volume corresponds to a collection of objects  , including scanned images of pages  , OCRed text  , manually-generated metadata  , among others. The system detects various types of structural information  , including sentence boundaries  , filler words  , and disfluencies  , within speech transcripts using lexical  , prosodic  , and syntactic features.We find that the superior retrieval effectiveness of GRH+NPQ is maintained when the hashcode length is varied between 16-128 bits for both LSH and PCA projections Figure 3a-b on CIFAR-10. Each of these increases are found to be statistically significant using a Wilcoxon signed rank test p-value < 0.01.The main contributions of this work are the conceptualization of insight sentences as sentences from one documentation source that provide insight to other documentation sources and a comparative study of four different approaches for the extraction of such insight sentences  , as well as a list of factors that can be used to distinguish insight sentences from sentences that are not meaningful or do not add useful information to another documentation source. These results indicate that taking into account Stack Overflow meta data as well as part-of-speech tags can significantly improve existing unsupervised approaches when applied to Stack Overflow data.Despite their different topics of interest  , Quora and Stack Overflow share many similarities in distribution of content and activity. This is the focus of the rest of our paper  , where we will study different Quora mechanisms to understand which  , if any  , can keep the site useful by consistently guiding users to valuable information.Since OpenStreetMap is a prominent example of volunteered geographic information VGI 7  , LinkedGeoData knowledge reflects the way in which the environment is experienced 8 . LinkedGeoData uses the information collected by the OpenStreetMap project with the aim of providing a rich integrated and interlinked geographic dataset for the Semantic Web.the top 100 answers according to their rating  , obtained from a sample of Stack Overflow different from the one we use for evaluation in Section 4.1. Thus  , the feature ty-klg compares the language model of the answer to the language model of a group of answers considered good i.e.We chose the EUSES corpus because it is by far the largest corpus that has been widely used for evaluation by previous spreadsheet research studies. Another threat to external validity of our evaluation concerns the representativeness of spreadsheets in the EUSES corpus and collected in our case study.Figure 9 shows various quantities of question quality indicators for 'closed' and deleted questions on Stack Overflow . Community Value.In the figure  , we plotted the results for an exemplary hotel from the TripAdvisor database. In Figure 4  , we analyze the effect of a varying λ on the runtime.Additionally  , these datasets contain textual reviews  , which we used to understand and describe the results of our method. The data consists of the IDs of the products/services to be rated as well as the related user IDs who evaluated them with star rating scores from 1 up to 5 at different timesteps in the case of TripAdvisor  , the rating scores range from 0 up to 5.Answers  , Stack- Overflow or Quora. Crowdsourcing  , where a problem or task is broadcast to a crowd of potential contributors for solution  , is a rapidly growing online phenomenon being used in applications ranging from seeking solutions to challenging projects such as in Innocentive or TopCoder  , all the way to crowdsourced content such as on online Q&A forums like Y!4 In Figure 7 we have already illustrated the distribution of ratings over time for the hotel Punta Cana Princess evaluated on TripAdvisor. However  , reviews shortly after the second anomaly note that " .. they are now doing some construction to try and fix things .. "   , giving hint to an improvement of the airport which may have caused the better evaluation almost no 1 star ratings at the end of the rating behavior.Section 3.2.1  , we considered all the Stack Overflow users and their questions and answers. To create the user graph cf.On the contrary  , the images in TinyImage data set have low-resolution. The LabelMe data set contains high-resolution photos  , in fact most of which are street view photos.The standard Dublin Core format is not suitable for RefSeq sequence data. From the NCBI site  , 4032 RefSeq records linked from our MEDLINE subset and that contain gene sequences were downloaded.We observe an interesting behavior: Starting from very small values of λ  , an increase in λ also increases the runtime. In the figure  , we plotted the results for an exemplary hotel from the TripAdvisor database.In addition  , questions which are dormant viz. Therefore  , questions on Stack Overflow which are extremely off topic or very poor in quality are deleted from the website 1.One very important issue is what we call " statisticalpresentation fidelity " . Further  , our ongoing work focuses on broadening the deployment base available 17   , making converters from and to SCOVO available  , and extending the framework itself.Controlled experiments An early experiment by Tenny 29 compared two types of procedures inlined or external procedures and the occurrence of comments. Additionally  , there was a strong correlation between usage of API elements and the amount of discussions on Stack Overflow.We also perform our experiments on the entire unadulterated dataset of Stack Overflow. In addition  , Stack Overflow consists of millions of questions with thousands of topics recall that there are 34 ,000+ tags.Other work has focused on detecting and preventing API documentation errors. In contrast  , we connect different forms of documentation through heuristics that match Stack Overflow threads to API types  , and instead of FAQs  , SISE produces insight sentences.A study conducted last year based on data from the U.S. Bureau of Labor Statistics shows that there are currently as many as 11 million end-user programmers in the United States  , compared to only * This work is partially supported by the National Science Foundation under the grant ITR-0325273 and by the EUSES Consortium http://EUSESconsortium.org. In the original scenario  , once a template was created and loadedWe chose to standardize this issue  , using the same criterion used by most evaluation tools  , e.g.  , those available for the Letor 3.0 and 4.0 and Microsoft datasets  , in order to allow fairer comparisons. assume the value of 1 for these cases  , which may lead to higher values of NDCG 2.BDBComp has several authors with only one citation. In all other four situations there is some drop in effectiveness .the usage of SCOVO  , let us assume we want to model airline on-time arrivals and departures. We note that the complete example  , including the exemplary queries in an executable form  , is available at http://purl.org/NET/scovoResults indicate that  , while respondents only considered the retrieved discussions fairly relevant to the fragments from where the queries were generated  , they almost totally agreed about the complementarity of the provided information. On the other hand  , if we look at Figure 7b  , we notice that the distribution is polarized towards the maximum value— first quartile  , median and third quartile equal to 3—with 14 82% of the total of the videos where the Stack Overflow discussions were considered as complementary.2  is that sentences extracted by our linking approach always reflect the latest content available on Stack Overflow. The advantage of using the Stack Overflow API over the Stack Overflow data dump used in previous research such as that of Bacchelli et al.For example  , each insight sentence could be accompanied by an expandable widget which shows the entire thread on Stack Overflow from which the insight sentence originated. Since this context e.g.  , surrounding code snippets  , the complete answer   , or the corresponding question is available on Stack Overflow  , it would be possible to display it along with an insight sentence.More details and further experimental results are available at http://swa.cefriel.it/geo/eswc2016.html. Furthermore  , according to global OpenStreetMap statistics 1   , Italy and UK are ranked 7th and 10th for number of created spatial objects  , and 4th and 5th for density of created spatial objects per square kilometer.University dragon 16 Their result merging runs were based on normalizing the document score based on the resource score by a simple multiplication. Ultimately  , the rank based resource score combined with the document score on the RS baseline provided by the FedWeb team performed the best drexelRS7mW.Since the Web content  , user interactions  , and networking are exactly the same for these browsers  , WPBench produces benchmark results fair to different Web browsers. These interactions are emulated during benchmarking browsers by instrumented JavaScript which is independent of Web browsers.The list-wise approach was proved to be more effective than pairwise and point-wise approaches  , as its optimization criterion is closer to the actual evaluation metrics. For list-wise LETOR  , we are using ListNet 6  , which uses a simple one layer Neural Network with Gradient Decent to optimize a defined list-wise loss function based on " top one probability " .Ask.com has a feature to erase the past searches. Search engines typically record the search strings entered by users and some search sites even make the history of past searches available to the user.Instead of artificially constructing Web content based on a model of typical Web 2.0 applications  , WPBench uses the real data from users' actually browsing and interacting with Web 2.0 sites. In this paper  , we report the benchmark called WPBench Web Performance Benchmark that we have recently designed and developed to measure the performance of browsers for Web 2.0 applications.This yields to complex SPARQL expressions  , as it will often require a verbose check to make sure that an item has only certain dimensions and no others. Using SCOVO in voiD allows a simple and extendable description of statistical information  , however  , a shortcoming has been identified: as scovo:Items are grouped into scovo:Datasets  , there is an implicit assumption that all items in such a dataset share the same dimensions. Resource selection: given a query  , a set of search engines/resources and a set of sample documents for each resource  , the goal of this task is to return a ranked list of search engines according to their relevance given the query. In FedWeb 2014  , participants are given 24 di↵erent verticals e.g.  , news  , blogs  , videos etc.We conduced 5-fold cross validation experiments  , using the partitions in LETOR. The idea is similar to that of sitemap based relevance propagation 24.It is based on a large and active community contributing both data and tools that facilitate the constant enrichment and enhancement of OSM maps. In particular  , OpenStreetMap OSM is an initiative for crowdsourcing map information from users.By means of a simple scenario  , we illustrate how Seahawk can help developers solving programming problems by leveraging Stack Overflow from within the Eclipse IDE. She first creates a socket by using the Socket class:The source tree ST is the only structure that our XPath evaluation and incremental maintenance algorithms require. From the source tree we can see that both fragments F2 and F3 are stored in the same site S2  , the nasdaq site.Some users are interested in highly unstructured text data OCRed from field journals  , or more conventional relational tables of data  , so BigSur does not require that these super-classes are used. This section of the schema is not mandatory.The Orkut graph is undirected since friendship is treated as a symmetric relationship. A user's vector has a 1 in any dimension that represents himself or anyone the user has listed as a " friend. "To assess the generality of using document similarities based on word embeddings for information retrieval in software engineering  , we evaluate the new similarity functions on the problem of linking API documents to Java questions posted on the community question answering cQA website Stack Overflow SO. The question score is larger than 20  , which means that more than 20 people have voted this question as " useful " .The first evaluation  , based on the LETOR datasets 17  , uses manual relevance assessments as ground-truth labels and synthetic clicks as feedback to BARACO. in two different ways.We import Stack Overflow documents from the public data dump provided as a set of XML file 5 . The Data Collection Mechanism component is responsible for gathering Q&A data from Stack Overflow.As future work  , we intend to evaluate the impact of the service in the expansion of BDBComp as well as on its sustainability. While several services exist with similar characteristics  , few  , if any  , comprehensive studies of such services have been reported in the DL literature.The four research questions RQ the study aims to answer are: RQ1: What are the perceived benefits and obstacles of using video tutorials ? The quality focus concerns i the perceived benefits and obstacles in using video tutorials during development and ii the quality of video fragments cohesiveness  , self-containment  , relevance to a query and Stack Overflow discussions relevance and complementarity to the video fragment  mined by CodeTube.For both CIFAR-10 and NUS-WIDE datasets  , we randomly sample 1 ,000 points as query set  , 1 ,000 points as validation set  , and all the remaining points as training set. All our experiments are conducted on a workstation with 24 Intel Xeon CPU cores and 64 GB RAM.In contrast  , Stack Overflow anonymizes all voters and only displays the accumulated number of votes  , which can be negative Sorted Topic Bucket By # of Followers Thus in our analysis of Quora  , we only refer to upvotes and disregard downvotes .Therefore   , Stack Overflow has attracted increasing attention from different research communities like software engineering  , human computer interaction  , social computing and data min- ing 6  , 9  , 10  , 21  , 22. Apart from existing as a question-answering website  , the objective of Stack Overflow is to be a comprehensive knowledge base of programming topics.Wilks manually disambiguated all occurrences of the word 'bank' within LDOCE according to the senses of its definition and compared this to the results of the cosine correlation. The occurrences of the defined word in all sentences whose vectors have the greatest similarity to the vector for a given sense are then assigned that sense7.Following LETOR convention  , each dataset is divided into 5 folds with a 3:1:1 ratio for training  , validation  , and test set. 16  , here we investigate whether a simple unweighted average is sufficient to give improve- ments.Section 4 identifies our research questions and our methodology. We review related work in Section 2  , and we introduce the key concepts of Stack Overflow in Section 3.As shown in Figure 2  , the documents selected by the two methods also exhibit very high similarity to each other. Both hedge and LETOR-like document selection methodology   , by design  , select as many relevant documents as possible .Second  , we with real-life spreadsheets the Institute of Software  , Chinese Academy of Sciences evaluation report in the EUSES corpus suffer which cover 21.6 putation smells reveal weakness and sheets. To repair a ous computation smell existing work on appropriate formula pattern in an array that suffers We evaluated our lyzed the EUSES corpus putation smells can formance of our smells.We concentrated on developing repositories for four different resources: Medline for biomedical literature  , Refseq for gene DNA sequence  , Refseqp for protein sequence and Swissprot for protein sequence. Our main goal for this project was to create and integrate different biomedical resources using OAI-PMH.Furthermore  , the combination of GRH+NPQ outperforms the adaptive thresholds allocation model VBQ of 3 by a relative margin of 27%. For example  , for LSH projections GRH+NPQ gains a relative increase in AUPRC of 60% over NPQ and 28% over GRH on CIFAR-10.When this signal is triggered  , the detector reports information about the object's allocation and deallocation sites  , as well as call stack and line number information for the instructions responsible for the use-after-free error. As with buffer overflow detection  , any writes to the watched address will generate a SIGTRAP signal.The spatial data is collected by the OpenStreetMap 5 project and it is available in RDF format. The goal of LinkedGeoData is to add a spatial dimension to the Semantic Web.All experimental results are averaged over 10 independent rounds of random training / validation / query partitions. Using normalized hyper-parameters described in Section 2.6  , the best hyper-parameters are selected by using the validation set of CIFAR-10.Six collections  , relevant to the assignment about television and film personalities  , from various archives were indexed: 1 a television program collection containing 0.5M metadata records; 2 a photo collection with 20K photos of people working at television studio; 3 a wiki dedicated to actors and presenters 20K pages; 4 25K television guides that are scanned and OCRed; 5 scanned and OCRed newspapers between 1900 and 1995 6M articles; and 6 digital newspapers between 1995 and 2010 1M articles. Collections.These primers are designed using a known normal sequence called the reference sequence  , which has been imported into our database by the Function Express Server from RefSeq. This software  , which is a wrapper around the popular Primer3 software package  , automatically designs primers for large numbers of genes in high throughput.The second part is conducted on the same Orkut data set to investigate the scalability of our parallel implementation. The first part is conducted on an Orkut community data set to evaluate the recommendation quality of LDA and ARM using top-k recommendations metric.The SVMRank 5 algorithm was used in this task and five-folds cross validation was done. Since a lot of features of LETOR we cannot get  , we droped those columns and then trained the ranking model.The Ionosphere Database consists of 351 instances with 34 numeric attributes and contains 2 classes  , which come from a classiication of radar returns from the ionosphere . We used the Ionosphere Database and the Spambase Database.The LETOR-like selection methodology also selects very similar documents  , since the documents selected are those that give high BM25 values and thus have similar characteristics. At the other end of the discrepancy scale  , infAP for small sampling percentages selects the most diverse relevant documents while it converges fast to the average discrepancy between documents in the complete collection.All these browsers can browse all the Web sites in WPBench normally except that IE 8 beta and Firefox 3.1 beta cannot browse one of them due to unsupported features used by the Web site. These browsers cover the most wellknown layout engines  , such as Trident and Gecko  , as well as several widely used JavaScript engines.We find a total of 9 ,350 undeleted questions on Stack Overflow. Table 7 shows some examples of undeleted questions on Stack Overflow.market  , we used data provided by TripAdvisor: The consumers that write reviews about hotels on TripAdvisor also identify their travel purpose business  , romance  , family  , friend  , other  and age group 1317  , 18-24  , 25-34  , 35-49  , 50-64  , 65+. For our experimental evaluation  , we instantiated our model framework using as target application the area of hotel search.Subsets of documents are chosen according to the six methods at different percentages of the complete document collection in our case the depth-100 pool  , and features are extracted from the selected query-document pairs. We employ five different document selection methodologies that are well studied in the context of evaluation  , along with the method used in LETOR for comparison purposes.investigated the recent phenomenon of crowd documentation   , as measured by the questions about API elements asked and answered on the Stack Overflow website 20. Parnin et al.There are a total of 36 ,643 tags on all questions in Stack Overflow. We analyze the tag distribution of closed and deleted questions and compare them to the overall tag distribution on Stack Overflow.The similar shapes for training and validation suggest that we are not overfitting. Secondly these all peak at or close to an α of 0  , indicating that  , for LETOR OHSUMED at least  , we don't get any benefit from explicit use of the model's uncertainty information .Dataset. Note that streams for synthetic data differs from NASDAQ data in terms of the lag and the missing update distributions.According to this methodology  , documents in the complete collection are first ranked by their BM25 scores for each query and the top-k documents are then selected for feature extraction.  LETOR: For comparison purposes  , a LETOR-like document selection methodology is also employed.In the 2 years since its foundation in 2008  , more than 1 million questions have been asked on Stack Overflow  , and more than 2.5 million answers have been provided. For the domain of software development   , the website Stack Overflow 4 facilitates the exchange of knowledge between programmers connected via the Internet .Related to our solution for linking Stack Overflow threads to API types is the work by Rigby and Robillard 30. in that we focus on single sentences from Stack Overflow that are relevant to an API type instead of a code snippet.The corpus has 4498 spreadsheets collected from various sources. The frequency of occurrences of cp-similar regions has been shown by the analysis carried out on the EUSES spreadsheet corpus as reported in 13.Before diving into main analytical results of our work  , we begin in this section by first describing our data gathering methodology and presenting some preliminary results. We also analyze some high level metrics of the Quora data  , while using Stack Overflow as a baseline for comparison.Formal releases of these two broswers are expected to fix these problems. All these browsers can browse all the Web sites in WPBench normally except that IE 8 beta and Firefox 3.1 beta cannot browse one of them due to unsupported features used by the Web site.Orkut is a general purpose social network. In this social network the friendship connections edges are directed.3.2.2 RQ2: To what extent are the extracted video tutorial fragments cohesive and self-contained ? For instance  , given the fragments of a video 16 showing how to code a layout in XML for Android  , CodeTube retrieved and suggested Stack Overflow discussions concerning typical problems a developer could encounter  , like a button not showing up 17   , or not well-formed XML 18 .In a comparative study with eight software developers to evaluate the meaningfulness and usefulness of the insight sentences  , we found that our supervised approach SISE resulted in the highest number of sentences which were considered to add useful information not found in the API documentation . We define insight sentences as those sentences on Stack Overflow that are related to a particular API type and that provide insight not contained in the API documentation of the type.The experimental results provided in the LETOR collection also confirm this. Note that it is commonly believed that Rank-Boost performs equally well as Ranking SVM.A simple RefseqP XML schema was created for the RefSeqP OAI repository. Also  , 2072 Refseq records linked from our MEDLINE subset and that contain protein sequences were downloaded.Applied to insight sentence extraction  , the idea is to create a summary of Stack Overflow threads related to an API type under the assumption that the reader is already familiar with the type's API documentation. The goal of update summarization is to produce a summary of a new document under the assumption that the reader is already familiar with the content of a given set of old documents.They find nine attributes of good questions like concise code  , links to extra resources and inline documentation. analyze questions on Stack Overflow to understand the quality of a code example 20.The earlier can be used to capture more information pertaining to the creation of a particular statistical item; – Defining sub-properties of using SCOVO-min and max. The latter is of particular help if an existing taxonomy or thesaurus is used as a base.For instance  , there are a number of badges for encouraging new users that nearly everyone obtains  , such as the " Editor " badge for contributing a first edit. There are over 100 different badges on Stack Overflow  , which vary greatly in how difficult they are to achieve.However  , despite of the presence of question posting guidelines and an ebullient moderation community  , a significant percentage of questions on Stack Overflow are extremely poor in nature. Stack Overflow is driven by the goal to be an exhaustive knowledge base on programming related topics and hence  , the community would like to ensure minimal possible noise on the website.Unfortunately  , there are rarely links between online resources and official API documentation: the official documentation does not link to the examples that could help developers  , and the examples rarely link to the documenta- tion. In fact  , they found that 87% of Android classes were referenced in Stack Overflow answers .Running AmCheck over the whole EUSES corpus took about 116 minutes. We let the officers study these smells before our interview.We extract a set of tourist attractions in the metadata of OpenStreetMap. A large value of F1 measure indicates a better clustering.by better interlinking the data with other Linked Data datasets and providing a proper ontology for querying. We also aim at improving the OpenStreetMap data usage scenario  , e.g.In the opposite direction  , Figure 6shows how the browser extension augments the official Android documentation with Stack Overflow examples. When the developer hovers their mouse over mChronometer as they are in Figure 5  , they are presented with a dynamic popup that contains links to the official Android Chronometer API documentation  , to the source code for Chronometer  , and links to 18 other Stack Overflow posts that also use Chronometer.The results of the state-ofthe-art algorithms are provided in the LETOR 3.0. In LETOR 3.0 package  , each dataset is partitioned into five for five-fold cross validation and each fold includes training   , testing and validation sets.We describe details below. A publicly available dataset periodically released by Stack Overflow  , and a dataset crawled  from Quora that contains multiple groups of data on users  , questions   , topics and votes.For reference  , we report the baseline score comparison in the following format: 3. 4 Our RankBoost baseline is comparable but different from LETOR18  , mainly due to different feature normalization mean-variance vs. 0  , 1 scaling.We use a non-linear Random Forest regression model for our experiments. To create features for the selection framework  , we use the published test runs 1 for these rankers to obtain the document scores for top 10 ranking documents  , and the list of 64 features that are available as part of LETOR.by using distributed IR test collections where also the complete description is available  , or the samples obtained by considering the diverse query sets for sampling in the FedWeb test collections; – the use of diverse weighting scheme at document level  , e.g. – the effect of sampling strategy on resource selection effectiveness  , e.g.To do so  , we test against three publicly available image datasets: 22k Labelme consisting of 22 ,019 images represented as 512 dimensional Gist descriptors 8; CIFAR-10 a dataset of 60 ,000 images represented as 512 dimensional Gist descriptors ; and 100k TinyImages a collection consisting of 100 ,000 images  , represented by 384 dimensional Gist descriptors  , randomly sub-sampled from the original 80 million tiny images dataset. We evaluate the effectiveness of NPQ in the domain of image retrieval  , although our approach is general and can be used for other types of data for example  , text  , video.In all other four situations there is some drop in effectiveness . In BDBComp see Table 9  , the effectiveness is not hurt only when we do not add new examples to the training data.By comparing against this gold standard  , we evaluate the lexicons constructed using different methods. In this section  , we describe how we create a gold standard by performing human annotation on a data set of hotel reviews from TripAdvisor.In LETOR  , there are a total of 16 ,140 query-document pairs with relevance judgments  , and 25 extracted features. definitely  , possibly  , or not relevant.The results of RankSVM  , RankBoost  , AdaRank and FRank are reported in the Letor data set. For AdaRank  , we use the version for optimizing the mean average precision for comparison in this study.We also applied a pattern-based approach that has been successfully used to detect and recommend fragments of API documentation potentially important to a programmer who has already decided to use a certain API element 7  , with similar results. Figure 1: Stack Overflow Example meaningful on their own without their surrounding code snippets or the question that prompted a given answer.We argue that question owners of author-deleted questions exhibit such a behavior as they want to maintain a healthy reputation earned on Stack Overflow 10 . The above observations show that despite being less experienced on the website  , question owners of author-deleted questions have more prior posts and have higher question scores on deleted questions than those of moderator-deleted questions.But no explicit social relationships are maintained in TripAdvisor   , so we need to construct an implicit influence network and learn the influence probabilities on the network. In the formulation of the participation maximization problem Section 4  , the social influence network is treated as an input of the problem.Additionally  , from the application of SCOVO in voiD we have learned that there is a demand for aggregates. It is for sure possible to concatenate single dimensions used on the scovo:Item-level—for example concluding from the range of the four quarters ex:Q12006 to ex:Q42006 that the dataset actually is referring to the year 2006.We conducted 5-fold cross validation experiments  , following the guideline of Letor. In total  , there are 44 features.The data is extracted through a XML dump importer and stored in a relational database for performance reasons. We import Stack Overflow documents from the public data dump provided as a set of XML file 5 .We augmented these by pulling from a few repositories on GitHub that were aimed at collecting source code examples. We obtained our snippets from the Stack Overflow data repository provided for the 2013 MSR Challenge 3.Each spreadsheet column in the EUSES corpus typically contains values from one category  , so columns were our unit of analysis for identifying data categories. We manually grouped the 66 unvalidated text fields into 42 categories   , such as person  , organization  , and education level.We start with generating a profile per topic—here  , a topic is a tag associated with a question on Stack Overflow—by retrieving all questions that are associated with the topic along with all comments  , answers  , and comments to answers associated with the question. It aggregates a user's textual relevance scores of answers as an indication of expertise.Depending on the user's option  , three possible scenarios can be generated from this pattern. With further customization  , the user can enable three possible methods for refreshing data from Nasdaq.In contrast to these tools  , SISE links natural language sentences from Stack Overflow to API documentation. 34 introduced Baker  , an iterative  , deductive method for linking source code examples to API documentation.The most common questions include how-to questions and questions about unexpected behaviors. Our preliminary findings indicate that Stack Overflow is particularly effective at code reviews  , for conceptual questions and for novices.A key observation is that given the broad and growing number of topics in Quora  , identifying the most interesting and useful content  , i.e. Despite their different topics of interest  , Quora and Stack Overflow share many similarities in distribution of content and activity.It is worth noting that the quality of and issues with cross references between multiple biological data sources is not well documented and often requires extensive experimentation in collecting and integrating data from these sources. EBI's Genome Reviews 14 had better annotations and cross references than RefSeq  , and therefore was selected as IMG's main source for public microbial genome data.Table 11shows the accuracy of FACTO. The result pages of Ask.com with fact answers can be accessed at http://lepton.research.microsoft.com/facto/doc/ask_answer.zip.The last step in the data pre-processing of CodeTube consists in indexing both the extracted video fragments and the Stack Overflow discussions  , using Lucene 9   , where each video fragment is considered as a document. We mined and extracted discussions related to the topics of the extracted video tutorials  , pre-processed them to reduce the noise  , and made them available to CodeTube.Our view is that we will eliminate whatever senses we can  , but those which we cannot distinguish or for which we have no preference  will be considered as falling into a word sense equivalence class. In the experiment in disambiguating the 197 occurrences of 'bank' within LDOCE  , Wilks found a number of cases where none of the senses was clearly 'the right one' Wilks 891.image or video files  , so the big-documents for such engines by concatenating the text from all its sampled pages would be empty  , which causes such resources would not be selected for any queries. In addition  , for some search engines  , like the resource e122 Picasa in FedWeb 2014  , all the sampled pages are non-text files  , e.g.Finally  , we evaluate the proposed method on LETOR 3.0 benchmark collections1. The empirical results indicate that even with sparse models  , the ranking performance is still comparable to that of the standard gradient descent ranking algorithm.Actually  , the results of Ranking SVM are already provided in LETOR. In comparison with this baseline  , we can see whether Relational Ranking SVM can effectively leverage relation information to perform better ranking.We compare the NDCG-Annealing algorithm with linear ranking function described in section 3 with baselines provided in the LETOR 3.0 datasets. The NDCG-Annealing algorithm is general to be applied to both linear and non-linear ranking functions at test time.For brevity  , we report MAP as the measure of system performance . This latter question is a matter of some interest  , as the MAP scores for LETOR 3 approach the 65% considered achievable with human-adjudicated relevance 5.On SemSearch ES  , ListSearch and INEX-LD  , where the queries are keyword queries like 'Charles Darwin'  , LeToR methods show significant improvements over FSDM. The overall improvements on all queries can be as large as 8%.The document collection is a subset of MEDLINE  , a database on medical publications. The OHSUMED data set in LETOR has been derived from the OHSUMED benchmark data 16 for information retrieval research.Additionally  , we employed Triplify to publish the 160GB of geo data collected by the OpenStreetMap project. We tested and evaluated Triplify by integrating it into a number of popular Web applications.Stack Overflow is another successful Q&A site started in 2008. Overflow.We see that tags on 'closed' questions are a subset of the overall tags which occur in regular questions. Figure 10shows the venn diagram of tag distributions of questions on Stack Overflow.We review related work in Section 2  , and we introduce the key concepts of Stack Overflow in Section 3. The remainder of this paper is structured as follows.In fact  , only 15% of the ratings for sentences extracted by SISE indicated that the sentence did not make sense. The results of our evaluation suggest that the context of sentences will play an important role when complementing API documentation with sentences from Stack Overflow.To highlight key results  , we use comparisons against Stack Overflow  , a popular Q&A site without an integrated social network. In this paper  , we perform a detailed measurement study of Quora  , and use our analyses to shed light on how its internal structures contribute to its success.the number of query topics  , on ranking performance by conducting comparison study with varying the value of n. Figure 3show the performance of TRSVM on Letor dataset with varying values of n in terms of MAP. In this experiment  , we explore the effects of different settings of the parameter n  , i.e.Hence  , we only compare the proposal algorithm with Ranking-SVM  , but not Rank-Boost. The experimental results provided in the LETOR collection also confirm this.We are not surprised that our systems did not work well on diversity metrics as shown in Table 3  , because the diversity module of our system was not functioning as we expected and eventually we chose to not to include it in our pipeline. This also suggests that our LETOR framework is effective in improving the overall precision.UiSPP Linear combination of the Document-centric and Collection-centric models. This value was chosen based on some preliminary experiments we performed on the FedWeb 2012 test collection Nguyen et al.  , 2012.So  , when we merge the group profiles the items considered in training were the items rated by at-least one member who has a group identifier. We note that the MoviePilot data does not contain the group information for all the users in the training data.Many " viral " videos take off on social media only after being featured on broadcast media  , which often follows their being highlighted on intermediary sites such as Reddit or Buzzfeed. The emergent media ecology is a mix of old and new media which is not strictly segregated by platform or even by device.Understanding the interactions on Q&A websites  , such as Stack Overflow  , will shed light on the information needs of programmers outside closed project contexts and will enable recommendations on how individuals  , companies and tools can leverage knowledge on Q&A websites. Future work will have to investigate the implications of this blurred line between Q&A websites and technical mini-blogs.Point annotations  , for example  , are originally stored as comma separated property-values assignments in a BLOB column within the database. In order to publish the OpenStreetMap data  , we performed some preprocessing of the data structures.BM25 instead of the TF·IDF; – the use of external evidence to obtain a more effective information need representation. by using distributed IR test collections where also the complete description is available  , or the samples obtained by considering the diverse query sets for sampling in the FedWeb test collections; – the use of diverse weighting scheme at document level  , e.g.They used topic models to find a set API classes on iOS and Android documentation which were difficult for developers to understand. Wang and Godfrey analyze iOS and Android developer questions on Stack Overflow to detect API usage obstacles 25.The feature extraction step uses OCRed text and the bounding box information to calculate line features for every text line contained within a scanned volume. There are two steps in the automatic metadata generation process: feature extraction and metadata labeling.We define insight sentences as those sentences on Stack Overflow that are related to a particular API type and that provide insight not contained in the API documentation of the type. In an effort to bring documentation from different sources together  , we presented an evaluation of different techniques for extracting insight sentences from Stack Overflow.There are 106 queries in the collection split into five folds. We also analyze the results of our approach on a different dataset; OHSUMED 5 which is also available in Letor 16.Previously  , sentiment diversification was mainly applied to controversial topics which required opinionated documents to appear in retrieval results 7. Having this in mind  , FedWeb dataset seemed appropriate for our experiments as it provides the federated environment on which we could incorporate opinions in federated search.In fact  , they found that 87% of Android classes were referenced in Stack Overflow answers . have previously studied online resources and have found that they do a good job of covering APIs 15.We observe up to 25 fold speedups in this distributed setting for the Microsoft LETOR data set. Fold 1 n=723K Set 1 n=473K Set 2 n=35K Perfect Figure 4: The speedups of pGBRT on the cluster as a function of CPU cores.In particular  , OpenStreetMap OSM is an initiative for crowdsourcing map information from users. In conjunction with the widespread use of smartphones and GPS enabled devices  , this has resulted in a large number of RDF datasets containing geospatial information  , which is of high importance in several application scenarios  , such as navigation  , tourism  , and location-based social media.Since all insight sentences used in this paper were obtained from sets of ten Stack Overflow threads associated with an API type  , we would expect comparable results for any API type with at least ten threads on Stack Overflow. SISE will only work if a topic is discussed on Stack Overflow.The precision of manual annotation may be well guaranteed  , but it has some difficulties in the practical applications since we are facing Web-scale images and Web-scale concepts. The LabelMe project 19 also presents a tool to users to help manually assign tags to local regions of the images .Other recommenders have focused on recovering links between code and documentation 2  , with some focusing on recommending or enhancing API doc- umentation 37  , 40. Among the various informal documentation sources  , Stack Overflow has been used by many recommender systems 7  , 30–32  , 36  , 41.Stack Overflow questions contain user supplied tags which indicate the topic of the question. Question Topics.First-time and secondtime reviewers excluded. c TripAdvisor.As a result of this situation  , developers often turn to online resources such as Stack Overflow. Recent work has confirmed the popular belief that writing documentation and keeping it up to date is very difficult 8  , 9; consequently  , developers ignore the documentation that does exist and declare that " code is king " 17.In Letor  , the data is represented as feature vectors and their corresponding relevance labels . There are 16 ,140 query-document pairs with relevance labels.Instead of using proxy measures  , we preferred to let developers evaluate video fragments and their related Stack Overflow discussions. Threats to construct validity are mainly related to the measurements performed in our studies  , and Study I in particular .To evaluate the effectiveness of the proposed method  , we performed a systematic set of experiments using the LETOR benchmark collections OHSUMED  , TD2004  , and TD2003 and several evaluation measures MAP  , NDCG and precision . This information may turn the generated rules more discriminative and accurate.From the source data  , we generated two datasets for question identification. 1 We obtained 1 ,212 ,153 threads from TripAdvisor forum 6 ; 2 We obtained 86 ,772 threads from LonelyPlanet forum 7 ; 3 We obtained 25 ,298 threads from BootsnAll Network 8 .This is because the LETOR data set offers results of Linear Ranking SVM. For all the SVM models in the experiment  , we employed Linear SVM. The DjVu XML file retains the bounding box information of every single OCRed word  , from which we can estimate format features. Figure 3shows logical structure and bounding box information embedded within a DjVu XML document.2 Our experimental results show significant improvements over the baseline and validate the utility of using behavioral and time-aware features from multiple behavioral channels. To answer these questions we use data from Stack Overflow  , a CQA platform for programming-related topics.These techniques assign a numeric value to each sentence and return the top-ranked sentences as a summary. To answer the first research question  , we applied two state-of-the-art extractive summarization techniques— LexRank 13 and Maximal Marginal Relevance MMR 4 —to a set of API types  , their documentation   , and related Stack Overflow threads.dataset by merging the original partitions into a single set  , and splitting the sorted queries into 5 folds  , distributed using the same proportions: 3 folds for training  , 1 for validation and 1 for test. For comparative purposes  , considering that the Microsoft and LETOR datasets were designed for a folded cross-validation procedure  , we applied this same strategy to the YA- HOO!Ultimately  , the rank based resource score combined with the document score on the RS baseline provided by the FedWeb team performed the best drexelRS7mW. On the other side  , the document score was based on its reciprocal rank of the selected resource.Feature ty-klt is similar but using only answers in the same categories of the answer being assessed that is  , categories in set T   , defined in Section 3.2.1. the top 100 answers according to their rating  , obtained from a sample of Stack Overflow different from the one we use for evaluation in Section 4.1.Also  , 2072 Refseq records linked from our MEDLINE subset and that contain protein sequences were downloaded. oai_dc: contains only the accession id in the title field to satisfy the mandatory requirement of OAI 1.We believe that a benchmark like WPBench is useful to evaluate the performance of Web browsers for modern Web 2.0 applications. To the best of our knowledge  , there exists no previous benchmark which can automatically emulate the process of user Web surfing in a way fair to Web browsers.Hence  , we analyze the entire 24 database snapshots over ≈ 5 years to construct our dataset. However  , the database dumps provided by Stack Overflow do not directly contain information about deleted questions.In the experiment in disambiguating the 197 occurrences of 'bank' within LDOCE  , Wilks found a number of cases where none of the senses was clearly 'the right one' Wilks 891. In addition  , it is not always clear just what the 'correct sense' is.Since none of the previous papers investigated JavaScript  , we just chose four Stack Overflow tags for which there were a large number of associated questions. For JavaScript we applied the same procedure for analyzing the snippets and assessing true positives  , false positives  , and true negatives.Experimental results  , obtained using the LETOR benchmark  , indicate that methods that learn to rank at query-time outperform the state-ofthe-art methods. By generating rules on a demand-driven basis  , depending on the documents to be ranked  , only the necessary information is extracted from the training data  , resulting in fast and effective ranking methods.To account for potential measurement errors when matching social media data with streets  , we add a buffer of 22.5 meters around each street's polyline. To describe those segments  , we rely on data gathered and distributed for free by OpenStreetMap OSM a global group of volunteer cartographers who maintain free crowdsourced online maps and by Ordnance Survey the national mapping agency for Great Britain.In total  , the 1 ,574 sentences originated from 309 different answers. However   , for each API type  , we considered ten different questions on Stack Overflow  , and for each question  , we considered up to ten answers.We analyze the tag distribution of closed and deleted questions and compare them to the overall tag distribution on Stack Overflow. Stack Overflow questions contain user supplied tags which indicate the topic of the question.We choose the top 20 hotels in Amish Country  , Lancaster County  , PA from Hotels.com and TripAdvisor. Case study: Finding hotels in Amish Country.Fig- ure 4shows the distribution of time taken to receive the first 'delete vote' on a deleted question on Stack Overflow. Hence in this section  , we restrict our analysis to 62 ,949 deleted questions which have received 'deleted votes'.We collected concrete examples of research tasks  , and classified them into categories. Through interviews we conducted with scholars  , we learned that while the uncertain quality of OCRed text in archives is seen as a serious obstacle to wider adaption of digital methods in the humanities  , few scholars can quantify the impact of OCR errors on their own research tasks.We conclude that considering the meta data available on Stack Overflow along with natural language characteristics can improve existing approaches when applied to Stack Overflow data. In a comparative study with eight software developers to evaluate the meaningfulness and usefulness of the insight sentences  , we found that our supervised approach SISE resulted in the highest number of sentences which were considered to add useful information not found in the API documentation .Finally  , generated metadata information and OCRed text are integrated to support navigation and retrieval of content within scanned volumes. After the scanning and text recognition process  , the metadata generation system generates metadata describing the internal structure of the scanned volume and published articles contained within the volume.Table 1shows the results obtained by evaluating our resource selection approaches on the FedWeb 2013 collection. In Section 3  , we evaluate the performance with different K values.We compare our proposed NDCG-Annealing algorithm with those baselines provided in LETOR 3.0. As a result  , the NDCG-Annealing algorithm is more stable and pronounced compared to the baselines in LETOR 3.0 dataset.To describe those segments  , we rely on data gathered and distributed for free by OpenStreetMap OSM a global group of volunteer cartographers who maintain free crowdsourced online maps and by Ordnance Survey the national mapping agency for Great Britain. We consider the area of Central London  , which consists of 3 ,368 street segments.Upon selection of one sentence  , the sentence is expanded to show the surrounding paragraph from the original source  , along with a link to the corresponding Stack Overflow thread. In the top left corner of the API documentation  , a widget is added that shows up to five insight sentences for the API type.Future work lies in enhancing these results and understanding the motivations of programmers to contribute content to a Q&A website. The most common use of Stack Overflow is for how-to questions  , and its dominant programming languages are C#  , Java  , PHP and JavaScript.It remains to be seen if this could convince API owners to answer questions about their APIs in Stack Overflow knowing that their answers will be tied directly back to their own documentation. This means that as long as questions are being asked and answered about an API  , the documentation will be updated.Some examples are: How does the snippet quality influence results merging strategies ? Apart from studying resource selection and results merging in a web context  , there are also new research challenges that readily appear  , and for which the FedWeb 2013 collection could be used.The authors applied AutoComment to Java and Android projects  , and they were able to automatically generate 102 comments for 23 projects. Auto- Comment extracts code-descriptions mappings  , which are code segments together with their descriptions  , from Stack Overflow  , and leverages this information to automatically generate descriptive comments for similar code segments in open-source projects.Once again  , the browser extension detects from the mapping file that the user is visiting a page for which it has API usage examples. In the opposite direction  , Figure 6shows how the browser extension augments the official Android documentation with Stack Overflow examples.We have implemented most of our ranking algorithms implemented using Lucene. Our goal is set to design a system as simple as possible  , without using any external processing engine or resources  , other than the standard Indri toolkit and a third party LETOR toolkit.Since the document relevance inferred from the PCC and DBN models is better than that from the CCM model in the above two experiments  , we only consider the PCC and DBN models in this part of experiment. For each query and document  , we extract about three hundred of features in the experiment  , where the features are similar to those defined in LETOR16.Further  , the discrepancy among the selected relevant documents   , along with the discrepancy among the selected relevant and non-relevant documents for the different selection methods is illustrated in Figures 2. The LETOR-like selection achieves both high precision and recall at small percentages of data used for training up to 5% and then it drops to the levels of statAP and depth pooling.We also find that some topics of deleted questions are entirely irrelevant to the Stack Overflow website. Prior analysis in this section revealed that deleted questions fare extremely poor on multiple community value quality indicators.We have shown very competitive results relative to the LETOR-provided baseline models. We have described an experimental method in which learnt uncertainty information can be used to guide design choices to avoid overfitting  , and have run a series of experiments on the benchmark LETOR OHSUMED data set for both types of model.Its responsiveness performance is closer to users' perception than any of other benchmarks. Since the Web content  , user interactions  , and networking are exactly the same for these browsers  , WPBench produces benchmark results fair to different Web browsers.Readability features are probably not suitable for this scenario  , characterized by short text and many code snippets. Relevance features have no impact probably due to the fact that most of the answers in Stack Overflow are relevant to their questions.The first is the unique document found containing both of the words " income " and " forecast " as well as the American Tobacco Company logo and a dollar amount a recognized entity type greater than $500K. We apply conjunctive constraints on document image components to a straightforward document ranking based on total query-word frequency in the OCRed document text; in Fig- ure 2we show document images retrieved for two such queries.For each video fragment   , we also show the top-three relevant Stack Overflow posts  , and ask RQ3 to what extent they are relevant and complementary to the video tutorial fragments. Then  , it asks RQ2 whether the fragment is cohesive and self-contained.However  , before making this service available it was necessary to collect some data to construct its " seed " collection. To achieve its goal as the main source of information about the scientific production of the Brazilian CS community  , BDBComp strongly relies on its self-archiving service.Based on the observation  , title pages have relatively fewer number of text lines and larger average distance between text lines  , and they contain text lines indicating volume number and issue number in issue title pages. We use rule-based approach for title detection using page and line features calculated from OCRed text  , bounding box information  , and context analysis.In addition  , we see that the community puts in more effort to improve a 'closed' question than it does for a deleted question. We also find that some topics of deleted questions are entirely irrelevant to the Stack Overflow website.Normally  , the source code snippet in the post does not contain the underlined elements and is treated as a plain text block. Figure 5shows how our browser extension modifies a Stack Overflow post.In Quora  , the top 10 includes topics in various areas including technology  , food  , entertainment  , health  , etc. " We observed 56K topics in our dataset  , which is twice more than that of Stack Overflow  , even though Quora is smaller by 0   20   40   60   80   100   10 0 10 1 10 2 10 3 10 4 10 5 10Table 2lists the top 10 topics with most number of questions in each site.Multiple LETOR methods have been tried  , which are different in many ways and we expect them to be complimentary during the final fusion. There are also features taken from the query that are independent from documents  , including query length  , the average  , minimum  , maximum of the collection frequencies of the query terms.Thus  , these results indicate that training over a collection of given characteristics cannot always lead to an effective ranking function when the function is deployed to rank documents in a collection of radically different characteristics. The retrieval performance achieved was at least as good as the LETOR 4.0 baselines.Those features are then piped into different LETOR algorithms to produce several rank lists  , and eventually all the rank lists are merged using the conventional Reciprocal Rank based data fusion method. We introduce a two-pass retrieval framework  , where in the first pass we aim to retrieve as many relevant document as possible to ensure a reasonable level of recall  , and in the second pass we process all the retrieved documents in the first pass and extract features.Using various data sources of substantial size gives the opportunity to find intended POIs  , which may fall into multiple concepts ranging from rather generic to more detailed ones such as " restaurant " vs. " pizzeria. " First  , for a meaningful search result  , we need to consider data obtained by integrating multiple data sources  , which may be provided by autonomous vendors in heterogeneous formats e.g.  , OpenStreetMap or Open Government Data data  , a restaurant guide  , etc.Therefore  , we have adopted Reciprocal Rank as the data fusion techniques in our final submissions. Our LETOR algorithms behave differently on some topics  , but Condorcet method tends to ignore high votes from the minority  , but instead prefer weak votes from the majority.There is ample research into how to reduce the error rates of OCRed text in a post-processing phase. While this makes it easier for scholars to use the archive  , it also denies them the possibility to investigate potential tool-induced bias.We also make our experimental dataset publicly available for research purposes under the Creative Commons Attribute-ShareAlike. However  , it must be noted that our dataset contains the maximum possible deleted questions which can be obtained given the publicly available Stack Overflow database snapshots.Each review provides a general rating of the hotel  , plus provides seven individual ratings on the following service characteristics: Value  , Room  , Location  , Cleanliness  , Service  , Check-in  , and Business Service. Hotel service characteristics: We extracted the service characteristics from the reviews from TripAdvisor.In this section  , in addition to our experimental dataset we utilize this data to analyze question quality indicators for deleted and 'closed' questions. We find that there are 254 ,446 0.25M 'closed' questions on Stack Overflow between August 2008 June 2013.In general  , deleted questions are extremely poor in worth to the Stack Overflow community. We also find that authors delete their own questions to salvage reputation points on the website.There are over 100 different badges on Stack Overflow  , which vary greatly in how difficult they are to achieve. We analyze the question-answering Q&A site Stack Overflow  , which makes extensive use of badges and was one of the first sites to use them on a large scale.Since " quality " is a subjective feature  , it is inferred from the opinion of the asker and from the votes received from the other users. According to the Stack Overflow guide 2   , a good answer  , besides being correct   , should be clear  , provide examples  , quote relevant material  , be updated  , and link to more information and further reading.Among the dissimilarities  , the following are noteworthy: a Information services/goods and network services have many more parameters other than just price and quantity  , which describe the products and services. Nasdaq.However  , any publishsubscribe system implementing the optimal centralized algorithm in XPath query processing 18 would require a single depth-first traversal of the document tree visiting  , in our example  , twice the nasdaq server. Publish-subscribe systems are more in-line with moving the processing to the data.To fill this gap  , we compare techniques for automatically extracting sentences from Stack Overflow that are related to a particular API type and that provide insight not contained in the API documentation. While a lot of research has focused on finding code examples for APIs e.g.  , 17  , 33  , less work has been conducted on improving or augmenting the natural language descriptions contained in API documentation.In contrast  , the sentence added by the Stack Overflow user clearly distinguishes the roles of the API types discussed and explains which one should be used in which situation. This is an example of the API documentation lacking information on purpose 18 since it only states which alternative is preferred without explaining why.The moderators are driven by the Stack Overflow motto to keep a low signal-to-noise ratio in order to maintain high content quality. This shows that once a question receives the required number of 'delete vote's  , moderators move quickly to take appropriate action.We see that most of these questions are very poor in quality and of little worth to the community. Table 1 shows examples of deleted questions on Stack Overflow.To answer these questions we use data from Stack Overflow  , a CQA platform for programming-related topics. RQ3 What is the most important individual feature within and across feature sets ?Each data set is partitioned on queries to perform 5 fold cross-validation. The results of RankSVM  , RankBoost  , AdaRank and FRank are reported in the Letor data set.The relevancy judgments provided in OHSUMED are scored 0  , 1 or 2 and there are 45 features for each querydocument pair. The datasets provided in the LETOR There are 106 queries in the OSHUMED dataset.For meta search aggregation problem we use the LETOR 14  benchmark datasets. Given an aggregate ranking π  , and relevance levels L  , NDCG is defined as:in that we focus on single sentences from Stack Overflow that are relevant to an API type instead of a code snippet. Our work differs from that by Wong et al.Feature examples include TF  , IDF  , LMIR and BM25 considering  , result title  , abstract  , body  , url and pagerank values. The training features are the ones used in LETOR benchmark 2 and are described in 2.We choose hotels in Amish Country because during our initial investigation many potentially suspicious hotels were present. We choose the top 20 hotels in Amish Country  , Lancaster County  , PA from Hotels.com and TripAdvisor.22K LabelMe contains 22 ,019 images sampled from the large LabelMe data set. Most images in LabelMe contain multiple objects.they display graph properties similar to measurements of other popular social networks such as Orkut 25. First  , our prior analysis 35  showed that they are representative of measured social graphs  , i.e.This logical structure information can be used to help the metadata extraction process. In each DjVu XML file  , the OCRed text is organized in a page  , paragraph  , line  , and word hierarchy.In question-answering sites  , e.g.  , Quora and Stack Overflow  , an important task is to route a newly posted question to the 'right' user with appropriate expertise and several methods based on link analysis have been proposed 45  , 6  , 46. The factorization technique can be naturally extended by adding biases  , temporal dynamics and varying confidence levels.From randomly sampled smells  , 434 error computation smells previously created can help end users the quality of their We summarize main contributions of this paper  Second  , we with real-life spreadsheets the Institute of Software  , Chinese Academy of Sciences evaluation report in the EUSES corpus suffer which cover 21.6 putation smells reveal weakness and sheets.It consists of almost 20 million nodes vectors and 2 billion links non-zero weights  , yielding roughly . The Orkut graph is undirected since friendship is treated as a symmetric relationship.Our research is guided by two main questions: RQ1: To what extent are unsupervised and supervised approaches able to identify meaningful insight sentences ? Applied to API documentation and content from Stack Overflow  , the idea is to create a summary of the discussions on Stack Overflow as they relate to a given API type  , assuming that the reader is already familiar with the type's API documentation.In this work  , we focus our attention on deleted questions poor quality on Stack Overflow. Therefore  , it is important to study poor quality questions and develop mechanisms to minimize them on the website.Given the IDE context   , Prompter automatically retrieves pertinent discussions from Stack Overflow  , evaluates their relevance and notifies developers about the available help if a given threshold is surpassed. 26 .This can be explained by the patterns' reliance on the systematic writing style of reference documentation which is not used in informal documentation formats such as Stack Overflow. Similar to the attempts of using text summarization techniques for the extraction of insight sentences  , the application of knowledge patterns to our development set did not produce encouraging results— we obtained a precision of 0.15 and a coverage of 0.8.These data could be used by the participants to build resource descriptions . The FedWeb 2014 collection contains search result pages for many other queries  , as well as the HTML of the corresponding web pages.For Stack Overflow we separately index each question and answer for each discussion. The last step in the data pre-processing of CodeTube consists in indexing both the extracted video fragments and the Stack Overflow discussions  , using Lucene 9   , where each video fragment is considered as a document.Thus  , line features are designed to estimate properties of OCRed text within a line  , which can be calculated based on OCRed text and bounding box information in the DjVu XML file. Additionally  , text within the same line usually has the same style.The second evaluation  , based on the WSDM 2014 Web search personalization challenge  , 1 uses dwell time as ground-truth labels and real clicks as feedback to BARACO. The first evaluation  , based on the LETOR datasets 17  , uses manual relevance assessments as ground-truth labels and synthetic clicks as feedback to BARACO.From the table below we conclude further that SCOVO seems to be the best combination of flexibility and usability  , allowing to recreate the data-table structures with a reasonable degree of fidelity in another environment that is  , on the Web. Both other approaches are not capable of representing historical data and only provide statistics for one point-in-time.This finding suggests that there is an advantage to interpreting sentences on Stack Overflow in the context of other documentation sources. Another interesting finding is that the two features that represent the similarity of a potential insight sentence to the corresponding API documentation were the features with the highest information gain.We make the following research contributions  We analyze deleted questions on Stack Overflow posted over ≈5 years and conduct a characterization study. We perform the first large scale study on poor quality or deleted questions on Stack Overflow.Our LETOR algorithms behave differently on some topics  , but Condorcet method tends to ignore high votes from the minority  , but instead prefer weak votes from the majority. This observation is similar to that in 22  , and it is likely to be the case that false positives that are common in all 4 posting lists will likely to receive higher ranking than true positives that are supported by a subset of posting lists.Our experimental results also show that: 1 there is some sensitivity of the method to the choice of the user-defined parameter  , φmax  , although there are some ranges of values in which the results are very stable and 2 the combination of the first step of our method with other supervised ones does not produce good results as we obtained with SAND. On the BDBComp collection  , SAND outperformed two unsupervised methods in more than 36% under the pF1 metric and in more 4% under the K metric.We plot the log of negative log-likelihood due to scale of the values  , and so lower value implies that model has higher likelihood. Figures 4b shows the performance of our model in comparison with the best baseline B3 over the NASDAQ.In addition  , we included two features for the similarity of a sentence to the corresponding API documentation  , following the idea of update summarization 4 . We designed the feature set to cover all meta data available on Stack Overflow as well as basic syntactic and grammatical features.We believe that we are the first to investigate augmenting natural language software documentation from one source with that from another source. We conclude that considering the meta data available on Stack Overflow along with natural language characteristics can improve existing approaches when applied to Stack Overflow data.Further  , an analysis of knowledge exchange sites e.g.  , Stack Overflow  conducted by Parnin et al. This is due in part to missing documentation or the suspicion that documentation may be out of date 26.We manually validated the 1 ,423 detected conformance errors in the 700 sampled cell arrays. In Table 3   , AmCheck detected a total of 8 ,481 conformance errors CE1 in the EUSES corpus.In addition   , we also introduce our failed attempt of using topic models to perform query expansion to capture documents of multiple topics in section 5  , and a brief reflection on why this approach did not work. In the rest of this paper  , we will introduce the general pipeline of our retrieval system in Section 2  , followed by an introduction to novel Language Modelling approaches in Section 3 that were used to generate features  , and later in Section 4 we introduce all other features and the LETOR algorithms.Each thread in our corpus contains at least two posts and on average each thread consists of 4.46 posts. From the TripAdvisor data  , we randomly sampled 650 threads.For datasets  , we used MQ2007 and MQ2008  , a collection of benchmarks released in 2009 by Microsoft Research Asia research.microsoft.com/en-us/um/beijing/projects/letor/. The depth of the complete solution is d = 8.The first part is conducted on an Orkut community data set to evaluate the recommendation quality of LDA and ARM using top-k recommendations metric. We divide our experiments into two parts.In particular  , the culprit was single-digit OCR errors in the scanned article year. This turned out to be an artifact of OCRed metadata.We expect the pairwise methods to perform better than point-wise approaches  , as the features collected from the error pairs are more meaningful as they define relative distances. We have tried using Support Vector Regression RankSVM with linear kernel for pairwise LETOR  , and were trained on a set of error pairs collected using the " web2013 " relevance judgments file.This result is statistically significant based upon a paired t-test across 10 random training/testing partitions of the dataset p-value: ≤ 1.7 × 10 −5 . We find a 33% performance gain over MQ for LSH-based projections for 22k Labelme.We chose this collection because it is freely available for download 10 and is the largest forum hosted by Stack Exchange. Our dataset consists of a sample of Stack Overflow  , a Q&A Forum for programmers.Users can up-vote answers they like  , and down-vote answers they dislike. Stack Overflow is centered around nine design decisions 7 : Voting is used as a mechanism to distinguish good answers from bad ones.The paper is structured as follows: We motivate the need for a simple RDB-to-RDF mapping solution in Section 2 by comparing indicators for the growth of the Semantic Web with those for the Web. Additionally  , we employed Triplify to publish the 160GB of geo data collected by the OpenStreetMap project.We found that  , unlike what was previously observed for collaborative encyclopedias  , review and user features are the most important in the Q&A domain. By using a dataset from the Stack- Overflow Q&A Forum  , we evaluated the sets of features and compared our method to 3 other ones previously published in literature.The first author is also supported under a National Defense Science and Engineering Graduate Fellowship. This work was funded in part by the National Science Foundation  , under NSF grant IIS-0329090  , and as part of the EUSES consortium End Users Shaping Effective Software under NSF grant ITR CCR-0324770.We note that the MoviePilot data does not contain the group information for all the users in the training data. Once again  , it is clear that the group recommendation model based on the IMM outperforms the other two methods.One of our participants  , an 18-year-old student  , was not only technologically-savvy in terms of adopting new websites and exploring advanced features of computer applications  , she was also a strong " gifter. " One reason for the ubiquity of Orkut is most likely due to the power of influencers and the practice of account gifting.From the remaining 306 topics  , we selected 75 topics as follows. We started from the 506 topics gathered for FedWeb 2013 5  , leaving out the 200 topics provided to the FedWeb 2013 participants.This work was funded in part by the National Science Foundation  , under NSF grant IIS-0329090  , and as part of the EUSES consortium End Users Shaping Effective Software under NSF grant ITR CCR-0324770. We would like to thank Scott Hudson  , James Fogarty  , Elsabeth Golden  , Santosh Mathan  , and Karen Tang for helping with the experiment design and execution  , and we also thank the study participants for their efforts.However  , the database dumps provided by Stack Overflow do not directly contain information about deleted questions. The statistics show that Stack Overflow is a very popular programming CQA with 5.1M questions   , 9.4M answers and 2.05M registered users.Whenever the need arises to more explicitly declare what kind of range is intended  , this technique can be used e.g. The earlier can be used to capture more information pertaining to the creation of a particular statistical item; – Defining sub-properties of using SCOVO-min and max.In this part  , we analyze edit histories of deleted questions on Stack Overflow. These suggestions are then reviewed by privileged users and are brought into effect at their discretion .OpenStreetMap. Our data is aggregated every 60 minutes  , comes from both TIM customers and roaming customers in the six cities  , and covers the time ranging from February to October 2014.The BDBComp architecture comprises three major layers Figure  1. In addition to the self-archiving service  , we envisage two other ways to collect metadata for the repository: 1 by extracting them from existing Web sites  , for instance  , by using tools such as the Web- DL environment 1We analyzed the titles and body texts of these questions and found the following categories  , ordered by their frequency: how-to. To further understand the characteristics of questions on Stack Overflow  , we coded a random sample of 385 questions from our data set 1%.It is possible to express SCOVO in OWL-DL  , if advanced reasoning is of necessity. 2  is currently defined in RDF- Schema.The ranking criteria used by their approach consists of the textual similarity of the question-and-answer pairs to the query and the quality of these pairs. They developed an improved search engine for content on Stack Overflow which recommends question-and-answer pairs as opposed to entire Q&A threads based on a query.We find that there are 254 ,446 0.25M 'closed' questions on Stack Overflow between August 2008 June 2013. A question can be marked as 'closed' due to five reasons – duplicate  , subjective  , off topic  , too localized or not a real question 11.For comparative purposes  , considering that the Microsoft and LETOR datasets were designed for a folded cross-validation procedure  , we applied this same strategy to the YA- HOO! Similarly to the WEB10K benchmark  , these datasets are partitioned into 5 folds to be used in a folded cross-validation procedure.We use this feature to examine the implications of different question characteristics on the success of a question. On Stack Overflow  , the user who is asking a question can mark at most one answer per question as accepted.This is because the LETOR data set offers results of linear RankSVM. For all the SVM models in the experiment  , we employ the linear SVM.To our knowledge  , we are the first paper to explicitly parallelize CART 5  tree construction for the purpose of gradient boosting. On the Microsoft LETOR data set  , we see a small decrease in accuracy  , but the speedups are even more impressive.Ranking functions exhibit their second worst performance when trained over data sets constructed according to the LETOR-like document selection methodology. the performance of RankBoost  , Regression and RankNet with the hidden layer.Indeed  , the exact  , quantifiable  , benefits of documentation are in need of further investigation. showed that 87% of the classes in the Android APIs are covered by questions and answers coming on the Stack Overflow web site 20.The motivation in this case was to find queries of a similar type e.g.  , navigational or information queries  , but no improvements were observed with smaller training sets such as Letor. There have been some attempts to do this 1  , 4.In order to avoid clutter  , only a representative subset of baselines is shown  , including the best and the worst of them. Figure 3b compares FITC-Rank with the LETOR base- lines 9 .Since we decided to focus on Milano and London  , however  , we can discard this potential issue: our direct knowledge of the city of Milano let us affirm that the spatial objects mapping is quite good and homogeneous throughout the city; OpenStreetMap coverage in the London area was evaluated in 18 and shown to be quite accurate in comparison to official sources. Still  , the mapping can be inhomogeneous some zones can be more detailed annotated than others.The index matching service that finds all web pages containing certain keywords is heavy-tailed. The applications used for the evaluation are two services from Ask.com 2 with different size distribution characteristics: a database index matching service and a page ranking service.When we applied their patterns to content on Stack Overflow  , we were not able to repeat their positive results in terms of precision and usefulness see Section 4.3. Chhetri and Robillard 7 categorized text fragments in API documentation based on whether they contain information that is indispensable  , valuable  , or neither  , using word patterns.They found the cosine similarity measure to show the best empirical results against other measures. 5 present an empirical comparison of six measures of similarity for recommending communities to members of the Orkut social network.A 'closed' question is low quality but has the potential to be improved upon. 'Closed' questions are questions which are deemed unfit for the Stack Overflow format.In addition  , users without sufficient privileges can suggest edits to questions. A question on Stack Overflow has three major sections which can be edited – title  , body and tags.The goal of our work is to automatically extract such sentences and use them to augment API documentation. Table 1shows  , for three Java API types  , a sentence taken from Stack Overflow that contains useful information about this type that is not stated in the type's API documentation.LQ12 designed a spider framework to crawl websites from tripadvisor  , in order to collect candidate pages related to attractions  , restaurants etc. The similarities are computed based on the either the category or description of the suggestions.Further  , our ongoing work focuses on broadening the deployment base available 17   , making converters from and to SCOVO available  , and extending the framework itself. For example  , one part of the UN data set—the Commodity Trade Statistics Database COMTRADE—alone provides commodity trade data for all available countries and areas since 1962  , containing almost 1.1 billion records.The OpenStreetMap project has successfully applied the Wiki approach to geo data. In addition to using Triplify for publishing RDF from the long tail of million of Web applications deployed  , we evaluated the software with the very large datasets produced by the OpenStreetMap project 14 .Table 7 shows some examples of undeleted questions on Stack Overflow. The similar voting procedure to that of deletion is followed to undelete a question.On the one hand  , the perceived relevance is relatively low  , with only 38% of the Stack Overflow discussions achieving a median relevance of 3. The distribution first quartile is 2  , the median 2 and the third quartile 3.The negative correlation in the informational setting of NP2003 dataset is due to a heavily skewed distribution of candidate rankers when the production ranker is at a Figure 4: The ROC curves for BARACO and MT  , using DBN for generation and interpretation  , TD2004 dataset  , informational user model LETOR evaluation. These results show that BARACO again outperforms the EM-based method.Given the datasets above  , we now describe how we tested and measured the efficacy of the recommendation algorithms described in Sections 2 and 3. We also used the MoviePilot data  , by disregarding the group memberships.P -perfect user model setting  , I -informational  , N -navigational LETOR eval- uation. The error bars are standard errors of the means.We first collected the top destinations recommended by TripAdvisor 8 for four travel intentions including Beaches & Sun  , Casinos  , History & Culture  , and Skiing. Comparing the two graphs in Figure  6a andFurthermore  , according to global OpenStreetMap statistics 1   , Italy and UK are ranked 7th and 10th for number of created spatial objects  , and 4th and 5th for density of created spatial objects per square kilometer. Since we decided to focus on Milano and London  , however  , we can discard this potential issue: our direct knowledge of the city of Milano let us affirm that the spatial objects mapping is quite good and homogeneous throughout the city; OpenStreetMap coverage in the London area was evaluated in 18 and shown to be quite accurate in comparison to official sources.Among the various informal documentation sources  , Stack Overflow has been used by many recommender systems 7  , 30–32  , 36  , 41. Numerous approaches have been proposed to provide developers with official or informal documentation for their task at hand  , as well as code samples they can reuse.Table 1shows  , for three Java API types  , a sentence taken from Stack Overflow that contains useful information about this type that is not stated in the type's API documentation. The main contributions of this work are the conceptualization of insight sentences as sentences from one documentation source that provide insight to other documentation sources and a comparative study of four different approaches for the extraction of such insight sentences  , as well as a list of factors that can be used to distinguish insight sentences from sentences that are not meaningful or do not add useful information to another documentation source.After filtering out locations not appearing in our corpus  , we built a location set consisting of 36 locations  , based on which pair-wise location similarities were computed as describe in Section 2.4.1 to form a location similarity graph. We first collected the top destinations recommended by TripAdvisor 8 for four travel intentions including Beaches & Sun  , Casinos  , History & Culture  , and Skiing.Spreadsheets collected in our case study are those used in practice and maintained by professional finance officers. We chose the EUSES corpus because it is by far the largest corpus that has been widely used for evaluation by previous spreadsheet research studies.Table 4: Retrieval examples by tags queries on the LabelMe database by the proposed method. It can be concluded that SCSM can achieve a comprehensively better performance among unsupervised methods.We employ five different document selection methodologies that are well studied in the context of evaluation  , along with the method used in LETOR for comparison purposes. If yes  , which one of these methods is better for this purpose ? "One of them reported that " the concept is amazing  , and has a lot of possibility of improvement  , given the huge amount of different sources of data available "   , while other participants asked for additional features to improve this functionality. The possibility of having complementary sources of information   , e.g.  , Stack Overflow has been appreciated by some participants.To show that these results also hold for code programmers struggle to write  , we repeated the same experiment on code snippets gathered from questions asked on the popular Stack Overflow website. This shows that the vast majority 99% in our study of statements in real Java code have depth at most 4  , which our results above show that CodeHint can easily search.As a result  , the NDCG-Annealing algorithm is more stable and pronounced compared to the baselines in LETOR 3.0 dataset. We also see from Figure 4 that our NDCG-Annealing algorithm outperforms all the other baseline algorithms on this dataset.'Closed' questions are questions which are deemed unfit for the Stack Overflow format. Pyramid.The ability of our models to take into account different levels of uncertainty for different data produces effective models  , and  , for FITC-Rank in particular  , performance is consistent across experiments and gives significant improvements against the baseline models. We have shown very competitive results relative to the LETOR-provided baseline models.Therefore  , questions on Stack Overflow which are extremely off topic or very poor in quality are deleted from the website 1. Hence  , it would like to keep the noise on their website as low as possible .For 40 randomly selected types of the Java SE 7 API 20 types with one-word identifiers  , such as List  , and 20 types with multi-word identifiers  , such as ArrayList  , we selected the first five threads that the Stack Overflow API returned for each non-qualified type  , and we manually annotated whether the thread actually mentioned the API type. We manually created a benchmark to measure the relevance of the threads that the filter identifies in step 2.For each query and document  , we extract about three hundred of features in the experiment  , where the features are similar to those defined in LETOR16. We follow the RankNet 5 method which is a pairwise ranking algorithm receiving the pairwise preferences to optimize the ranking function.We also applied the algorithm to rank ads with non-linear ranking function described in section 7 in contextual advertising in both online and offline scenarios. The results on seven datasets in LETOR 3.0 show that the NDCG-Annealing algorithm can outperform the baselines and it is more stable.All figures are generated by our modified version of Java OpenStreetMap Editor 2 which is a map editor for OpenStreetMap 3 written in Java. The detail of our data preparation can be found in Section 6.To ensure critical mass  , several programmers were explicitly asked to contribute in the early stages of Stack Overflow. Web pages on stackoverflow .com are optimized towards search engines and performance .These data could be used by the participants to build resource descriptions. The FedWeb 2013 collection contains search result pages for many other queries  , as well as the HTML of the corresponding web pages.Using normalized hyper-parameters described in Section 2.6  , the best hyper-parameters are selected by using the validation set of CIFAR-10. For both CIFAR-10 and NUS-WIDE datasets  , we randomly sample 1 ,000 points as query set  , 1 ,000 points as validation set  , and all the remaining points as training set.In this case  , the Baker extension detects that the user has navigated to a Stack Overflow post. Normally  , the source code snippet in the post does not contain the underlined elements and is treated as a plain text block.These 149 engines were a subset of the 157 search engines in the FedWeb 2013 test collection. This test collection consists of sampled search results from 149 web search engines crawled between April and May 2014.Our empirical study reports that there are altogether 16 ,385 cell arrays among 993 out of 4 ,037 spreadsheets in the EUSES corpus 11. In this paper  , we focus only on those cell arrays subject to computational semantics expressed in formula patterns without using " if " conditions.Stack Overflow is centered around nine design decisions 7 : Voting is used as a mechanism to distinguish good answers from bad ones. To ensure critical mass  , several programmers were explicitly asked to contribute in the early stages of Stack Overflow.We are surprised to find that the curves from Stack Overflow and Quora are nearly identical. Next  , we plot the distribution of views and answers per question in Figure 5and Figure 6.Figure 5and Figure 6show the results on the Letor TD2003 and TD2004 datasets. We tried treating 'partially relevant' as 'irrelevant'  , it did not work well for SVM map .We randomly sample a subset of CIFAR-10 with 5000 points for evaluation. To get a deeper comparison  , we perform another experiment on smaller datasets where the full supervised information can be used for training.While a lot of research has focused on finding code examples for APIs e.g.  , 17  , 33  , less work has been conducted on improving or augmenting the natural language descriptions contained in API documentation. On the other hand  , " how-to " questions 35 also referred to as " how-to-do-it " questions 10 are the most frequent question type on the popular Question and Answer Q&A site Stack Overflow  , and the answers to these questions have the potential to complement API documentation in terms of concepts  , purpose  , usage scenarios  , and code examples.Figure 10shows the venn diagram of tag distributions of questions on Stack Overflow. There are a total of 36 ,643 tags on all questions in Stack Overflow.We conduct the first large scale study of deleted questions on Stack Overflow. We employ four categories of feature sets – user profile   , community based  , content based and stylistic features – and report most discriminatory features.These codes were a fascinating repository of raw linguistic " ore " from which the possibility of additional " finds " could be made. Additionally   , the MPD and w7 were the result of an extensive organization effort by a whole series of computational lexicologists who had refined its format to a very easily computed structural description Reichert  , Oiney & Paris 69  , Sherman 74  , Amsler and White 79  , Peterson 82  , Peterson 871 The LDOCE while very new  , offered something relatively rare in dictionaries  , a series of syntactic and semantic codes for the meanings of its words.To compute these metrics we used the standard evaluation tool available for the LETOR 3.0 benchmark for binary datasets  , as well the tool available for the Microsoft dataset for all multi-label relevance judgment datasets 5 . The statistical tests are computed over the values for Mean Average Precision MAP and the Normalized Discounted Cumulative Gain at the top 10 retrieved documents hereafter   , NDCG@10  , the two most important and frequently used performance metrics to evaluate a given permutation of a ranked list using binary and multi-relevance order 31.To conduct our scalability experiments  , we used the same Orkut data set as was used in Section 5.1. Our parallel LDA code was implemented in C++.Most images in LabelMe contain multiple objects. With the help of this annotation tool  , the current LabelMe data set contains as large as 200 ,790 images which span a wide variety of object categories.Also  , 'closed' questions questions which are deemed unfit for Stack Overflow which do not serve as useful sign points may also be deleted. have no activity over a significant period of time are also deleted.However  , we have found little evidence  , at least for the LETOR OHSUMED data set  , that explicit use of the uncertainty information can improve model performance in terms of NDCG. The ability of our models to take into account different levels of uncertainty for different data produces effective models  , and  , for FITC-Rank in particular  , performance is consistent across experiments and gives significant improvements against the baseline models.We also make observations about the relative quality of deleted questions in context to 'closed' questions on Stack Overflow. In this section  , in addition to our experimental dataset we utilize this data to analyze question quality indicators for deleted and 'closed' questions.Usage instructions and further information can be also found at http://LinkedGeoData.org. The dynamic of the OpenStreetMap project will ensure a steady growth of the dataset.We see that the Stack Overflow and Programmers communities stand out  , one with low and the other one with high values for all three topic properties. Table 1 describes the five behaviour categories with regards of the topic properties.We profile terms by ranking them using tf.idf scoring and select the top-100 terms for a topic's profile. We start with generating a profile per topic—here  , a topic is a tag associated with a question on Stack Overflow—by retrieving all questions that are associated with the topic along with all comments  , answers  , and comments to answers associated with the question.In the next sections  , we describe our investigation of the means to automatically identify sentences on Stack Overflow that are meaningful and add useful information not contained in the API documentation. In the third example   , a user again compares two alternatives and explains which one to use in a particular situation  , this time in an answer to " Thread sleep and thread join " .The tags were mainly used to learn about the topics covered by Stack Overflow  , while the question coding gave insight into the nature of the questions. To analyze the different kinds of questions asked on Stack Overflow  , we did qualitative coding of questions and tags.Then  , for every query the system should return a ranking such that the most appropriate search engines are ranked highest without using the actual results for the given query which were  , in fact  , provided after the submission deadline of this task. The input for this task is a collection provided by the organisers FedWeb 2013 collection consisting of sampled search results from 157 search engines.To focus our evaluation on string data  , we only extracted columns that contained at least 20 string cells i.e. Each spreadsheet column in the EUSES corpus typically contains values from one category  , so columns were our unit of analysis for identifying data categories.In contrast  , we connect different forms of documentation through heuristics that match Stack Overflow threads to API types  , and instead of FAQs  , SISE produces insight sentences. They propose to connect API documentation and informal documentation through the capture of developers' Web browsing behaviour.The CIFAR-10 data set contains 60 ,000 tiny images that have been manually grouped into 10 concepts e.g.  , airplane  , bird  , cat  , deer. Given a query image  , the images sharing at least one common concept with the query image are regarded as the relevant ones.The possibility of having complementary sources of information   , e.g.  , Stack Overflow has been appreciated by some participants. The extraction of fragments from video tutorials has been appreciated and considered " very useful for developers who are already knowledgeable about the topic  , they can save a lot of time " .For each tags query second column  , the top several retrieved images are shown in the fourth column. Table 4: Retrieval examples by tags queries on the LabelMe database by the proposed method.To avoid the aforementioned implication  , these extra documents with low BM25 scores were dropped in the latest LETOR release 13. This is a highly counterintuitive outcome.As a result  , the extra characters in the parameter overwrite the stack  , often causing return addresses to be changed so that the program returns to the attacker's code rather than to where it was called. For instance  , buffer overflow attacks are the result of the user intentionally passing parameters that are larger than expected.For example   , we find on Stack Overflow that users' votes on questions are significantly more positive before they receive the Electorate badge than after it. Incentivizing users to increase their activity naturally brings up the question of how this affects the quality of their actions.The nature of GP algorithms is also prone to overfitting. According to the authors  , GP based LETOR was able to achieve competitive performance with RankSVM and RankBoost  , but its computational cost is higher.Figure 1: Stack Overflow Example meaningful on their own without their surrounding code snippets or the question that prompted a given answer. In the next sections  , we describe our investigation of the means to automatically identify sentences on Stack Overflow that are meaningful and add useful information not contained in the API documentation.We started from the 506 topics gathered for FedWeb 2013 5  , leaving out the 200 topics provided to the FedWeb 2013 participants. </narrative> </topic>We propose CodeTube  , a novel approach that effectively leverages the information found in video tutorials and other online resources  , providing it to developers for a task at hand. While approaches have been proposed to support developers by mining API documentation 36  , 37 and Q&A websites such as Stack Overflow 14 ,32   , or by automatically synthesizing code examples from existing code bases 1 ,6 ,16 ,25  , there is currently no approach aimed at leveraging relevant information found within fragments of video tutorials and linking these fragments to other relevant sources of information.Thus  , we choose a 60 day period from 01/01/2009 to 03/01/2009 for our experiments. In TripAdvisor   , t win is about 60 days.The dictionary we are using in our research  , the Longman Dictionary of Contemporary English LDOCE Proctor 781  , has the following information associated with its senses: part of speech  , subcategorizationl   , morphology  , semantic restrictions   , and subject classification. What's important for our purposes is that the senses have information associated with them that will help us to distinguish them.Low-level features include term frequency tf  , inverse document frequency idf  , document length dl  , and their combinations. Features in Letor OHSUMED dataset consists of 'low-level' features and 'high-level' features.In an evaluation  , the authors found that the inclusion of different types of contextual information associated with an exception can enhance the accuracy of recommendations. The Stack Overflow API is one of the search APIs used in their work  , and their approach captures the context in a similar fashion to the work by Cordeiro et al.Each image of size 32 × 32 is represented by a 512-dimensional GIST feature vector. The CIFAR-10 dataset 11 consists of 60 ,000 color images drawn from the 80M tiny image collection 29.Additionally  , there was a strong correlation between usage of API elements and the amount of discussions on Stack Overflow. They found that over 35000 developers contributed in crowd-documenting Android   , covering 87% of the classes.Table 1 shows more detailed information about the collections and its ambiguous groups. This collection was created by us and contains the 10 largest ambiguous groups found in BDBComp.However  , some of the topics on deleted questions are extremely off topic to the interests of the community. The topics of 'closed' questions are relevant to the community despite the questions themselves being unfit for the Stack Overflow format.These long requests are often kept running because the number of such requests is small  , and derived results can be cached for future use. For example in Ask.com search site  , some uncached requests may take over one second but such a query will be answered quickly next time from a result cache.Very few text analysis tools can  , for example  , deal with different confidence values in their input  , apart from the extensive standardization these would require for the input/output formats and interpretation of these values. For task T4 not in the table  , the use of OCRed texts in other tools  , our findings are also mainly negative.This latter question is a matter of some interest  , as the MAP scores for LETOR 3 approach the 65% considered achievable with human-adjudicated relevance 5. So the meta-learner constitutes the best known method  , and the result raises the lower bound of what is known to be learnable from the dataset.Stack Overflow provides a periodic database dump of all user-generated content under the Creative Commons Attribute- ShareAlike 8 . 3We use our work on constructing the concept ontology for LabelMe 1 as an example to depict our algorithm: 1 Labels in LabelMe contain text information of dominant salient objects as well as their contours and locations  , but there are no explicit labels at the image concept levels 8. In this paper  , we have developed a semi-automatic scheme for concept ontology construction.We notice the presence of programming related tags like objective-c  , android and c# which points out these undeleted questions are relevant to Stack Overflow. Fig- ure 16shows the word cloud of the top-50 tags that occur in undeleted questions on Stack Overflow.2 Stack Overflow has detailed  , explicit guidelines on posting questions and it maintains a firm emphasis on following a question-answer format. Stack Overflow is a programming based CQA and the most popular Stack Exchange website consisting of 5.1M questions  , 9.4M answers and 2.05 registered users on its website.regression trees on large data sets  , since the required tree depth grows with increasing data set size. We observe up to 25 fold speedups in this distributed setting for the Microsoft LETOR data set.In comparison with their original publication   , the FedWeb submission assumed that all resources are of the same size. As these were not available  , document samples were used instead.For the arithmetic component  , other codes include overflow and zero divide. The other condition codes returned by the stack operations include stuck overflow for Push and siaclc emp-ty for Pop and Top.From the TripAdvisor data  , we randomly sampled 650 threads. From the source data  , we generated two datasets for question identification.In order to publish the OpenStreetMap data  , we performed some preprocessing of the data structures. Overall  , the project had produced a 160GB database of geo data until July 2008  , in some regions surpassing commercial geo data providers in terms of precision and detail.Any opinions  , findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the National Science Foundation. This work was funded in part by the National Science Foundation  , under NSF grant IIS-0329090  , and as part of the EUSES consortium End Users Shaping Effective Software under NSF grant ITR CCR-0324770.Please note that the authors of ANN_SIFT1M provide only the extracted features without any original images of their data. Figure 3 shows some representative images sampled from LabelMe and TinyImage data sets.The community takes significant time to detect a potential deleted question but moderators take swift appropriate action. We observe an increasing trend in the number of deleted questions on Stack Overflow over the last 2 years.Furthermore  , the extended ontology includes the mappings resulted by the schema matching. The spatial data is collected by the OpenStreetMap 5 project and it is available in RDF format.Conversely  , when annotating official API documentation with usage examples we would only want to include exact matches as this context would not be present. For example   , when annotating a Stack Overflow example with FQN information  , presenting a small number of possible type options for the developer to choose from would be reasonable as they could use their own intuition gained from the snippet text to make an informed selection.Orkut: This graph represents the Orkut social network. We consider better  , in terms of quality  , those algorithms that have better matching with the gold standard  , independently of the type of algorithm under consideration.We evaluate the three strategies of generating resource representations as discussed in Section 2.2  , with varying numbers of topics K in training the LDA topic model. Table 1shows the results obtained by evaluating our resource selection approaches on the FedWeb 2013 collection.The LabelMe data set contains high-resolution photos  , in fact most of which are street view photos. From Figure 3   , it is easy to see that LabelMe and TinyImage have different characteristics.8 ,536 of these total questions were originally deleted by the question author while 814 questions were deleted by a moderator. We find a total of 9 ,350 undeleted questions on Stack Overflow.A question score is the net worth of the usefulness of a question as determined by the Stack Overflow community. We see that deleted questions have higher percentage of questions with zero score than 'closed' questions.Since Quora does not show when a question is posted  , we estimate the posting time by the timestamp of its earliest answer. Assuming we are correct about the use of qid  , we can plot an estimate of the growth of Quora and Stack Overflow   , by plotting qid against time.The topics of 'closed' questions are relevant to the community despite the questions themselves being unfit for the Stack Overflow format. We extract such tags found in deleted questions for further analysis .Note that streams for synthetic data differs from NASDAQ data in terms of the lag and the missing update distributions. The stream generation process is as follows: A stream would pick elements of the Z vector sequentially and could perform the following three operations: a Simulate missing update: Ignore the picked element and move to the next element with Bernouilli probability = pmiss k   , b Simulate independent error: Add Gaussian noise with precision β k > 1  , c Simulate Lag: Publish the noisy update after lag governed by Uniform distribution in the range 1 − 10.The key point of this algorithm is that it guarantees that a large object which is constructed via successive append operations will have maxima1 leaf utilization i.e.  , all but the last two leaves will be completely full. Propagate the counts and pointers for the new leaves upward in the tree using the stack built in l  , and handle node overflow as in the insertion algorithm.Rare exceptions like the new Ask.com has a feature to erase the past searches.We note that the complete example  , including the exemplary queries in an executable form  , is available at http://purl.org/NET/scovo 4—shows the list of high-performing airports along with the time period  , starting with the best airport in terms of " on-timeness " .Questions on Stack Overflow are marked 'closed' if they are deemed unfit for the question-answer format on Stack Overflow and indicate low quality. This shows that author-deleted questions are inferior in quality than moderator-deleted questions and require more work to improve their content.It is a graph  , where each user corresponds to a vertex and each user-to-user connection is an edge. Orkut is a large social networking website.Note that we did not use the public LETOR data 15  , because the numbers of queries in the datasets are too small to conduct meaningful experiments on query dependent ranking. All the methods tested in this section are based on the same feature set.An exception was BROOF gradient which converged at about 100 iterations for the largest datasets. On average  , our strategies converge at about 15 iterations on the LETOR datasets  , and around 5 to 10 iterations on the multi-relevance judgment datasets.The statistics show that Stack Overflow is a very popular programming CQA with 5.1M questions   , 9.4M answers and 2.05M registered users. Table 3shows the overall statistics of user-generated content on Stack Overflow between August 2008 inception to June 2013 current.To answer the first research question  , we applied two state-of-the-art extractive summarization techniques— LexRank 13 and Maximal Marginal Relevance MMR 4 —to a set of API types  , their documentation   , and related Stack Overflow threads. * non-qualified API type prefixed with " a " or " an "   , case-insensitiveThe first evaluation is based on the LETOR datasets 17  , which include manual relevance assessments. The rankers are compared using the metric rrMetric 3.Figure 3shows the cumulative distribution area chart of deleted questions over a 49-month period between September 2009 and June 2013. We now perform a temporal trend analysis of deleted questions on Stack Overflow.P2 explicitly stated that while he did publish results based on quantitative methods in the past  , he would not use the same methods again due to the potential of technology-induced bias. We asked P1  , P2 and P4 about the possibilities of more quantitative tools on top of the current digital archive  , and in all cases the interviewees' response was that no matter what tools were added by the archive  , they were unlikely to trust any quantitative results derived from processing erroneous OCRed text.They may be classified as distinct documents by some users  , and duplicates by some others. Similarly  , a digital document may exist in different media types  , such as plain text  , HTML  , I&TEX  , DVI  , postscript  , scanned-image  , OCRed text  , or certain PC-a.pplication format.In both systems  , one question can have multiple topics. In contrast  , Stack Overflow anonymizes all voters and only displays the accumulated number of votes  , which can be negative Sorted Topic Bucket By # of FollowersThe FedWeb 2014 Dataset contains both result snippets and full documents sampled from 149 web search engines between April and May 2014. Though classification of resources into verticals was available  , our system did not make use of them.moviepilot provides its users with personalized movie recommendations based on their previous ratings. In order to empirically estimate the magic barrier  , a user study on the real-life commercial movie recommendation community moviepilot 4 was performed.The JavaScript precision was 0.97 while the recall was 0.96. Since none of the previous papers investigated JavaScript  , we just chose four Stack Overflow tags for which there were a large number of associated questions.Question content features are based on the text and metadata of the question while syntactic or text syntax features are based on the writing style of the text in the question. Community based features are derived via the crowdsourced information generated by the Stack Overflow community.Second  , we will use the rank of spots on TripAdvisor and the rate of the reviews as the indicators of spots' quality  , it embodies the commonness of recommendation system  , while we use the probability of user interest for each category and the classification label of each user‐spots pairs as the reflecting of the user personalized interest  , it embodies the  Rest of the spots sorting First of all  , we sort the probability of user interest of dislike for each category in ascending way. First of all  , the recommended spots should conform to the requirements of location in context  , so we will use location as a criterion of the recommendation system.Since its creation in 2005  , it has been widely used for spreadsheet research and evaluation. The EUSES corpus consists of 4 ,037 real-life spreadsheets from 11 categories.We collected 250 attractions in Paris from the TripAdvisor website . The second dataset is used to generate the second feature representation described in Section 4.1.2.In WPBench  , user interactions are recorded when users are browsing a set of the most popular Web 2.0 applications. Therefore WPBench produces a fairer benchmark for different Web browsers.This model is easily extensible by defining new factors and agents pertaining to the actual statistical data. Dimensions of a statistical item are factors of the corresponding events  , attached through the dimension property  , pointing to an instance of the SCOVO Dimension class.For each respondent  , the second section is repeated for two video tutorials randomly chosen from a sample of 20 video tutorials randomly selected from the 4 ,747. While approaches to recommend Stack Overflow discussions exist 32  , our aim is to determine whether the textual content of the video tutorial fragment can be used to retrieve relevant discussions .The results also suggest that query terms are valuable information for sake of ranking. Experimental results  , obtained using the LETOR benchmark  , indicate that methods that learn to rank at query-time outperform the state-ofthe-art methods.Thus  , many authors do not have any citation example in the training set. BDBComp has several authors with only one citation.We use the 5-fold cross validation partitioning from LETOR 10. The optimal parameters for the final GBRT model are picked using cross validation for each data set.We have proposed a vocabulary  , SCOVO  , and discussed good practice guidelines for publishing statistical data on the Web in this paper. When the data is present in a table with a certain layout  , it turns out to be advantageous to not only repurpose and link the data  , but also reuse the data table in the author's intended form.Allamanis and Sutton perform a topic modeling analysis on Stack Overflow questions to combine topics  , types and code 5. They observe certain characteristics of unanswered questions which include vagueness  , homework questions etc.Section 5 evaluates SERT with application benchmarks from Ask.com. Section 4 describes our implementation.Table 6shows four edit types edit tags  , edit body  , edit title and suggested edits for deleted questions in our experimental dataset. In this part  , we analyze edit histories of deleted questions on Stack Overflow.The category Microsoft has a homonymous page  , categorized under Companies listed on NASDAQ which has the head lemma companies. An example is provided in Figure 2.CodeTube starts from a set of videos relevant to a broad topic of interest e.g.  , Android development  , J2EE. CodeTube recommends video tutorial fragments relevant to a given textual query  , and complements them with Stack Overflow discussions related to the extracted video fragments.In an answer to a question about " DataSource and DriverManager on J2SE "   , the Stack Overflow user first cites a statement from the API documentation  , but then elaborates on it further than the API documentation does. Figure 1 shows the source of the sentence in the first example .In fact  , by taking the OpenStreetMap polygons for Santa Barbara and Ventura and defining a regular point grid of 1 × 1 km  , we can compute the probability of grid points contained in Ventura to locate in the southeast of Santa Barbara grid points. This may be true for a certain point-feature representation of the cities but is not correct for all points inside the city boundaries.One threat to internal validity of our evaluation is that we were unable to validate analysis results of spreadsheets in the EUSES corpus by their original users. We made best effort in choosing representative and real-life experimental subjects.Questions which are very poor in quality or extremely off topic in nature are deleted from the website. In this work  , we focus our attention on deleted questions poor quality on Stack Overflow.Note that  , this pre-defined hard categorization can also be used to improve ranking by applying the same method in Section 3.3. The statistics of queries for three categories in LETOR 3.0 can be found in 16.Each performance value on test sets shown in the figures is averaged using five-fold cross validation as the same way used in LETOR. By varying β from 0 to 1 with a step of 0.05  , the curves of the ranking performance of FocusedRank in terms of κ-NDCG 4 and κ-ERR are shown in Figure 1and Figure 2.Table 2summarizes the most popular point-of-interest annotations currently found in the OpenStreetMap data. As a result  , we obtained 192 million pointsof-interest   , which are annotated with roughly 800 million property-value combinations.Simple K-nearest neighbour KNN with K set to 20 and Regression Tree was used to perform point-wise LETOR. Multiple LETOR methods have been tried  , which are different in many ways and we expect them to be complimentary during the final fusion.The publication of the OpenStreetMap data using Triplify adds a completely new dimension to the Data Web: spatial data can be retrieved and interlinked on an unprecedented level of granularity. Performance results for retrieving points-of-interest in different areas are summarized in Table 3.The reason for this is that the performance of the neighbourhood and latent factor models was close to 0 7 . Most notably  , we have only reported MAP scores for the MoviePilot data.This section presents various digital resources of each scanned volume  , selection of input for the metadata generation system  , the method for automatic metadata generation  , and the set of metadata elements generated by the system. Finally  , generated metadata information and OCRed text are integrated to support navigation and retrieval of content within scanned volumes.Friendster 1 and Orkut 2 are among the earliest and most successful SNSs. As a kind of online application  , SNSs are useful to register personal information including a user's friends and acquaintances on these systems; the systems promote information exchange such as sending messages and reading Weblogs.Stack Overflow is driven by the goal to be an exhaustive knowledge base on programming related topics and hence  , the community would like to ensure minimal possible noise on the website. Stack Overflow is a free  , open no registration required website to all users on the Internet and hence  , it is a necessity to maintain quality of content on the website 4.Beyond the social values associated with the online forums  , the owners of the forums also directly benefit from the traffic of active forums  , e.g. discussing travel experiences in TripAdvisor.but outperforms several supervised methods  , achieving the state-of-the-art performance. However  , our unsupervised method not only surpasses the unsupervised methods  , Table 1: MAP scores of unsupervised SCSM and other methods on the Pascal VOC  , Wiki  , Wiki++ and LabelMe datasets  , while CDFE  , GMMFA  , GMLDA  , LCFS and JFSSL are supervised methods.We also aim at improving the OpenStreetMap data usage scenario  , e.g. Hence  , we envision some extensions to Triplify such as a more external annotation of the SQL views in order to allow optionally SPARQL processing on Triplify endpoints.One reason for the ubiquity of Orkut is most likely due to the power of influencers and the practice of account gifting. Contrasting the social stigma in America where only young people are perceived to use popular social networks  , Orkut is part of society in Brazil  , as it is not only used by teenagers  , but parents  , relatives  , and even taxi drivers as well.The other condition codes returned by the stack operations include stuck overflow for Push and siaclc emp-ty for Pop and Top. By convention  , procedures return SUCCESS when they execute normally.Our study design was driven by several features that we discovered in this massive corpus. For scanned articles  , per-article metadata such as titles  , issue dates  , and boundaries between articles are also derived algorithmically from the OCRed data  , rather than manually curated.As another example  , in case the program can not recognize the volume and issue number due to OCR error  , such as " IV " was OCRed as " it "   , the program will use the previous or the following title page information  , if available  , to construct the current volume or issue metadata. For instance  , in order to tolerate OCR errors in volume and issue number line  , we set the Levenshtein Distance20 between an examined string and the target " volume " and " issue " keywords as a parameter and choose the optimal value based on experiments.They developed an improved search engine for content on Stack Overflow which recommends question-and-answer pairs as opposed to entire Q&A threads based on a query. 10.A question on Stack Overflow has three major sections which can be edited – title  , body and tags. experienced community members and moderators to maintain content quality. LETOR: Using only statistical features associated with matched terms features L1−10 and H1−3 in Tab. 1.One area where none of the standards provided duced above was far from trivial. Some users are interested in highly unstructured text data OCRed from field journals  , or more conventional relational tables of data  , so BigSur does not require that these super-classes are used.While approaches have been proposed to support developers by mining API documentation 36  , 37 and Q&A websites such as Stack Overflow 14 ,32   , or by automatically synthesizing code examples from existing code bases 1 ,6 ,16 ,25  , there is currently no approach aimed at leveraging relevant information found within fragments of video tutorials and linking these fragments to other relevant sources of information. To the best of our knowledge  , there is no support for integrating this kind of complementary  , cross-platform information about a programming topic.Thus  , the results reported here refer to non-normalized data. We also tried different strategies to normalize our feature vectors  , including L2-norm  , z-score and the LETOR normalization procedure 17  , with no improvements. LETOR: For comparison purposes  , a LETOR-like document selection methodology is also employed. Hedge finds many relevant documents " common " to various retrieval systems   , thus documents likely to contain many of the query words.According to the Stack Overflow guide 2   , a good answer  , besides being correct   , should be clear  , provide examples  , quote relevant material  , be updated  , and link to more information and further reading. In forums such as Stack Overflow  , the answers are expected to be correct and should be ranked according to their quality.Auto- Comment extracts code-descriptions mappings  , which are code segments together with their descriptions  , from Stack Overflow  , and leverages this information to automatically generate descriptive comments for similar code segments in open-source projects. 39  , since it also harnesses the natural language text available on Stack Overflow.Therefore  , there exists a strong need for mechanisms for archiving  , preserving  , indexing  , and disseminating the wealth of scientific knowledge produced by the Brazilian CS community. BDBComp has been designed to be OAI compliant and adopts Dublin Core DC as its metadata standard.This is an example of the API documentation lacking information on purpose 18 since it only states which alternative is preferred without explaining why. In an answer to a question about " DataSource and DriverManager on J2SE "   , the Stack Overflow user first cites a statement from the API documentation  , but then elaborates on it further than the API documentation does.Our preliminary findings  , obtained through the analysis of archival data from Stack Overflow and qualitative coding  , indicate that Q&A websites are particularly effective at code reviews  , explaining conceptual issues and answering newcomer questions. In this paper  , we pose research questions and report preliminary results to identify the role of Q&A websites in software development using qualitative and quantitative research methods.The two key issues which arise in the context of crowdsourcing are quality— is the obtained solution or set of contributions of high quality ?— as well as participation— there is a nonzero effort or cost associated with making a contribution of any quality in a crowdsourcing environment which can be avoided by simply choosing to not participate  , and indeed many sites have too little content . Answers  , Stack- Overflow or Quora.In addition  , user input similar to the one we gathered as part of our comparative study could be used to continuously improve the extraction of insight sentences. For example  , each insight sentence could be accompanied by an expandable widget which shows the entire thread on Stack Overflow from which the insight sentence originated.From the NCBI site  , 4032 RefSeq records linked from our MEDLINE subset and that contain gene sequences were downloaded. Only the default OAI metadata format  , oai_dc  , is available for each OAI item.We also find statistically significant gains in performance on the larger CIFAR-10 and 100k TinyImages datasets. This result is statistically significant based upon a paired t-test across 10 random training/testing partitions of the dataset p-value: ≤ 1.7 × 10 −5 .ing monthly harvest of fruits. illustrate ambiguous computation smells using extracted from the EUSES corpus to detect and repair these smells.Stack Overflow 4 : This dataset comes from a popular question answering service found among the datasets of the Stack Exchange XML dump. We created a HIN by categorizing the entities into vertex labels: author  , paper  , conference  , and terminology.Seahawk automatically formulates queries from the current context in the IDE and presents a ranked and interactive list of results. 2  is an Eclipse plug-in that integrates Stack Overflow content into an integrated development environment IDE.Both other approaches are not capable of representing historical data and only provide statistics for one point-in-time. The most distinguishing feature of SCOVO is the ability to express complex statistics over time while still keeping the structural complexity very low.She not only used Orkut herself but created accounts for her mother  , sister  , and brother. One of our participants  , an 18-year-old student  , was not only technologically-savvy in terms of adopting new websites and exploring advanced features of computer applications  , she was also a strong " gifter. "Experimental results manifest that RSRank not only achieves good sparsity in practice  , but also exhibits a high level of performance in comparison with several proposed baseline algorithms. Finally  , we evaluate the proposed method on LETOR 3.0 benchmark collections1.In Figure 1  , the performance variations on graded MQ2007 are represented as curves with open symbols while that on0.0 0 . Each performance value on test sets shown in the figures is averaged using five-fold cross validation as the same way used in LETOR.Since we are only training on a single topic  , resulting accuracy is far lower than what typically published LETOR results. Instead  , for each topic we train on it alone and evaluate on all other topics  , then we average this over all topics.In Ranking SVM plus relation  , we make use of both content information and relation information. Actually  , the results of Ranking SVM are already provided in LETOR.analyze questions on Stack Overflow to understand the quality of a code example 20. Nasehi et al.Next  , we experiment with the extent that the algorithms can produce quality recommendations for groups  , using the MoviePilot data. Recommendations to Groups.We now perform a temporal trend analysis of deleted questions on Stack Overflow. Hence  , it is important to perform a longitudinal study about deleted questions on Stack Overflow.We divide our experiments into two parts. The second part is conducted on the same Orkut data set to investigate the scalability of our parallel implementation.The retrieval performance achieved was at least as good as the LETOR 4.0 baselines. To test the correctness of our SVM ranking algorithm and the correctness of the feature extractor we extracted features from the Million Query 2008 collection  , and performed a five-fold cross validation.Community Value. We also make observations about the relative quality of deleted questions in context to 'closed' questions on Stack Overflow. The DjVu XML file presents logical structures of the OCRed text. We choose the DjVu XML 2 file as the main input of the metadata generation system for several reasons:  The DjVu XML file contains full OCRed text.Unfortunately  , the LDA based topic mining approach has failed in this task. And we hopped to be able to generate weighted distribution of words that could potentially identify multiple topics of a query from the top ranked documents  , and by using these approximations of multiples topics  , we can perform multiple searches for the same query with different expansions  , followed by separate LETOR for each expanded query  , and eventually merge the results with data fusion.The latter is typical in our case because the scores generate by different LETOR algorithms are different in terms of scale and rank-score curves. Many previous studies on Data fusion 17 18 19 suggested that when the scores of the systems to be combined are commensurable   , using score based fusion methods are better than using only the rank positions  , but when the scores are incompatible or if the systems generate different rank-score curves  , rank based fusion techniques are better.Section 6 summarizes related work. Section 5 evaluates SERT with application benchmarks from Ask.com.These were estimated from a set of double annotations for the FedWeb 2013 collection  , which has  , by construction  , comparable properties to the FedWeb 2014 dataset. We see that the best resource depending on the queries from the General search engines achieves the highest number of relevant results and/or the results with the highest levels of relevance  , followed by the Blogs  , Kids  , and Video verticals.Also  , data mining for high-level behavioral patterns in a diachronous  , heterogeneous  , partially- OCRed corpus of this scale is quite new  , precedented on this scale perhaps only by 8 which brands this new area as " culturomics " . We list them here to explain our study design.In both experimental setups  , i.e.  , the LETOR and WSDM setup  , both BARACO and MT have good performance in most cases and high agreement with the ground truth. The estimated difference between rankers is strongly correlated with the ground truth in the WSDM dataset  , suggesting that both methods can estimate the difference between rankers well given the logged user interactions with the production ranker.In calculation of MAP  , we viewed 'definitely' and 'partially relevant' as relevant. Figure 4shows the results on Letor OHSUMED dataset in terms of MAP and NDCG  , averaged over five trials.However  , our unsupervised method not only surpasses the unsupervised methods  , Table 1: MAP scores of unsupervised SCSM and other methods on the Pascal VOC  , Wiki  , Wiki++ and LabelMe datasets  , while CDFE  , GMMFA  , GMLDA  , LCFS and JFSSL are supervised methods. This is because supervised methods rely on semantic labels to reduce the semantic gap of different modalities  , but unsupervised methods only use pair-wised information.For each query in the query set  , all the points in the training set are ranked according to the Hamming distance between their binary codes and the query's. We perform Hamming ranking using the generated binary codes on the CIFAR-10 and NUS-WIDE datasets.Runs are ordered by decreasing CF-IDF score. nDCG@20  nDCG@10  nP@1  nP@5  uiucGSLISf2 0Figure 1: Per-topic nDCG@20 and nDCG@10 for both FedWeb RS runs.We perform Hamming ranking using the generated binary codes on the CIFAR-10 and NUS-WIDE datasets. This is because the number of iterations needed to learn U decreases as the code length increases.Even assuming that these slow algorithms scale linearly with the problem size  , which is not true for most of them  , the analysis of large graphs may require unaffordable times. Oslom takes several days to analyze the Orkut graph whereas SCD finds the communities in a few minutes.The goal of this study is to evaluate CodeTube with the purpose of determining the quality of the extracted video fragments and related Stack Overflow discussions perceived by developers. The context of the study consists of participants and objects.With LETOR data  , since HP and NP are similar tasks but TD is rather different  , we conducted experiments on HP03- to-NP04 and NP03-to-TD04 adaptation  , where the former setting is for adapting to a similar domain and the latter for adapting to a distinct one. The free parameters λs and λt were set such that λs λt is inversely proportional to |Ds| |Dt|They found that over 35000 developers contributed in crowd-documenting Android   , covering 87% of the classes. investigated the recent phenomenon of crowd documentation   , as measured by the questions about API elements asked and answered on the Stack Overflow website 20.For the ease of presentation   , we highlight the clusters by different colors such that the size and shape of the clusters are clearly illustrated in the figures. All figures are generated by our modified version of Java OpenStreetMap Editor 2 which is a map editor for OpenStreetMap 3 written in Java.They experimented with a baseline run utTailyM400  , and a variation using a Gaussian distribution instead of a Gamma distribution utTailyNormM400. In comparison with their original publication   , the FedWeb submission assumed that all resources are of the same size.Also  , deleted questions are significantly low in quality than 'closed' questions. In general  , deleted questions are extremely poor in worth to the Stack Overflow community.On the other hand  , if we look at Figure 7b  , we notice that the distribution is polarized towards the maximum value— first quartile  , median and third quartile equal to 3—with 14 82% of the total of the videos where the Stack Overflow discussions were considered as complementary. On the one hand  , the perceived relevance is relatively low  , with only 38% of the Stack Overflow discussions achieving a median relevance of 3.We then combine page features and line features for volume level and issue level metadata generation. During the parsing of the XML file  , the system calculates features for every word  , line  , paragraph  , and page of the OCRed text.We also used the MoviePilot data  , by disregarding the group memberships. We repeat this process five times to compute 5-fold cross validated results.Information about trees and parks is extracted from OpenStreetMap. on whether the street is in or near a park.Deleting answers in Brainly: Brainly tries to maintain high quality answers  , and moderators are recruited to participate heavily in deleting questions. Answers and Stack Overflow  , which encourage users to contribute to the site in order to earn a high reputation.Many of such features cannot be easily integrated in the formulae of conventional retrieval models due to lack of theoretical foundations. One advantage of LETOR is that it allows incorporating features that addresses various characteristics of a document and its relevance to a topic.We highlight our contributions and key results below. In our study  , we use more than 15M reviews from more than 3.5M users spanning three prominent travel sites  , Tripadvisor   , Hotels.com  , Booking.com spanning five years for each site.NPQ is orthogonal to existing approaches for improving the accuracy of LSH  , for example multi-probe LSH 7  , and can be applied alongside these techniques to further improve retrieval performance. Figure 3: 1 LSH PR curve for 22k Labelme 2 LSH AUPRC on 22k Labelme 3 LSH PR curve for CIFAR-10 4 LSH AUPRC for CIFAR-10 5 LSH PR curve for 100k TinyImages 6 LSH AUPRC for 100k TinyImages ment of quantisation thresholds.The performance of runs is measured by the nDCG@20  , which is the main evaluation metric used at the FedWeb research selection task. The " engines " column shows the results of the runs generated using the big-document strategy; " search " column is about all the runs generated by the snippetbased big-document strategy; and " docs " column presents the results of the runs generated by the small-document strategy.Compared to state-of-the-art summarization techniques or pattern-based techniques which do not take any meta data into account  , SISE achieved higher precision and usefulness. This work shows that the large amount of meta data on Stack Overflow can be used for the extraction of insight sentences .Such query-independent factors are orthogonal to our approach  , so combination of the two could probably further improve the performance. The good performance of their runs largely depends on a queryindependent prior ranking of the resources learned on the results from FedWeb 2013.Applications of social influence in social media. We adapt the E-M algorithm of Saito  , Nakano  , and Kimura 2008 to extract social influence in TripAdvisor  , and use it as input to our participation maximization algorithm.And we hopped to be able to generate weighted distribution of words that could potentially identify multiple topics of a query from the top ranked documents  , and by using these approximations of multiples topics  , we can perform multiple searches for the same query with different expansions  , followed by separate LETOR for each expanded query  , and eventually merge the results with data fusion. Originally  , this was performed after the first pass when most of the relevant documents are assumed to be retrieved by using BM25 and Indri with pseudo relevance feedback .Figure 4shows the results on Letor OHSUMED dataset in terms of MAP and NDCG  , averaged over five trials. We conducted 5-fold cross validation experiments  , following the guideline of Letor.They learn multiple ranking models for each of these clusters where they incorporate the notion of freshness into the traditional letor approach by generating hybrid labels based on relevance and freshness judgments similar to Dong et al. The clustering employed is a soft-clustering where each query is associated with all the clusters with different association weights.For our experiments we work with three public data sets: TD2004 and MQ2007 from LETOR data sets 24 and the recently published MSLR-WEB10K data set from Microsoft Research 1. All three data sets are pre–folded and come with evaluation scripts that allow fair comparison of different ranking algorithms.Underflow and Overflow denote unsafe objects. there are less than 100 elements on the stack before an element is pushed on it.The owners of author-deleted questions observe that their questions are attracting down votes which affects their reputation. We argue that question owners of author-deleted questions exhibit such a behavior as they want to maintain a healthy reputation earned on Stack Overflow 10 .Two OAI metadata formats are provided for each OAI item: refseq: contains the refseq records in our refseq XML format. Hence  , we created a simple RefSeq XML schema for the RefSeq OAI repository 2.The coordination mechanism allows an additional filter to be added to filter out the sidebars and footers  , and to return only the pure article text. For example  , most of the 10 news sites  , which are used for the current GeoTopics  , have sidebars and footers in their articles  , which cause falsematching problems e.g.  , 'NASDAQ' was ranked high because it is appeared on the side bars in many of the news articles.Combining each time different subsets to make the training  , the validation and the test set  , the LETOR authors create 5 different arrangements for five-fold cross validation. In LETOR  , data is partitioned in five subsets.Dart's home page 5 acknowledges the incompleteness of the documentation and points to Stack Overflow. 26   , several industrial developers do not trust documentation and prefer to rely on source code.By using a dataset from the Stack- Overflow Q&A Forum  , we evaluated the sets of features and compared our method to 3 other ones previously published in literature. Our L2R method was trained to learn the answer rating  , based on the feedback users give to answers in Q&A Forums.In this section  , we introduce Quora  , using Stack Overflow as a basis for comparison. Quora is a question and answer site with a fully integrated social network connecting its users.With the help of this annotation tool  , the current LabelMe data set contains as large as 200 ,790 images which span a wide variety of object categories. LabelMe is a web-based tool designed to facilitate image annotation.The frequency of occurrences of cp-similar regions has been shown by the analysis carried out on the EUSES spreadsheet corpus as reported in 13. Similarities in spreadsheet formulas have been exploited in consistency checking 16 and testing of spreadsheets 8.Users can import code snippets and create links between documents and source code by means of language-independent annotations  , and they can use annotations to take advantage of the versioning system to collaborate and suggest documents to teammates. We detailed how it lets users interact with Stack Overflow documents in a novel way.We use the validation set to decide which kernels to use in the transductive system. Following LETOR convention  , each dataset is divided into 5 folds with a 3:1:1 ratio for training  , validation  , and test set.The Why block of Fig- ure 1corresponds to this section. Also  , 'closed' questions questions which are deemed unfit for Stack Overflow which do not serve as useful sign points may also be deleted.We recall that experienced community members viz. Stack Overflow delineates an elaborate procedure to delete a question.We believe that video tutorials have a different purpose than Stack Overflow discussions. Results indicate that  , while respondents only considered the retrieved discussions fairly relevant to the fragments from where the queries were generated  , they almost totally agreed about the complementarity of the provided information.Both hedge and LETOR-like document selection methodology   , by design  , select as many relevant documents as possible . Furthermore   , when relevant and non-relevant documents in the training data set are very similar to each other  , performance of the resulting ranking functions decline.It thus took about 1.7 seconds to analyze one spreadsheet on average. Running AmCheck over the whole EUSES corpus took about 116 minutes.However  , this information is not directly available in the publicly available data dumps provide by Stack Overflow . We recall that a question on Stack Overflow can either be deleted by the author of the question or by a moderator .Hilliness. Its score depends on the number of shops  , bars  , restaurants  , and parks on the street extracted from OpenStreetMap and on the street's type.While all topics in Stack Overflow are different  , they are all related to programming. Startups " is the most popular one which takes 3.7% of the questions .In addition  , Stack Overflow consists of millions of questions with thousands of topics recall that there are 34 ,000+ tags. We do not have access to information on answers and other crowdsourced information like view counts  , favorite votes and question score.the LETOR benchmark 5 requires only one parameter c as input. Since higher NDCG values are obtained when the most relevant results are ranked on the top followed by the less relevant and the irrelevant ones  , the above observation enforces our claim that our method favors the most relevant results of each query.Apart from existing as a question-answering website  , the objective of Stack Overflow is to be a comprehensive knowledge base of programming topics. The underlying theme of Stack Overflow is programming-related topics and the target audience are software developers  , maintenance professionals and programmers .In most cases  , significant increases in effectiveness are found for other popular projection functions including SH and SKLSH across both datasets Tables 1-2. We find that the superior retrieval effectiveness of GRH+NPQ is maintained when the hashcode length is varied between 16-128 bits for both LSH and PCA projections Figure 3a-b on CIFAR-10.Similarly  , a digital document may exist in different media types  , such as plain text  , HTML  , I&TEX  , DVI  , postscript  , scanned-image  , OCRed text  , or certain PC-a.pplication format. Note that  , however  , indirection duplicates are not possible with technical reports.Empirically measuring the quality of recommendations has  , in the past  , fallen into two camps. This dataset  , from the German movie-rental site MoviePilot  , was released as part of theThis lag may be due to the time required for the undelete voting procedure as defined by the Stack Overflow community guidelines. We observe that moderator-deleted questions take a relatively longer time to undelete. We evaluate Section 4 the probabilistic model alongside state-of-the-art CF approaches  , including popularity based  , neighbourhood  , and latent factor models using household rating data from MoviePilot 1 .  We then use this model to derive a framework for group recommendation Section 3.2 that  , unlike previous work—which focuses on merging recommendations computed for individual users—uses the principles of information matching in order to compute the probabilities of items' relevance to a group  , while taking the entirety of the group into consideration.Nevertheless  , in a setup similar to LETOR setup  , as in our experiments  , we show that substantially less documents than the ones used in LETOR can lead to similar performance of the trained ranking functions. Therefore  , in the case where hundreds of raw features are employed  , ranking functions may need more than 1% of the complete collection to achieve optimal performance.In the experiment we used data obtained from a commercial search engine. Note that we did not use the public LETOR data 15  , because the numbers of queries in the datasets are too small to conduct meaningful experiments on query dependent ranking.Study 2 S2 is a pilot survey that gathers data from 11 developers who asked Java cryptography-related questions on Stack- Overflow. Study 1 S1 analyzes the top 100 Java cryptography questions asked on the popular question/answer site StackOverflow.After sending out the invitation  , invitees had two weeks to respond. The Qualtrics survey platform allowed us to achieve randomization and balancing  , by automatically selecting video tutorials with related Stack Overflow discussion and queries to be evaluated by each respondent.To ensure our example repository is always current  , we also continually monitor Stack Overflow to parse new source code examples as they are posted. If either of these is true  , the pages are augmented by injecting new HTML elements into the page that represents links to other related resources.We also showed an application of the proposed method in an online ad serving system for matching and ranking ads for a given Web page which indicates the applicability of the proposed algorithm in real online applications as we improved the CTR and RPM significantly in the online bucket tests. Our experiments on LETOR 3.0 benchmark dataset show that the  NDCG-Annealing algorithm outperforms the state-of-theart algorithms both in terms of performance and stability.We see that deleted questions have higher percentage of questions with zero score than 'closed' questions. Figure 9 shows various quantities of question quality indicators for 'closed' and deleted questions on Stack Overflow .In terms of votes  , both Quora and Stack Overflow allow users to upvote and downvote answers. We use this as a minimum threshold for our later analyses on social factors on system performance.Quora makes visible the list of upvoters  , but hides downvoters. In terms of votes  , both Quora and Stack Overflow allow users to upvote and downvote answers.Lastly  , we plan to integrate additional sources of information other than Stack Overflow  , towards the concept of a holistic recommender. We also plan to improve the user experience.The methodology that we adopted sought to align itself to the structure of the CAMRa challenge. Next  , we experiment with the extent that the algorithms can produce quality recommendations for groups  , using the MoviePilot data.We use rule-based approach for title detection using page and line features calculated from OCRed text  , bounding box information  , and context analysis. In terms of the mapping between page index  , the index of a scanned page in the viewable PDF file  , and page number  , the number printed on the original volume  , the program recognizes available page numbers on scanned pages by analyzing the OCRed text in particular areas of pages.The results on seven datasets in LETOR 3.0 show that the NDCG-Annealing algorithm can outperform the baselines and it is more stable. We compare the NDCG-Annealing algorithm with linear ranking function described in section 3 with baselines provided in the LETOR 3.0 datasets.LabelMe is a web-based tool designed to facilitate image annotation. The first data set is 22K LabelMe used in 22  , 32.To safeguard user privacy  , all user and community data were anonymized as performed in 17. Our community membership information data set was a filtered collection of Orkut in July 2007.We also tried different strategies to normalize our feature vectors  , including L2-norm  , z-score and the LETOR normalization procedure 17  , with no improvements. Using cross-validation in V  , we also varied cost j between 10 −3 and 10 3   , finding that the best choice was j=100  , in most cases.We imported the Shapefiles into a PostGIS database and created virtual geospatial RDF views on top of them using Ontop-spatial  , as described at https://github. However  , it was more convenient for us to download the most up-todate original OpenStreetMap data about Bremen  , available as Shapefiles 10 .On the other hand  , the boosting method is highly dependent on the ranking of the resources  , as we observe when a better resource selection method is used BM25 desc in FedWeb 2013 or the hybrid run in FedWeb 2012. This suggests that  , when the resource ranking is not good the performance of the hybrid method in resource selection is far from optimal  , the diversification approach seems to help a little bit.Determining whether a sentence is meaningful on its own is non-trivial  , and while our evaluation showed that a supervised approach can detect such sentences based on part-of-speech tags with a higher precision than summarization or pattern-based approaches   , we expect that the precision can further be improved with a deeper understanding of each sentence and its dependencies on other sentences or code snippets. We plan to extend this work beyond the Java API and we plan to experiment with more features that capture the grammatical structure of sentences on Stack Overflow.We present our parallelization framework of LDA in Section 4 and an empirical study on our Orkut data set in Section 5. In Section 3  , we show how ARM and LDA can be adapted for the community recommendation task.However  , the relationship between the number of reviews and the number of hotels Figure 3c is below an ideal straight line for the first few points  , since there are surprisingly few hotels with fewer than 50 reviews. TripAdvisor 1-star  , 2-star  , 3-star  , 4-star  , 5-star distribution of the number of reviews per reviewer Figures 4c also follows a power law.Finally  , we offer our concluding remarks in Section 6. We present our parallelization framework of LDA in Section 4 and an empirical study on our Orkut data set in Section 5.Figure 7shows the distribution of question deletion initiator moderator or author on Stack Overflow. We download the unique web pages of deleted questions in our experimental dataset and employ a regular expression to extract this information.In addition  , CodeTube searches and indexes Stack Overflow discussions relevant to each video fragment. Finally  , all this information is indexed using information retrieval techniques.More in particular  , only results from the top 20 highest ranked resources in the selection run were allowed in the merging run. An important new condition in the Results Merging task  , as compared to the analogous FedWeb 2013 task  , is the requirement that each Results Merging run had to be based on a particular Resource Selection run.We obtained our snippets from the Stack Overflow data repository provided for the 2013 MSR Challenge 3. We first populated Baker with a number of source code examples .Whenever applicable  , We also used terms from SDMX extensions 19 which augment the Data Cube Vocabulary by defining URIs for common dimensions  , attributes and measures. It extends SCOVO 10 with the ability to explicitly describe the structure of the data and distinguishes between dimensions  , attributes and measures.The text pre-processing phase is identical to the one explained in Section 2.2. For Stack Overflow we separately index each question and answer for each discussion.We analyzed the data to classify values into categories. We tested topes using the 720 spreadsheets in the EUSES Spreadsheet Corpus's " database " section  , which contains a high concentration of string data 10.Table 1summarizes the properties of these data sets. For our experiments we work with three public data sets: TD2004 and MQ2007 from LETOR data sets 24 and the recently published MSLR-WEB10K data set from Microsoft Research 1.We presented CodeTube  , a novel approach to extract relevant fragments from software development video tutorials. Lastly  , we plan to integrate additional sources of information other than Stack Overflow  , towards the concept of a holistic recommender.The question dataset stack overflow  , question  consists of 6 ,397 ,301 questions from 1 ,191 ,748 distinct users  , while the answer dataset stack overflow  , answer consists of 11 ,463 ,991 answers from 790 ,713 distinct users. For our analysis  , we extracted questions asked and answers posted between July 2008 and September 2013.The calculator itself augments the condition codes to include too few operands on stack. For the arithmetic component  , other codes include overflow and zero divide.The similar voting procedure to that of deletion is followed to undelete a question. Stack Overflow provides a procedure to undelete a deleted question.Previous research has successfully used knowledge patterns to detect and recommend fragments of API documentation potentially important to a programmer using an API element. This can be explained by the patterns' reliance on the systematic writing style of reference documentation which is not used in informal documentation formats such as Stack Overflow.However  , it was more convenient for us to download the most up-todate original OpenStreetMap data about Bremen  , available as Shapefiles 10 . OpenStreetMap datasets are available in RDF format from the LinkedGeoData project 9 .The second collection is the largest provided by the Wikia service  , Wookieepedia  , about the Starwars universe. In this article  , we refer to this sample as WPEDIA.Table 1 shows examples of deleted questions on Stack Overflow. Questions which are very poor in quality or extremely off topic in nature are deleted from the website.Table 1shows the statistics of the datasets included in the LETOR 3.0 benchmark. For these datasets  , there are 64 features extracted for each query-document pair and a binary relevance judgment for each pair is provided.The newspaper data set made available to us ranges from 1618 to 1995 4 and consists of more than 102 million OCRed newspaper items. We used the default Snowball stemmer for Dutch 6 .Linked- GeoData is derived from OpenStreetMap and OpenStreetMap is an open  , collaborative bottom-up effort for collecting this large-scale spatial knowledge base. The assumptions we make on the considered dataset are as follows.39  , since it also harnesses the natural language text available on Stack Overflow. Arguably the work that is most similar to ours is AutoComment   , the automatic comment generation approach introduced by Wong et al.These include 32 categories of data that occur most prevalently in the EUSES spreadsheet corpus's " database " section 211  , as well as 14 categories of data that we identified by logging what four administrative assistants typed into their web browsers over a 3 week period 10. To evaluate expressiveness  , we have used the TDE to implement and use topes for dozens of kinds of data.Stack Overflow provides a procedure to undelete a deleted question. We notice the presence of programming related tags like objective-c  , android and c# which points out these undeleted questions are relevant to Stack Overflow.Table 2We download all the available 24 database dumps for our study. The database dump contains publicly available information of questions  , answers  , comments  , votes and badges from the genesis of Stack Overflow August 2008 to the release time of the dump.Our experiments with two applications from Ask.com indicate the proposed techniques can effectively reduce response time and improve throughput in overloaded situations. Our design dynamically selects termination threshold  , adaptive to load condition and performs early termination safely.The results of our evaluation suggest that the context of sentences will play an important role when complementing API documentation with sentences from Stack Overflow. Upon selection of one sentence  , the sentence is expanded to show the surrounding paragraph from the original source  , along with a link to the corresponding Stack Overflow thread.This indicates that deleted questions are generally of very little worth and interest to the Stack Overflow community. Only ≈5-6% of deleted questions attract a positive question score or favorite votes.1 We obtained 1 ,212 ,153 threads from TripAdvisor forum 6 ; 2 We obtained 86 ,772 threads from LonelyPlanet forum 7 ; 3 We obtained 25 ,298 threads from BootsnAll Network 8 . We selected three forums of different scales to obtain source data.The Spambase Database is derived from a collection of spam and non-spam e-mails and consists of 4601 instances with 57 numeric attributes. The Ionosphere Database consists of 351 instances with 34 numeric attributes and contains 2 classes  , which come from a classiication of radar returns from the ionosphere .We observe similar trends in Quora. 60% of Stack Overflow users did not post any questions or answers  , while less than 1% of active users post more than 1000 questions or answers.The underlying theme of Stack Overflow is programming-related topics and the target audience are software developers  , maintenance professionals and programmers . Stack Overflow is a collaborative question answering Stack Exchange website.This context provides the hint that the user may not be interested in the search service provided by www.ask.com but instead be interested in the background information of the company. ask.com before query " Ask Jeeves " .However  , even in the 7 categories where programmers have published regexps on the web  , or where we could convert dropdown or radio button widgets to regexps  , F 1 was only 0.31 the same accuracy as Condition 4 in those categories  , owing to a lack of regexps for unusual international formats that were present in the EUSES spreadsheet corpus. F 1 would likely be higher if programmers were in the habit of validating more fields.Features in Letor OHSUMED dataset consists of 'low-level' features and 'high-level' features. In Letor  , the data is represented as feature vectors and their corresponding relevance labels .We evaluated our linking approach separately for one-word types and multi-word types. For 40 randomly selected types of the Java SE 7 API 20 types with one-word identifiers  , such as List  , and 20 types with multi-word identifiers  , such as ArrayList  , we selected the first five threads that the Stack Overflow API returned for each non-qualified type  , and we manually annotated whether the thread actually mentioned the API type.All presented NDCG  , Precision and MAP results are averaged across the test queries and were obtained using the evaluation script available on the LETOR website. To compute P@k and MAP on the MQ datasets the relevance levels are binarised with 1 converted to 0 and 2 converted to 1.While Baker actively monitors new Stack Overflow for new examples  , any snippet the browser extension has encountered can be included in the example list. Once again  , the browser extension detects from the mapping file that the user is visiting a page for which it has API usage examples.MAP is then computed by averaging AP over all queries. All presented NDCG  , Precision and MAP results are averaged across the test queries and were obtained using the evaluation script available on the LETOR website.The filtering and boundary extraction algorithms do however create classifiable ontology segments see Section 5.3.2. All current tableaux algorithm-based description logic reasoner systems stack-overflow when attempting to classify the basic extract of GALEN.There are 106 queries in the collection. Letor OHSUMED dataset consists of articles from medical journals .Contrasting the social stigma in America where only young people are perceived to use popular social networks  , Orkut is part of society in Brazil  , as it is not only used by teenagers  , but parents  , relatives  , and even taxi drivers as well. In Brazil  , Orkut  , a popular social network  , is the most popular website in the country 3.With the increasing number of topics  , i.e. The performance of runs is measured by the nDCG@20  , which is the main evaluation metric used at the FedWeb research selection task.Stack Overflow helps developers access a crowd of experts willing to help them with their challenges 9 . Developers blog about their experiences and disseminate them among their readers 12 .This enhancement enables a variety of new Linked Data applications such as geo data syndication or semantic-spatial searches. The publication of the OpenStreetMap data using Triplify adds a completely new dimension to the Data Web: spatial data can be retrieved and interlinked on an unprecedented level of granularity.The category of each community is defined on Orkut. The topic distributions of their Table 5: The community information for user Doe#1.The FedWeb 2013 collection contains search result pages for many other queries  , as well as the HTML of the corresponding web pages. Participants had to rank the 157 search engines for each test topic without access to the corresponding search results.The database dump contains publicly available information of questions  , answers  , comments  , votes and badges from the genesis of Stack Overflow August 2008 to the release time of the dump. Stack Overflow provides a periodic database dump of all user-generated content under the Creative Commons Attribute- ShareAlike 8 .These features attempt to describe the answer contents organization   , analyzing the use of images  , separation into sections  , links  , and HTML formatting tags. These are particularly important features in our study case since in Stack Overflow the asker is often searching for programming solutions.If the NASDAQ Computer Index were further divided into software  , hardware  , services  , etc.  , one can further analyze comparisons with them. In this instance  , the computer sector has been outperformed by one of its members Apple by a large margin.illustrate ambiguous computation smells using extracted from the EUSES corpus to detect and repair these smells. concludes this paper.Datasets: CIFAR-10 3 and Tiny 100K image 8 datasets both encoded with GIST features. Finally  , we then find the optimal value for the flexibility of margin C ∈ {0.01  , 0.1  , 1.0  , 10  , 100}.The goal of Stack Overflow is to be the most extensive knowledge base of programming related topics. The Why block of Fig- ure 1corresponds to this section.We have participated all the three tasks of FedWeb 2014 this year. Section 3 shows combination of the basic methods for different runs and the results will also be introduced.OpenStreetMap datasets are available in RDF format from the LinkedGeoData project 9 . OpenStreetMap OSM maintains a global editable map that depends on users to provide the information needed for its improvement and evolution.This is a highly counterintuitive outcome. When the LETOR collection was built  , the fact that documents with low BM25 score were selected only if they were relevant resulted in BM25 being negatively correlated with relevance in the LETOR collection.The snapshot of the Orkut network was published by Mislove et al. It is a graph  , where each user corresponds to a vertex and each user-to-user connection is an edge.The community counts its users in hundreds of thousands  , ratings in dozens of millions and movies in tens of thousands. moviepilot provides its users with personalized movie recommendations based on their previous ratings.Microsoft has a supercategory Computer and video game companies with the same head lemma. The category Microsoft has a homonymous page  , categorized under Companies listed on NASDAQ which has the head lemma companies.Answers on Stack Overflow often become a substitute for official product documentation when the official documentation is sparse or not yet existent 5 . In the 2 years since its foundation in 2008  , more than 1 million questions have been asked on Stack Overflow  , and more than 2.5 million answers have been provided.CodeTube recommends video tutorial fragments relevant to a given textual query  , and complements them with Stack Overflow discussions related to the extracted video fragments. After a discussion of the related literature Section 6  , Section 7 concludes the paper.For each scanned volume  , the metadata generation system takes the DjVu XML file as input and parses the hierarchy of objects contained within the file. Thus  , line features are designed to estimate properties of OCRed text within a line  , which can be calculated based on OCRed text and bounding box information in the DjVu XML file.5 present an empirical comparison of six measures of similarity for recommending communities to members of the Orkut social network. Spertus et al.Letor OHSUMED dataset consists of articles from medical journals . In the first experiment  , we used the Letor benchmark datasets 18: OHSUMED  , TD2003  , and TD2004.We apply conjunctive constraints on document image components to a straightforward document ranking based on total query-word frequency in the OCRed document text; in Fig- ure 2we show document images retrieved for two such queries. We consider integrated queries that our prototype makes possible for the first time.≈80% of deleted questions have a zero score which indicates that most deleted questions are of very little worth to the community. A question score is the net worth of the usefulness of a question as determined by the Stack Overflow community.To create features for the selection framework  , we use the published test runs 1 for these rankers to obtain the document scores for top 10 ranking documents  , and the list of 64 features that are available as part of LETOR. We use the results from three ranker baselines: Rank- Boost 5  , Regression  , and FRank 9 .Developing principled ways of incorporating action quality into models of user behavior in the presence of badges is an exciting direction for future work. For example   , we find on Stack Overflow that users' votes on questions are significantly more positive before they receive the Electorate badge than after it.The results obtained  , however  , with the FedWeb 2013 collection are completely different see Table 7. We present in the table only the best values for each of them Jelinek LM for the description field and TF-IDF for the title  and an additional method BM25 desc which will serve us as reference later.The CIFAR-10 dataset 11 consists of 60 ,000 color images drawn from the 80M tiny image collection 29. We evaluate our method on two standard large image datasets with semantic labels: CIFAR-10 11  and NUS- WIDE 3.On the Microsoft LETOR data set  , we see a small decrease in accuracy  , but the speedups are even more impressive. In particular  , on Set 1 the larger data set  , our parallel algorithm  , within a matter of hours  , achieves Expected Reciprocal Rank results that are within 1.4% of the best known results 6.Several researchers have contributed efforts for the improvement of API documentation. In contrast to these tools  , SISE links natural language sentences from Stack Overflow to API documentation.We did not investigate UML documentation as it has already been investigated by Arisholm et al. This is also similar to the format of questions and answers in Stack Overflow  , strongly suggesting this is the type of documentation developers favor.use-case  , we show how an analyst can use our solution  , PRO- HEAPS  , to study e.g.  , to verify the expertise of people publicly available forums such as Stack Overflow. This indicates that their interests are still on the same general subject.TD2004 have more relevant documents per topic than other LETOR collections  , relevant documents remain relatively sparse. As the histogram shows  , relevant documents per topic are quite sparse  , restricting the number of feedback iterations possible with stable evaluation.These data sets were chosen because they are publicly available  , include several baseline results  , and provide evaluation tools to ensure accurate comparison between methods. For meta search aggregation problem we use the LETOR 14  benchmark datasets.36 examined the long-term effect of the posts in Stack Overflow by developing a new scalable regression model. Yao et al.For evaluating the quality of a set of 10 results as returned by the resources in response to a test topic  , we use the relevance weights listed above to calculate the Graded Precision introduced by 11  as the generalized precision. These were estimated from a set of double annotations for the FedWeb 2013 collection  , which has  , by construction  , comparable properties to the FedWeb 2014 dataset.Figure 3depicts the distribution of number of friends per user. Answers and Stack Overflow  , there is no formalized friendship connection.For the domain of software development   , the website Stack Overflow 4 facilitates the exchange of knowledge between programmers connected via the Internet . Questions and answers on Q&A websites represent archives with millions of entries that are of value to the community 3 .Our goal is set to design a system as simple as possible  , without using any external processing engine or resources  , other than the standard Indri toolkit and a third party LETOR toolkit. How to optimize towards diversity under the context LETOR is yet another problem to be studied in future.We separate total running time into three parts: computation time  , communication time and synchronization time. In Table 9we report the speedup on the Orkut data set.Applied to API documentation and content from Stack Overflow  , the idea is to create a summary of the discussions on Stack Overflow as they relate to a given API type  , assuming that the reader is already familiar with the type's API documentation. Update summarization is often applied to summarizing overlapping news stories.Any API usage detected in these examples is added to a database we maintain so that usage links can be injected into API documentation. To ensure our example repository is always current  , we also continually monitor Stack Overflow to parse new source code examples as they are posted.ask.com before query " Ask Jeeves " . However  , the vlHMM notices that the user input query " ask.com " and clicked www.The features of Letor TD2003 and TD2004 datasets include low-level features such as term frequency tf  , inverse document frequency idf  , and document length dl  , as well as high-level features such as BM25  , LMIR  , PageRank  , and HITS. Each query document pair is given a binary judgment: relevant or irrelevant.The Data Collection Mechanism component is responsible for gathering Q&A data from Stack Overflow. 5: The architecture of Seahawk Data Collection Mechanism.The similar reviews include similar expressions such as " would definitely return "   , " will definitely return " . Table 4presents one positive seed review from TripAdvisor.An important new condition in the Results Merging task  , as compared to the analogous FedWeb 2013 task  , is the requirement that each Results Merging run had to be based on a particular Resource Selection run. Different evaluation measures are shown: 1. nDCG@20 official RS metric  , with the gain of duplicates set to zero see below  , and where the reference covers all results over all resources.We also recall that questions on Stack Overflow are not digitally deleted i.e. However  , this information is not directly available in the publicly available data dumps provide by Stack Overflow .Additionally   , the MPD and w7 were the result of an extensive organization effort by a whole series of computational lexicologists who had refined its format to a very easily computed structural description Reichert  , Oiney & Paris 69  , Sherman 74  , Amsler and White 79  , Peterson 82  , Peterson 871 The LDOCE while very new  , offered something relatively rare in dictionaries  , a series of syntactic and semantic codes for the meanings of its words. The MPD and w7 provided a mature collection of definitions   , and the family resemblance of the smaller MPD to the w7 and the w7 to the definitive American English dictionary  , the unabridged Merriam-Webster Third international ~31 provided the ability to find out more about definitions in any of the smaller books by consulting its " big brother " when the need arose.We have tried using Support Vector Regression RankSVM with linear kernel for pairwise LETOR  , and were trained on a set of error pairs collected using the " web2013 " relevance judgments file. Our intuition is that rank lists generated by point-wise methods are better at the top potions  , but the precision drops quickly if we go further down the list  , as they are prone to over-fit to certain features that are most dominating.Stack Overflow requires a minimum of 1 topic and a maximum of 5 topics per question  , and the results are evenly distributed between 1 and 5. Figure 4shows the number of topics per question.Wang and Godfrey analyze iOS and Android developer questions on Stack Overflow to detect API usage obstacles 25. They find nine attributes of good questions like concise code  , links to extra resources and inline documentation.In the replaying stage  , the data in WPBench Store are fed to browsers by a proxy according to the local configuration so that browsers could obtain the Web content as if they were actually from the Internet. Our benchmark meets all the aforementioned requirements.We feel that software engineering community  , as well as hardware community  , may have failed to provide good software tools to help the file and rank of embedded system developers who need to deal with such resource-poor  , yet cutting edge devices. Formal verifiers to guard for stack overflow and such will be very valuable.Example 2 shows a similar problem in a different domain. If the NASDAQ Computer Index were further divided into software  , hardware  , services  , etc.  , one can further analyze comparisons with them.In LETOR 3.0 package  , each dataset is partitioned into five for five-fold cross validation and each fold includes training   , testing and validation sets. Table 1shows the statistics of the datasets included in the LETOR 3.0 benchmark.Table 3shows the overall statistics of user-generated content on Stack Overflow between August 2008 inception to June 2013 current. Table 2We download all the available 24 database dumps for our study.While approaches to recommend Stack Overflow discussions exist 32  , our aim is to determine whether the textual content of the video tutorial fragment can be used to retrieve relevant discussions . For each video fragment   , we also show the top-three relevant Stack Overflow posts  , and ask RQ3 to what extent they are relevant and complementary to the video tutorial fragments.Besides  , we also plot the minimum bounding rectangles MBRs of tourist attractions for reference  , where the tourist attractions are collected from the metadata of OpenStreetMap. For the ease of presentation   , we highlight the clusters by different colors such that the size and shape of the clusters are clearly illustrated in the figures.In order to empirically estimate the magic barrier  , a user study on the real-life commercial movie recommendation community moviepilot 4 was performed. After 20 opinions were collected the next button terminated the study.35 ,556 deleted questions do not have information about their question authors available. We recall that we have a total of 270 ,604 deleted questions as available by using the Stack Overflow database snapshots.Figure 3 shows some representative images sampled from LabelMe and TinyImage data sets. We conduct our experiments only on the database subset  , which consists of 1 ,000 ,000 images each represented as 128-dimensional SIFT de- scriptors.The LETOR-like selection achieves both high precision and recall at small percentages of data used for training up to 5% and then it drops to the levels of statAP and depth pooling. Since infAP is based on uniform random sampling  , the precision of infAP stays constant while the recall grows linearly with the sample percentage.To achieve its goal as the main source of information about the scientific production of the Brazilian CS community  , BDBComp strongly relies on its self-archiving service. Thus  , in addition to the two tables required to represent the entity types work and set  , there is a separate table for each multivalued attribute.We detailed how it lets users interact with Stack Overflow documents in a novel way. We presented Seahawk  , an Eclipse plugin to leverage the crowd knowledge provided by Q&A services.Table 4presents one positive seed review from TripAdvisor. Both the similar reviews are negative and contain negative words like " horrible "   , " bad "   , " nauseous " which are synonyms to " awful " in the seed.This is because we were counting on topic modelling based query expansion to improve diversity performance  , such that we have not defined a dedicated list-wise optimization criterion on top of the rank list that addresses diversity. It is also worth noticing that even though most of these features are directly consistent to the relevance of a document to a query  , none of our LETOR methods include diversity into account .We recall that a question on Stack Overflow can either be deleted by the author of the question or by a moderator . In order to stem further decrease reputation points  , question owners see deletion of question as a quick fix and therefore  , proceed to delete the posted question in an attempt to salvage their reputation.For example   , when annotating a Stack Overflow example with FQN information  , presenting a small number of possible type options for the developer to choose from would be reasonable as they could use their own intuition gained from the snippet text to make an informed selection. The impact of high-cardinality matches depends on the intended use of the data; some tasks favour recall over precision while for other tasks the reverse is preferred.In our study  , we use more than 15M reviews from more than 3.5M users spanning three prominent travel sites  , Tripadvisor   , Hotels.com  , Booking.com spanning five years for each site. We provide True- View as a proof of concept that a cross-site analysis can significantly improve the information that the user sees.We created a script to extract questions along with all answers  , tags and owners using the Stack Overflow API. Our data collection follows a mixed-methods approach  , collecting both quantitative and qualitative data.When the properties of the above document selection methodologies are considered  , one can see that infAP creates a representative selection of documents  , statAP and depthk pooling aim at identifying more relevant documents utilizing the knowledge that retrieval systems return relevant documents at higher ranks  , the LETOR-like method aims at selecting as many relevant documents according to BM25 as possible  , hedge aims at selecting only relevant documents  , and MTC greedily selects discriminative documents. The LETOR-like selection methodology also selects very similar documents  , since the documents selected are those that give high BM25 values and thus have similar characteristics.The images are 32 × 32 pixels and we represent them with 512-D GIST descriptors. The CIFAR-10 data set contains 60 ,000 tiny images that have been manually grouped into 10 concepts e.g.  , airplane  , bird  , cat  , deer.In detail  , in the first pass we use the standard Indri retrieval algorithm and BM25 with pseudo relevance feedback on the topby the length of the document. Those features are then piped into different LETOR algorithms to produce several rank lists  , and eventually all the rank lists are merged using the conventional Reciprocal Rank based data fusion method.Users contribute and interact by posting questions  , answers and comments  , and provide feedback by voting on questions and answers and by selecting the best answer to their question. Community Question Answering CQA sites such as Stack Overflow 1 provide a growing resource of information.We observe an increasing trend in the number of deleted questions on Stack Overflow over the last 2 years. We conduct the first large scale study of deleted questions on Stack Overflow.The relatively high correlation between the topic properties themselves and the fact that our simple classifier performs with an accuracy of 58.6% indicates room for future improvements in the definition of the topic properties. We see that the Stack Overflow and Programmers communities stand out  , one with low and the other one with high values for all three topic properties.Another threat to external validity of our evaluation concerns the representativeness of spreadsheets in the EUSES corpus and collected in our case study. As such  , we validated the results by ourselves partially and manually in due diligence.These collection are indexed using Lucene SOLR 4.0 and we use BM25 as the retrieval model. Six collections  , relevant to the assignment about television and film personalities  , from various archives were indexed: 1 a television program collection containing 0.5M metadata records; 2 a photo collection with 20K photos of people working at television studio; 3 a wiki dedicated to actors and presenters 20K pages; 4 25K television guides that are scanned and OCRed; 5 scanned and OCRed newspapers between 1900 and 1995 6M articles; and 6 digital newspapers between 1995 and 2010 1M articles.Basic methods that we used for these tasks will be described in section 2. We have participated all the three tasks of FedWeb 2014 this year.This work was funded in part by the National Science Foundation  , under NSF grant IIS-0329090  , and as part of the EUSES consortium End Users Shaping Effective Software under NSF grant ITR CCR-0324770. We would like to thank Andrew Ko and Justin Weisz for their valuable help with this paper.This dataset  , from the German movie-rental site MoviePilot  , was released as part of the We overcome this by using a dataset that contains individual user preferences and their group membership.Overflow. Second  , users in Stack Overflow are fully independent and no social connections exist between users.However  , it must be noted that our dataset contains the maximum possible deleted questions which can be obtained given the publicly available Stack Overflow database snapshots. Questions deleted between two data snapshots would not be captured in our experimental dataset.3 How would you grade your knowledge of bibliographic self-archiving after using the BDBComp service ? 2 How would you grade your knowledge about the Dublin Core metadata standard ?The Wookieepedia collection provides two distinct quality taxonomies. These are the two Wikia encyclopedias with the largest number of articles evaluated by users regarding their quality.The SO website enables users to ask and answer computer programming questions  , and also to vote on the quality of questions and answers posted on the website. To assess the generality of using document similarities based on word embeddings for information retrieval in software engineering  , we evaluate the new similarity functions on the problem of linking API documents to Java questions posted on the community question answering cQA website Stack Overflow SO.In Study 3 S3  , we analyze 100 randomly selected public GitHub repositories that use Java's Study 2 S2 is a pilot survey that gathers data from 11 developers who asked Java cryptography-related questions on Stack- Overflow.It extends SCOVO 10 with the ability to explicitly describe the structure of the data and distinguishes between dimensions  , attributes and measures. Data Cube model is compatible with SDMX – an ISO standard for sharing and exchanging statistical data and metadata.The poor performance of SVM-DBSCAN is mainly due to the small number of attributes used when compared with the original proposed method described in 17. In the BDBComp collection  , SAND outperforms the KWAY and SVM-DBSCAN methods by more than 36% under the pF1 metric.Political news flowing out of Arab Spring uprisings to broadcast media was often curated by sites such as Nawaat.org that had emerged as trusted local information brokers. Many " viral " videos take off on social media only after being featured on broadcast media  , which often follows their being highlighted on intermediary sites such as Reddit or Buzzfeed.Our preliminary findings indicate that Stack Overflow is particularly effective at code reviews  , for conceptual questions and for novices. Understanding the interactions on Q&A websites  , such as Stack Overflow  , will shed light on the information needs of programmers outside closed project contexts and will enable recommendations on how individuals  , companies and tools can leverage knowledge on Q&A websites.This value was chosen based on some preliminary experiments we performed on the FedWeb 2012 test collection Nguyen et al.  , 2012. The relevance cut-off parameter N is set to 200.The EUSES corpus consists of 4 ,037 real-life spreadsheets from 11 categories. We observe that ambiguous computation smells occur commonly in the corpus:We mined and extracted discussions related to the topics of the extracted video tutorials  , pre-processed them to reduce the noise  , and made them available to CodeTube. To illustrate this  , we added as an additional online information source the Stack Overflow data dump.This indicates that cell arrays are common in real-life spreadsheets. Our empirical study reports that there are altogether 16 ,385 cell arrays among 993 out of 4 ,037 spreadsheets in the EUSES corpus 11.Oslom takes several days to analyze the Orkut graph whereas SCD finds the communities in a few minutes. For practical purposes  , this computational complexity creates a barrier to analyze large networks by the group of slow algorithms.This means  , for example  , that if the PageRank feature is important for the current query  , there is a good chance it will be important for other queries as well. With LETOR  , in contrast  , features capture general properties of query-document compatibility .oai_dc: contains only the accession id in the title field to satisfy the mandatory requirement of OAI 1. Two OAI metadata formats are provided for each OAI item: refseq: contains the refseq records in our refseq XML format.As presented before  , we experimented with one run based on document relevance and with three other runs depending on the output of the previous task  , that is  , a ranking of resources. On the other hand  , the boosting method is highly dependent on the ranking of the resources  , as we observe when a better resource selection method is used BM25 desc in FedWeb 2013 or the hybrid run in FedWeb 2012.From Figure 3   , it is easy to see that LabelMe and TinyImage have different characteristics. Please note that the authors of ANN_SIFT1M provide only the extracted features without any original images of their data.In analyzing the runtime speedup for parallel LDA  , we trained LDA with 150 topics and 500 iterations. To conduct our scalability experiments  , we used the same Orkut data set as was used in Section 5.1.We also note that ≈23% of questions in our experimental dataset have received 'delete votes' i.e. The moderators are driven by the Stack Overflow motto to keep a low signal-to-noise ratio in order to maintain high content quality.Furthermore  , unlike on LETOR where the performance of ActiveAda-S-T converges more quickly than others  , we do not observe any trend of convergence up to the point with 2000 selected queries. It is more likely that these cross-domain relevant knowledge in source domain are even more helpful than those most informative target queries identified by Active-T.This comprises articles  , advertisements  , ocial notifications  , and the captions of illustrations see Table 1for details. The newspaper data set made available to us ranges from 1618 to 1995 4 and consists of more than 102 million OCRed newspaper items.A publicly available dataset periodically released by Stack Overflow  , and a dataset crawled  from Quora that contains multiple groups of data on users  , questions   , topics and votes. Our analysis relies on two key datasets.Accordingly   , let IDCGp be the maximum possible discounted cumulative gain for a given query. We chose to standardize this issue  , using the same criterion used by most evaluation tools  , e.g.  , those available for the Letor 3.0 and 4.0 and Microsoft datasets  , in order to allow fairer comparisons.SCOVO is used in voiD  , the " Vocabulary of Interlinked Datasets " 1  to express information about the number of triples  , resources and so forth. 5kudos to Andreas Langegger for the screen shot  , that generates statistics for datasets behind SPARQL-endpoints and RDF documents.We have described an experimental method in which learnt uncertainty information can be used to guide design choices to avoid overfitting  , and have run a series of experiments on the benchmark LETOR OHSUMED data set for both types of model. The Gaussian process allows the smoothing uncertainties in SoftRank to reflect modelling uncertainty in the learnt score function.Code- Tube also automatically complements the video fragments with relevant Stack Overflow discussions. Our approach extracts video fragments by merging the code information located and extracted within video frames  , together with the speech information provided by audio transcripts.The Merriam-Webster and Longman dictionaries offered different capabilities as repositories of data about lexical concepts. Three of the most accessible were the Merriam-Webster Pock& Dictionary MPD  , its larger sibling  , the Merriam-Webster Seventh Colegiate ~7 and the Longman Di@ionary of Contemporary English LDOCE.For example in Ask.com search site  , some uncached requests may take over one second but such a query will be answered quickly next time from a result cache. meet the soft deadline.Our empirical comparisons using the top-k recommendations metric show a surprisingly intuitive finding: that LDA performs consistently better than ARM for the community recommendation task when recommending a list of 4 or more communities.  We apply both algorithms to an Orkut data set consisting of 492  , 104 users and 118  , 002 communities.Stack Overflow is a programming based CQA and the most popular Stack Exchange website consisting of 5.1M questions  , 9.4M answers and 2.05 registered users on its website. 1 CQA websites on Stack Exchange span across different orthogonal themes like Technology Web Applications  , Game Development  , Culture Travel  , Christianity  , Arts Photograph  , Scientific Fiction and Sciences Mathematics   , Physics.These results indicate that taking into account Stack Overflow meta data as well as part-of-speech tags can significantly improve existing unsupervised approaches when applied to Stack Overflow data. The study showed that sentences extracted by SISE were considered significantly more meaningful and resulted in the most sentences that added useful information not contained in the API documentation.their ground truth difference with the production ranker would be the same as ordering them by the estimated difference. P -perfect user model setting  , I -informational  , N -navigational LETOR eval- uation.Most QA systems are substantial team efforts  , involving the design and maintenance of question taxonomies 14  , 15  , question classifiers  , and passage-scoring heuristics. Fal- con 14  , Webclopedia 15  , Mulder 18  , AnswerBus 28 and AskMSR 11 are some well-known research systems  , as are those built at the University of Waterloo 7  , 8  , and Ask Jeeves http://ask.com.The quality focus concerns i the perceived benefits and obstacles in using video tutorials during development and ii the quality of video fragments cohesiveness  , self-containment  , relevance to a query and Stack Overflow discussions relevance and complementarity to the video fragment  mined by CodeTube. The goal of this study is to evaluate CodeTube with the purpose of determining the quality of the extracted video fragments and related Stack Overflow discussions perceived by developers.As with our first batch of results presented for Ro- bust04  , we again assume the user provides correct feedback. Since we are only training on a single topic  , resulting accuracy is far lower than what typically published LETOR results.With LETOR  , in contrast  , features capture general properties of query-document compatibility . For example  , while the word " dog " might be very important to the current query  , RF on this query cannot tell us anything about how important this word should be for other queries.On the BDBComp collection  , SAND outperforms all methods under all metrics by more than 60%. More important  , when we provided the same training data to the second step of SAND  , it outperforms all other supervised methods by 6% against SVM and 13% against NB  , showing that it is able to better explore the manually provided training data along with its other self-training  , transductive characteristics.This open-source alternative mapping service also publishes regular database dumps. OpenStreetMap OSM.Therefore  , we make estimation from the crawled posting data. However  , accurate estimation of visit probabilities is impossibile due to the lack of login and browsing data of TripAdvisor users.Using SCOVO in voiD allows a simple and extendable description of statistical information  , however  , a shortcoming has been identified: as scovo:Items are grouped into scovo:Datasets  , there is an implicit assumption that all items in such a dataset share the same dimensions. SCOVO is used in voiD  , the " Vocabulary of Interlinked Datasets " 1  to express information about the number of triples  , resources and so forth.The most common use of Stack Overflow is for how-to questions  , and its dominant programming languages are C#  , Java  , PHP and JavaScript. Our preliminary findings  , obtained through the analysis of archival data from Stack Overflow and qualitative coding  , indicate that Q&A websites are particularly effective at code reviews  , explaining conceptual issues and answering newcomer questions.While manually detecting irregularities for this data might be difficult  , examining the distribution of the pt values cf. 4 In Figure 7 we have already illustrated the distribution of ratings over time for the hotel Punta Cana Princess evaluated on TripAdvisor.oai_dc: contains only the accession id in the title field to satisfy the mandatory requirement of OAI. Two OAI metadata formats are provided for each OAI item: refseqp: contains the refseq records in our refseqp XML format.As such  , we validated the results by ourselves partially and manually in due diligence. One threat to internal validity of our evaluation is that we were unable to validate analysis results of spreadsheets in the EUSES corpus by their original users.Fig- ure 16shows the word cloud of the top-50 tags that occur in undeleted questions on Stack Overflow. We now analyze the tags on undeleted questions.To repair a ous computation smell existing work on appropriate formula pattern in an array that suffers We evaluated our lyzed the EUSES corpus putation smells can formance of our smells. We tection to a constraint satisfaction problem.We seek to answer several key questions:  What role do traditional question topics play in focusing user attention ? To highlight key results  , we use comparisons against Stack Overflow  , a popular Q&A site without an integrated social network.Of the 197 occurrences of 'bank'  , the vector analysis correctly assigned 45 percent of them to the correct sense. Wilks manually disambiguated all occurrences of the word 'bank' within LDOCE according to the senses of its definition and compared this to the results of the cosine correlation.The second data set further referred to as Hotel consists of reviews of hotels crawled from TripAdvisor 5 along with the meta-data of review authors   , such as location  , gender and age 6 . This first data set further referred to as Auto consists of reviews crawled from FordForum.com 4   , a public automobile on-line review website  , which provides the meta-data of review authors  , such as location  , gender and occupation in this work  , we are only interested in location and gender.To the best of our knowledge  , this is the first work which studies poor quality questions on a large-scale CQA website like Stack Overflow. The properties of deleted questions are different in both topic and content.We conducted two studies to evaluate CodeTube. Code- Tube also automatically complements the video fragments with relevant Stack Overflow discussions.We filter out those points which are either outside of the city boundary or in the ocean. In fact  , by taking the OpenStreetMap polygons for Santa Barbara and Ventura and defining a regular point grid of 1 × 1 km  , we can compute the probability of grid points contained in Ventura to locate in the southeast of Santa Barbara grid points.Snippets contain document title  , description  , and thumbnail image when available. The FedWeb 2014 Dataset contains both result snippets and full documents sampled from 149 web search engines between April and May 2014.Cultural context may be a big reason why account gifting is more predominant in developing regions. She taught them how to upload pictures and leave scraps for each other  , and in this way  , was their gateway to Orkut.We also conducted experiments to observe the training curve of PermuRank.MAP in terms of MAP on OHSUMED. Figure 5and Figure 6show the results on the Letor TD2003 and TD2004 datasets.The dynamic of the OpenStreetMap project will ensure a steady growth of the dataset. This enhancement enables a variety of new Linked Data applications such as geo data syndication or semantic-spatial searches.In the current system  , the page number of a scanned page is recognized by analyzing the OCRed text. As another example  , in case the program can not recognize the volume and issue number due to OCR error  , such as " IV " was OCRed as " it "   , the program will use the previous or the following title page information  , if available  , to construct the current volume or issue metadata.For example  , NASDAQ real-time data feeds include 3 ,000 to 6 ,000 messages per second in the pre-market hours 43; Network and application monitoring systems such as Net- Logger can also receive up to a thousand messages per sec- ond 44. Depending on the application  , the number of messages per second ranges from several to thousands.2  is an Eclipse plug-in that integrates Stack Overflow content into an integrated development environment IDE. Seahawk by Bacchelli et al.Those questions and answers consist of archives with millions of entries that are of value to the users  , as well as the researchers who have interest in studying user information needs  , uses  , and seeking behaviors. Answers and Stack Overflow allow people to meet their information needs by asking questions and receiving answers from their peers on a broad range of topics.On all evaluation metrics the ranking perceptron achieves scores comparable to SVM on the OHSUMED and TD2003 datasets  , and comparable to RankBoost on TD2004. However  , to get an estimate for the accuracy of the methods we implemented  , we evaluated the ranking perceptron see Section 3  , on the Letor dataset 21.Last community is the withheld community while the rest are joined communities. The category of each community is defined on Orkut.Stack Overflow is a collaborative question answering Stack Exchange website. To the best of our knowledge  , this is the first work which studies poor quality questions on a large-scale CQA website like Stack Overflow.In both cases we used a target dimensionality o f d tar = 10 for the generalized nearest neighbor. The Spambase Database is derived from a collection of spam and non-spam e-mails and consists of 4601 instances with 57 numeric attributes.discussing travel experiences in TripAdvisor. Answers while others could be more general e.g.We perform a longitudinal study of deleted questions  , community voting patterns  , deletion behavior by question owners and discover question quality structure. We make the following research contributions  We analyze deleted questions on Stack Overflow posted over ≈5 years and conduct a characterization study.Community Question Answering CQA sites such as Stack Overflow 1 provide a growing resource of information. Our work differs from previous work on early expertise discovery in two ways:Concretely  , questions available in the earlier database snapshots August 2009 – questions on Stack Overflow. Hence  , we analyze the entire 24 database snapshots over ≈ 5 years to construct our dataset.Orkut is a large social networking website.  Orkut.To illustrate this  , we added as an additional online information source the Stack Overflow data dump. CodeTube can be enriched by mining other online resources   , as our long-term goal 29 is to offer a holistic point of view on the information at disposal  , also because we argue that no single type of resource can offer exhaustive assistance.Figure 5shows how our browser extension modifies a Stack Overflow post. With this mapping  , the browser extension can automatically determine the correct target page that should be annotated in either direction with either the source code example or API documentation link.We can also inject the links to the official API into the Stack Overflow post; these two additions to the documentation would make it easier for developers to learn how to use this class. With this information we can automatically augment the HTML version of the official API documentation for History by dynamically injecting the code example into the web page.Orkut also offers friend relationship. Once a user joins orkut  , one can publish one's own profile  , upload photos  , and join communities of interest.Table 6shows the results obtained for some of these methods with the FedWeb 2012 collection. For the resource selection task we tested different variations of the strategies presented above.How to optimize towards diversity under the context LETOR is yet another problem to be studied in future. This is because we were counting on topic modelling based query expansion to improve diversity performance  , such that we have not defined a dedicated list-wise optimization criterion on top of the rank list that addresses diversity.The study was performed through a webpage mimicking the look-and-feel of the moviepilot website  , on this page users were presented with a random selection of movies they had previously rated  , with the ratings withheld. The purpose was withheld so to not affect the outcome.The second source of information is trade-level data for over 8000 publically traded companies on the NYSE  , AMEX and NASDAQ exchanges. 6 6 We do not consider the many important news stories that appear " after the bell  , " focusing here only on stories for which we have trading data.Explicit uncertainty information can  , of course  , be used for other purposes — for example  , to provide for the user a confidence level for each document in the ranking. However  , we have found little evidence  , at least for the LETOR OHSUMED data set  , that explicit use of the uncertainty information can improve model performance in terms of NDCG.The framework aims at supporting people to publish their statistics on the Web of Data in an effective and efficient manner. We have proposed a vocabulary  , SCOVO  , and discussed good practice guidelines for publishing statistical data on the Web in this paper.These interactions are emulated during benchmarking browsers by instrumented JavaScript which is independent of Web browsers. In WPBench  , user interactions are recorded when users are browsing a set of the most popular Web 2.0 applications.The input for this task is a collection provided by the organisers FedWeb 2013 collection consisting of sampled search results from 157 search engines. Besides  , since each snippet has both a title and a description  , we tested considering only the title field to match the query  , only the description field desc  , or both.Four thousand queries were adopted to gather samples from the diverse search engines; these samples were the basis for building descriptions for the informative resources at the various levels search engines and verticals. These 149 engines were a subset of the 157 search engines in the FedWeb 2013 test collection.We then give details on the key Quora graph structures that connect different components together. In this section  , we introduce Quora  , using Stack Overflow as a basis for comparison.It should be noted that for different classes of requests  , an application may deploy different termination ranges and control parameters and our API design can support such differentiation. Our experiments with two applications from Ask.com indicate the proposed techniques can effectively reduce response time and improve throughput in overloaded situations.Community based features are derived via the crowdsourced information generated by the Stack Overflow community. Profile based features are based on the user-generated content on the Stack Overflow website.Pyramid. Figure 14shows this underlying question quality pyramid structure on Stack Overflow.It is a good datasest for our experiments since we will be discovering patterns from features that have already been proven to work. The LETOR dataset conveniently extracts many stateof-the-art features from documents  , including BM25 22  , HITS 14  , and Language Model 34.The data reported in this paper was extracted on November 23  , 2010 and contains all questions that were asked between November 1  , 2010 and November 15  , 2010. We created a script to extract questions along with all answers  , tags and owners using the Stack Overflow API.To achieve higher accuracy than we did with topes  , programmers would need to combine numerous international formats into a single regexp for each data category  , which stands in stark contrast to current practice. However  , even in the 7 categories where programmers have published regexps on the web  , or where we could convert dropdown or radio button widgets to regexps  , F 1 was only 0.31 the same accuracy as Condition 4 in those categories  , owing to a lack of regexps for unusual international formats that were present in the EUSES spreadsheet corpus.Two OAI metadata formats are provided for each OAI item: refseqp: contains the refseq records in our refseqp XML format. A simple RefseqP XML schema was created for the RefSeqP OAI repository.From the Wikia service  , we selected the encyclopedias Wookieepedia  , about the Star Wars universe  , and Muppet  , about the TV series " The Muppet Show " . From now on  , we refer to this encyclopedia as WPEDIA.Our experiments on LETOR 3.0 benchmark dataset show that the  NDCG-Annealing algorithm outperforms the state-of-theart algorithms both in terms of performance and stability. As an instance of the method  , we optimize NDCG in this paper  , but other metrics in ranking can also be applied directly.Each question has between one and five tags that are set by the person asking a question. The tags were mainly used to learn about the topics covered by Stack Overflow  , while the question coding gave insight into the nature of the questions.To analyze the different kinds of questions asked on Stack Overflow  , we did qualitative coding of questions and tags. It is interesting to note that the community answered review   , conceptual and how-to / novice questions more frequently than other kinds of questions.